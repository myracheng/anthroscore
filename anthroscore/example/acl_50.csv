,acl_id,abstract,full_text,corpus_paper_id,pdf_hash,numcitedby,url,publisher,address,year,month,booktitle,author,title,pages,doi,number,volume,journal,editor,isbn,ENTRYTYPE,ID,language,note,Model Predicted Topics
0,O02-2002,"There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together.","There is a need to measure word similarity when processing natural languages, especially when using generalization, classification, or example -based approaches. Usually, measures of similarity between two words are defined according to the distance between their semantic classes in a semantic taxonomy . The taxonomy approaches are more or less semantic -based that do not consider syntactic similarit ies. However, in real applications, both semantic and syntactic similarities are required and weighted differently. Word similarity based on context vectors is a mixture of syntactic and semantic similarit ies. In this paper, we propose using only syntactic related co-occurrences as context vectors and adopt information theoretic models to solve the problems of data sparseness and characteristic precision. The probabilistic distribution of co-occurrence context features is derived by parsing the contextual environment of each word , and all the context features are adjusted according to their IDF (inverse document frequency) values. The agglomerative clustering algorithm is applied to group similar words according to their similarity values. It turns out that words with similar syntactic categories and semantic classes are grouped together. Introduction It is well known that word-sense is defined by a word's co-occurrence context. The context vectors of a word are defined as the probabilistic distributions of its left and right co-occurrence contexts. Conventionally , the similarity between two context vectors is measured based on their cosine distance [Alshawi and Cater, 1994; Grishman and Sterling, 1994; Pereira et al., 1993; Ruge, 1992; Salton, 1989] . However, the conventional measurement * Institute of Information Science, Academia Sinica E-mail: kchen@iis.sinica.edu.tw; swimming@hp.iis.sinica.edu.tw suffers from the following drawbacks. First of all, the information in the context vectors is vague. All co-occurrence words are collected without distinguishing whether they are syntactically or semantically related. Second, the coordinates are not pair-wise independent (i.e., the axes are not orthogonal), and it is hard to apply singular value decomposition to find the orthogonal vectors [Schutze, 1992] . In this paper, we propose to use only syntactic related co-occurrences as context vectors [Dekang Lin, 1998 ] and adopt information theoretic models to solve the above problems. In our study, the context vectors of a word are defined as the probabilistic distributions of its thematic roles and left/right co-occurrence semantic classes. The context features are derived from a treebank. All context features are weighted according to their TF × IDF values (the product of the term frequency and inverse document frequency) [Salton, 1989] . For the context features, the Cilin semantic classes (a Chinese thesaurus) are adopted. The Cilin semantic classes are divided into 4 different levels of granularity. In order to cope with the data sparseness problem, the weighted average of the similarity values at four different levels will be the similarity measure of two words. The weight for each level is equal to the information-content of that level [Shannon, 1948; Manning and Schutze 1999] . A agglomerative clustering algorithm is applied to group similar words according to the above defined similarity measure. Obviously, words with similar behavior in the corpus will be grouped together. We have compared the clustering results to the Cilin classifications. It turns out that words in the same synonym class and with the same syntactic categories have higher similarity values than the words with different syntactic categories. Data Resources Ideally, to derive context vectors, a large corpus with semantic tags is required. Furthermore, to extract co-occurrence words along with their exact syntactic and semantic relations, the corpus structure has to be annotated. However, such an ideal corpus does not exist. Therefore, in this paper we will adopt the resources that are available and try to derive a useful but imperfect Chinese tree bank. Since the similarity measure based on the vector space model is a rough estimation, minor errors made at the stage of context vector extraction are acceptable. Sinica Corpus The Sinica corpus contains 12,532 documents and nearly 5 million words. Each sentence in the corpus was parsed by a rule parser [Chen, 1996] . The parsed trees were tagged with the structure brackets, syntactic categories and thematic roles of each constituent [Huang et al., 2000] as exemplified below, (Sinica corpus: http://www.sinica.edu.tw/ftms -bin/kiwi.sh): Original sentence: 小 'small' 狗 'dog' 跳舞 'dance' Parsed tree : S(Agent:NP:(Property:Adj:'小 small'| Head: N: ' 狗 dog' )|Head:V: ' 跳舞 dance') Although these labels may not be exactly correct, we believe that, even with these minor errors, the majority of word -to-word relations extracted from the trees are correct. However, the semantic label is not provided for each word in the parsed trees. In this paper, we will use Cilin classifications for semantic labeling. Cilin-a Chinese Thesaurus Cilin provides the Mandarin synonym sets in a hierarchical structure [Mei et. al., 1984] . It contains 51,708 Chinese words, and 3918 classes. There are five levels in the Cilin semantic hierarchy, denoted in the format L 1 -L 2 -L 3 -L 4 -L 5 . For example, the Cilin class of the word 我們 'we' is ""A -a-02-2-01"". In level 1, ""A"", denotes the semantic class of human; in level 2, ""a"", indicates a group of general terms; level 3, ""02"", means pronouns in the first person, and in level 4, ""2"" represents the plural property. In level 5, ""01"" represents the order rank in the level 4 group. This means that ""01"" in level 5 is the first prototypical concept representation of ""A-a-02-2"". In the rest of this paper, only the first four levels will be used. The fifth level is for sense disambiguation only (section 2.3). Sense Disambiguation A polysemous word has more than one Cilin semantic class. In order to tag appropriate Cilin classes, we have designed a simple sense tagging method as follows [Wilks, 1999] . The sense tagging algorithm is based on the facts that the syntactic categories of each word in the tree bank are assigned uniquely, and that each Cilin class has its own major syntactic category. If a word has multiple Cilin classes, we select the sense class whose major syntactic category is the same as the tagged category of this word. For example , 計畫 "" Jie -Hwa"" has two meanings. One is for ""project"" as a noun and the other is ""attempt"", therefore, if 計畫 ""Jie -Hwa"" was tagged with a noun category, we will assign the Cilin class whose major category is ""project"". Sense ambiguity can be distinguished by measure of syntactic properties for most words. However, there are still cases in which the syntactic category constraints cannot resolve the sense ambiguities. Then, we simply choose the prototypical sense class, i.e, the word that has the highest rank in this sense class with respect to all its sense classes in Cilin. The Extraction of Co-occurrence Data The extracted syntactically related pairs have either a head-modifier relation or head-argument relation. For instance, two syntactically related pairs extracted from the example in section 2.1 are: (<Thematic role > <Cilin> <word1>), (<Thematic role> <Cilin> <word2>) (agent B -i-07-2 狗'dog'), (Head(S) H-h-04-2 跳舞'dance' ) (property E -a-03-3 小'small'), (Head(NP) B -i-07-2 狗'dog') The context data of the word 1 狗 ""dog"" consists of its thematic role ""agent"" and the Cilin class ""B-i-07-2"" ; the word 2 跳舞 'dance' consists the thematic role ""Head(S)"" and the Cilin class ""H-h-04-2"" and so on. The word 小 ""small"" and 跳舞 ""dance"" are not syntactically related even though they co-occur. Therefore , they will not be extracted. Context Vector Model There are three context vectors of a word: role vector, left context vector and right context vector. The role vector is a fixed 48-dimension vector, and each dimension value is equal to the probabilistic distribution of its thematic roles. The left/right context vectors are closer to the probabilistic distributions of its left/right co-occurrence words and their semantic classes. The role vector characterizes a word based more on syntax and less on semantics, but the left/right context vectors are just the opposite. The cosine distance between their context vectors is a measure of the similarity of the two words. We will illustrate the derivations of context vectors and their similarity rating with a simplified example using (貓 ""cat"", 狗 ""dog""). The role vector of ""dog"" is {127, 207, 169… , 0} 48 , which represents the values of ""agent"", ""goal"", ""theme""… and ""topic"" respectively, generated by Equation (1). The role vector of ""cat"" is {28, 73, 56… , 0} 48 , which is also acquired by Equation (1). Role vector of word W = { V 1 , V 2 ,… ,V 48 } 48 Vi = Frequency (R i ) × log (1/P i ) (1) R i : We label thematic roles ""agent"", ""goal""… and ""topic"" from R 1 to R 48 listed in Table2 in the Appendix. Frequency (R i ): The frequency of R i played by word W in the corpus. P i : = Total frequency of R i in the corpus / Total frequency of all roles in the corpus. log(1/P i ): The information-context of R i [Shannon, 1948; Manning and Schutze, 1999] The derivation of left/right context vectors is a bit more complicated. The syntactically related co-occurrence word pairs are derived first as illustrated in section 2.4. We will illustrate the derivation of the left context vector only. The right context vector can be derived simila rly. The left co-occurrence word vector of word W is generated from frequency(word i ), where word i precedes and is syntactically related to W in the corpus. Due to the data sparseness problem, the feature dimensions of context vectors are generalized into C ilin classes instead of co-occurrence words. The generalization process reduces the effect of data sparseness. On the other hand, it also reduces the precision of characterization since each word has different information content and two words that have the same co-occurrence semantic classes may not share the same co-occurrence words. In order to resolve the above dilemma, when we compare the similarity between word X and word Y , 4 levels of left context vectors and right context vectors for word X and word Y are created 1 . The weighting of each feature dimension is adjusted using the TF*IDF value if word X and word Y have shared context words. Equation ( 2 ) illustrates the creation of the 4 th level left context vector of word X . The other context vectors for word X and word Y are created by a similar way. Left context vector of word X = {f1, f2,… ,f 3918 } L4 Where fi = Sum of { TF(word j ) × IDF(word j ) if word X has the same neighbor word j with word Y TF(word j ) if word X does not have the same neighbor word j with word Y } ; for every left co-occurrence word j with the ith Cilin semantic class. (2) TF(word j ): the frequency of pair ( word j , Cilin(word j ) ) in wordx' s co-occurrence context. IDF(word j ): -log(the number of the documents that contains the word j /total document number of the corpus) In Equation ( 2 ), we adjust the term weight using TF × IDF, which is commonly used in the field of information retrieval [Salton, 1989] to adjust the discrimination power of each feature dimension. We will examine the difference in the adjustment of weights using TF × IDF and TF in section 4.2. We will next give a simplified example. Assume that the word 狗 ""dog"" has only three left syntactically related words: ( 小 ""small"" Ea033 ) with frequency 30, ( 可愛 ""cute"" Ed401 ) with frequency 5 and ( 養 ""raise"" Ib011 ) with frequency 10; and assume that the word 貓 ""cat"" has only two left syntactically related words: ( 黑 ""black"" Ec043 ) and ( 養 ""raise"" Ib011 ). Assume that we are measuring the similarity between 狗 ""dog"" and 貓 ""cat"". Then, we can compute the left context data of 狗 ""dog"" as {TF(Aa011),… ,TF(Ea033),… ,TF(Ed041),… ,TF(Ib011) × IDF(養'raise') 2 , … , TF(La064) } L4 3 1 IDF(養'raise') = 4.188, the IDF values of all words range from 0.19 to 9.12. 2 The granularities of the 4 levels of semantic classes are partially shown in Figure2. The four left context vectors and their dimensions are shown below and the right context vectors are similarly derived. <LeftCilin1> L1 A vector of 12 dimensions from ""A"" to ""L"". <LeftCilin2> L2 A vector of 94 dimensions from ""Aa"" to ""La"". <LeftCilin3> L3 A vector of 1428 dimensions from ""Aa01"" to ""La06"". <LeftCilin4> L4 A vector of 3918 dimensions from ""Aa011"" to ""La064"". Similarities between Two Context Vectors Once we know the feature vectors of these two words, we can calculate the cosine distance of two vectors as shown in Equation (3). vector A= <a1,a2… ,an>,vector B= <b1,b2… ,bn> ... ) , cos( 1 2 1 2 1 ∑ ∑ ∑ = = = × × = n i n i n i bi ai bi ai B A (3) Therefore, the similarity of the two words x and y can be calculated as the linear combination of the cosine distances of all the feature vectors as shown in the Equation 4 . The weight of each feature vector can be adjusted according to different requirements. For instance, if the syntactic similarity is more important, we can increase the weight w. On the other hand, if the semantic similarity is more important, the weights w1 to w4 can be increased. If more training data is available, the level 4 vector will be more reliable. Hence, the weight w4 should increase. (4) y)} > 4 RightCilin < x, > 4 RightCilin cos(< w42 + y) > LeftCilin4 < x, > LeftCilin4 (< cos w41 { w4 y)} > 3 RightCilin < x, > 3 RightCilin (< cos w32 + y) > LeftCilin3 < x, > LeftCilin3 (< cos w31 { w3 y)} > 2 RightCilin < x, > 2 RightCilin (< cos w22 + y) > LeftCilin2 < x, > LeftCilin2 (< cos w21 { 2 y)} > 1 RightCilin < x, > 1 RightCilin (< cos w12 + y) > LeftCilin1 < , x > LeftCilin1 (< cos w11 { 1 y) > vector role < x, > vector role (< cos w = y) (x, similarity × + × + × + × + × w w In the experiments, w = 0.3, w1 = 0.1 × 0.7, w2 = 0.1 × 0.7, w3 = 0.4 × 0.7, and w4 = 0.4 × 0.7. Similarity Clustering Because of the lack of objective standards for evaluating of similarity measures, a agglomerative clustering algorithm is applied to group similar words according to a similarity value. It turns out that words with similar syntactic usage and similar semantic classes are grouped together. We will evaluate our algorithm by comparing the automatic clustering results with manual classifications of Cilin. Clustering Algorithm To evaluate the proposed similarity measure, we tried to group words according to various parameters. We adopted bottom-up agglomerative clustering algorithm to group words. In order to compare the clustered results with Cilin classifications and reduce the data sparseness, we picked the 1000 highest frequency words in Cilin for testing. First of all, we produced a 1000 × 1000 symmetric similarity matrix called SMatrix, where SMatrix (x, y) = similarity 4 3 2 1 4 , 3 , 2 , 1 K RightCilin K n RighttCili K LeftCilin x K LeftCilin K RightCilin x K RightCilin K RightCilin K n RighttCili K LeftCilin x K LeftCilin K LeftCilin x K LeftCilin 2 1 > < = + + + + = > < + > < + > < + > < > < + > < = > < + > < + > < + > < > < + > < = [i] SMatrix[x] , q , p 1 q p 1 q ≤ ≤ ≤ ≤ < < × = ∑∑ = = p n m m p n n q 1 word contains ) wordj ( Group m 1 word )contains wordx ( Group x j 0 ) word , (word similarity [x] SMatrix[j] , q , p 1 q p 1 q ≤ ≤ ≤ ≤ < < × = ∑∑ = = p n m m p n Clustering Results vs Cilin Classification We will make a comparison between the clustering results and Cilin classifications. There are two simple examples shown in Figure 1 Among our 1000 testing words, the number of words that were clustered in the 4 th level of Cilin was 658; i.e., they were labeled with 459 different level-4 Cilin classes and among them, 342 classes contained only one testing word, and the classes with multiple testing words contained a total of 658 testing words. With the threshold=0.7, our method clustered 830 words, and only 167 words of them were clustered in the correct Cilin class. Therefore, by Equation ( 5 ), recall 4 = 167/658 = 0.25 and with Equation ( 6 ) precision 4 = 167/830 = 0.20. We adopted two methods for measuring similarity; one used Equation TF × IDF, and the other used Equation TF. The results are shown in Figure 3 to Figure 6 in the Appendix. We measured the performance by computing the F -score, which is (recall+precision)/2. We discovered that the best F-score of level1 was that 0.7648 located at a threshold equal to 0.65, the best F-score of level2 was that 0.5178 located at a threshold equal to 0.7, the best F-score of level3 was that 0.3165 located at a threshold equal to 0.8, and that the best F-score of level4 was that 0.2476 located at a the threshold equal to 0.8. All were obtained using TF × IDF strategy. Hence, we can see that the TF × IDF equation achieves better performance than the TF equation does. We list the detailed F-socre data for various parameters in appendix. Although the clustering results didn't fit Cilin completely, they are still alike to some degree. From the results, we find that they are similar to syntax taxonomy under a lower threshold and close to semantic taxonomy under higher thresholds. Cilin classifications re -examined To examine the practicability of our proposed method, we also inspected the similarity values of these 658 testing words which were clustered into 117 4 th level Cilin classes. For each semantic class, the average similarity between words in the class and their standard deviation was comp uted. The results are listed in Table1 in the Appendix. We expected that synonyms would have high similarity values, but this was not always the case. According to the assumption noted above, synonyms might have similar syntactic and semantic contexts in language use. Therefore, the average similarity should be pretty high, and the standard deviation should be quite low. However, some of the results didn't follow the assumption. We analyzed the data offer explanations in the following. The context s of the noun (思想, ""thinking"") and the verbs (考慮,考量,思考, ""think"", ""consider"", ""deliberate"") were quite different. As a result, the average similarity value was quite low, and the standard deviation was very high. A fter we remo ved the noun from the word-set, we recomputed the values and obtained the The results conform to our assumption. They also reveal that the context of synonyms may vary from POS to POS. b) Error in Cilin Classification: The classifications in Cilin could be arbitrary. For example, the three words, 數量 ""quantity"", 多少 ""how many"" and 人數 ""the number of people"", were classified in a Cilin group. They might be slightly related, but grouping them together seems inappropriate according to the following table : Word set Average similarity Stand deviation 數量,多少 人數 0.379825 0.253895 c) Different uses: Differences in their usage cause synonyms to behave differently . For example, when we measured the similarity of 美國 ""America"" to 日本 ""Japan"" and to 中國 ""China"", the results we obtained were 0.86 and 0.62,respectively, for each pair. Accroding to human intuition, they simply refer to names of countries and should not have such different similarity values. The reason for these result is that the corpus we adopted is an original Taiwan corpus. As a result, the usage of 中國 ""China"" is different from that of 美國 ""America"" and 日本 ""Japan"". d) Polysemy: The word senses that Cilin adopted were not those frequently used in the corpus. See the following table : . Word set Average similarity Stand deviation 十分,非常, 特別 0.45054 0.305209 Although the three words, 十分 ""very/ten points"", 非常 ""very"" and 特 別""special, extraordinary"" might seem to be very close in meaning to ""very"", the polysemous word 特別 ""special, extraordinary"" is different in its major sense. This influenced the result. e) Words with similar contexts might not be synonyms: A disadvantage does exist when the context vector model is used. Words that are similar in terms of their context s might not be similar in meaning. For example, the similarity value of 結婚 ""marry"" and 長大 ""grow"" is 0.8139. Although the two words have similar contexts, they are not alike in meaning. Therefore, the vector space model should incorporate the taxonomy approach to solve this phenomenon. Conclusions In this paper, we have adopted the context vector model to measure word similarity. The following new features have been proposed to enhance the context vector models : a) The weights of features are adjusted by applying TF × IDF. b) Feature vectors are smoothed by using Cilin categories to reduce data sparseness. c) Syntactic and semantic similarity is balanced by using related syntactic contexts only. The performance of our method might have been influenced by the small scale of the Chinese corpus and accuracy of the extracted relations. Further more, Cilin was published a long time ago and has not been update recently, which may have influenced our results. However, our experimental results are encouraging. They supports the theory that using context vectors to measure similarity is feasible and worthy of further research. Appendix",18022704,0b09178ac8d17a92f16140365363d8df88c757d0,14,https://aclanthology.org/O02-2002,,,2002,August,"International Journal of Computational Linguistics {\&} {C}hinese Language Processing, Volume 7, Number 2, August 2002: Special Issue on Computational {C}hinese Lexical Semantics","Chen, Keh-Jiann  and
You, Jia-Ming",A Study on Word Similarity using Context Vector Models,37--58,,,,,,,inproceedings,chen-you-2002-study,,,
1,L02-1310,,,8220988,8d5e31610bc82c2abc86bc20ceba684c97e66024,93,http://www.lrec-conf.org/proceedings/lrec2002/pdf/310.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Mihalcea, Rada F.",Bootstrapping Large Sense Tagged Corpora,,,,,,,,inproceedings,mihalcea-2002-bootstrapping,,,
2,R13-1042,"Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus.","Thread disentanglement is the task of separating out conversations whose thread structure is implicit, distorted, or lost. In this paper, we perform email thread disentanglement through pairwise classification, using text similarity measures on non-quoted texts in emails. We show that i) content text similarity metrics outperform style and structure text similarity metrics in both a class-balanced and class-imbalanced setting, and ii) although feature performance is dependent on the semantic similarity of the corpus, content features are still effective even when controlling for semantic similarity. We make available the Enron Threads Corpus, a newly-extracted corpus of 70,178 multiemail threads with emails from the Enron Email Corpus. Introduction Law enforcement agencies frequently obtain large amounts of electronic messages, such as emails, which they must search for evidence. However, individual messages may be useless without the conversational context they occur in. Most modern emails contain useful metadata such as the MIME header In-Reply-To, which marks relations between emails in a thread and can be used to disentangle threads. However, there are easy methods of obfuscating email threads: opening an email account for a single purpose; using multiple email accounts for one person; sharing one email account among multiple persons; changing the Subject header; and removing quoted material from earlier in the thread. How can emails be organized by thread without metadata such as their MIME headers? We propose to use text similarity metrics to identify emails belonging to the same thread. In this paper, as a first step for temporal thread disentanglement, we perform pairwise classification experiments on texts in emails using no MIME headers or quoted previous emails. We have found that content-based text similarity metrics outperform a Dice baseline, and that structural and style text similarity features do not; adding these latter feature groups does not significantly improve total performance. We also found that contentbased features continue to outperform the others in both a class-balanced and class-imbalanced setting, as well as with semantically controlled or non-controlled negative instances. In NLP, Elsner and Charniak (2010) described the task of thread disentanglement as ""the clustering task of dividing a transcript into a set of distinct conversations,"" in which extrinsic thread delimitation is unavailable and the threads must be disentangled using only intrinsic information. In addition to emails with missing or incorrect MIME headers, entangled electronic conversations occur in environments such as interspersed Internet Relay Chat conversations, web 2.0 article response conversations that do not have a hierarchical display order, and misplaced comments in Wiki Talk discussions. Research on disentanglement of conversation threads has been done on internet relay chats (Elsner and Charniak, 2010) , audio chats (Aoki et al., 2003) , and emails with headers and quoted material (Yeh, 2006; Erera and Carmel, 2008) . However, to the best of our knowledge, no work has investigated reassembling email threads without the help of MIME headers or quoted previous emails. Previous researchers have used a number of email corpora with high-precision (non-Subject-clustered) thread marking. Joti et al. (2010) used the BC3 corpus of 40 email threads and 3222 emails for topic segmentation. Carenini et al. (2008) annotated 39 email ""conversations"" from the Enron Email Corpus for email summariation. Wan and McKeown (2004) used a privatelyavailable corpus of 300 threads for summary generation. Rambow et al. (2004) used a privatelyavailable corpus of 96 email threads for thread summarization. Data The Enron Email Corpus (EEC) 1 consists of the 517,424 emails (159 users' accounts and 19,675 total senders) that existed on the Enron Corporation's email server (i.e., other emails had been previously deleted, etc) when it was made public . Gold Standard Thread Extraction from the Enron Email Corpus We define an email thread as a directed graph of emails connected by Reply and Forward relations. In this way, we attempt to identify email discussions between users. However, the precise definition of an email thread actually depends on the implementation that we, or any other researchers, used to identify the thread. Previous researchers have derived email thread structure from a variety of sources. Wu and Oard (2005) , and Zhu et al. (2005) auto-threaded all messages with identical, non-trivial, Fwd: and Re:-stripped Subject headers. Klimt and Yang (2004) auto-threaded messages that had stripped Subject headers and were among the same users (addresses). Lewis and Knowles (1997) assigned emails to threads by matching quotation structures between emails. Wan and McKeown (2004) reconstructed threads by header Message-ID information. Rambow et al. (2004) used a privately-available corpus of 96 email threads, but did not specify how they determined the threads. As the emails in the EEC do not contain any inherent thread structure, it was necessary for us to create email threads. First, we implemented Klimt and Yang (2004) 's technique of clustering the emails into threads that have the same Subject header (after it has been stripped of pre-fixes such as Re: and Fwd:) and shared participants. To determine whether emails were among the same users, we split a Subject-created email proto-thread apart into any necessary threads, such that the split threads had no senders or recipients (including To, CC, and BCC) in common. The resulting email clusters had a number of problems. Clusters tended to over-group, because a single user included as a recipient for two different threads with the Subject ""Monday Meeting"" would cause the threads to be merged into a single cluster. In addition, many clusters consisted of all of the issues of a monthly subscription newsletter, or nearly identical petitions (see Klimt and Yang (2004) 's description of the ""Demand Ken Lay Donate Proceeds from Enron Stock Sales"" thread), or an auto-generated log of Enron computer network problems auto-emailed to the Enron employees in charge of the network. Such clusters of ""broadcast"" emails do not satisfy our goal of identifying email discussions between users. Many email discussions between users exist in previously quoted emails auto-copied at the bottom of latter emails of the thread. A singleannotator hand-investigation of 465 previously quoted emails from 20 threads showed that none of them had interspersed comments or had otherwise been altered by more recent thread contributors. Threads in the EEC are quoted multiple times at various points in the conversation in multiple surviving emails. In order to avoid creating redundant threads, which would be an information leak risk during evaluation, we selected as the thread source the email from each Klimt and Yang (2004) cluster with the most quoted emails, and discarded all other emails in the cluster. We used the quoteidentifying regular expressions from Yeh (2006) (see Table 1 ) to identify quoted previous emails. 2  There are two important benefits to the creation methodology of the Enron Threads Corpus 3 . First, since the emails were extracted from the same document, and the emails would only have been included in the same document by the email client if one was a Reply or Forward of the other, precision is very high (approaching 100%). 4 This is  better precision than threads clustered from separate email documents, which may have the same Subject, etc. generating false positives. Some emails will inevitably be left out of the thread, reducing recall, because they were not part of the thread branch that was eventually used to represent the thread, or simply because they were not quoted. Our pairwise classification experiments, described in Section 4, are unaffected by this reduced recall, because each experimental instance includes only a pair of emails, and not the entire thread. Second, because the thread source did not require human annotation, using quoted emails gives us an unprecedented number of threads as data: 209,063 emails in 70,178 threads of two emails or larger. The sizes of email threads in the Enron Threads Corpus is shown in Table 2 . Emails have an average of 80.0±201.2 tokens, and an average count of 4.4±9.3 sentences. Many of the emails are quite short: 18% are under 10 tokens, 19% are 10-20 tokens, and 13% are 20-30 tokens. Text Similarity Features We cast email thread disentanglement as a text similarity problem. Ideally, there exists a text similarity measure that marks pairs of emails from the system misidentified about 1% of emails from regular expression error. same thread as more similar than pairs of emails from different threads. We evaluate a number of text similarity measures, divided according to Bär et al. (2011) 's three groups: Content Similarity, Structural Similarity, Style Similarity. Each set of features investigates a different manner in which email pairs from the same thread may be identified. In our experiments, all features are derived from the body of the email, while all headers such as Recipients, Subject, and Timestamp are ignored. Content features. Content similarity metrics capture the string overlap between emails with similar content. A pair of emails with a high content overlap is shown below. The Longest Common Substring measure (Gusfield, 1997) identifies uninterrupted common strings, while the Longest Common Subsequence measure (Allison and Dix, 1986 ) and the singletext-length-normalized Longest Common Subsequence Norm measure identify common strings containing interruptions and text replacements and Greedy String Tiling measure (Wise, 1996) allows reordering of the subsequences. Other measures which treat texts as sequences of characters and compute similarities with various metrics include Levenshtein (1966) , Monge Elkan Second String measure (Monge and Elkan, 1997) , Jaro Second String measure (Jaro, 1989) , and Jaro Winkler Second String measure (Winkler, 1990) . A Cosine Similarity-type measure was used, based on term frequency within the document. Sets of ngrams from the two emails are compared using the Jaccard coefficient (from Lyon et al. (2004) ) and Broder's (1997) Containment measure. Structural features. Structural features attempt to identify similar syntactic patterns between the two texts, while overlooking topicspecific vocabulary. We propose that sturctural features, as well as style features below, may help in classification by means of communication accommodation theory (Giles and Ogay, 2007 ). Stamatatos's Stopword n-grams (2011) capture syntactic similarities, by identifying text reuse where just the content words have been replaced and the stopwords remain the same. We measured the stopword n-gram overlap with Broder's (1997) Containment measure and four different stopword lists. We also tried the Containment measure and an NGram Jaccard measure with part-of-speech tags. Token Pair Order (Hatzivassiloglou et al. 1999 ) uses pairs of words occurring in the same order for the two emails; Token Pair Distance (Hatzivassiloglou et al., 1999) measures the distance between pairs of words. Both measures use computed feature vectors for both emails along all shared word pairs, and the vectors are compared with Pearson correlation. Style features. Style similarity reflects authorship attribution and surface-level statistical properties of texts. Type Token Ratio (TTR) measure calculates text-length-sensitive and text-homogeneitysensitive vocabulary richness (Templin, 1957) . However, as this measure is sensitive to differences in document length between the pair of documents (documents become less lexically diverse as length and token count increases but type count levels off), and fluctuating lexical diversity as rhetorical strategies shift within a single document, we also used Sequential TTR (McCarthy and Jarvis, 2010), which corrects for these problems. Sentence Length and Token Length (inspired by (Yule, 1939) ) measure the average number of tokens per sentence and characters per token, respectively. Sentence Ratio and Token Ratio compare Sentence Length and Token Length between the two emails (Bär et al., 2011) . Function Word Frequencies is a Pearson's correlation between feature vectors of the frequencies of 70 pre-identified function words from Mosteller and Wallace (1964) across the two emails. We also compute Case Combined Ratio, showing the percentage of UPPERCASE characters in both emails combined ( U P P ERCASE e1 +U P P ERCASE e2 ALLCHARS e1 +ALLCHARS e2 ), and Case Document similarity, showing the similarity between the percentage of UPPERCASE characters in one email versus the other email. Evaluation In this series of experiments, we evaluate the effectiveness of different feature groups to classify pairs of emails as being from the same thread (positive) or not (negative). Each instance to be classified is represented by the features from a pair of emails and the instance classification, positive or negative. We used a variation of K-fold cross-validation for evaluation. The 10 folds contained carefully distributed email pairs such that email pairs with emails from the same thread were never used in pairs of training, development, and testing sets, to avoid information leakage. All instances were at one point in a test set. Instance division was roughly 80% training, 10% development, and 10% test data. Reported results are the weighted averages across all folds. The evaluation used logistic regression, as implemented in Weka (Hall et al., 2009) . Default parameters were used. We use a baseline algorithm of Dice Similarity between the texts of the two emails as a simple measure of set similarity. We created an upper bound by annotating 100 positive and 100 negative instances. A single native English speaker annotator answered the question, ""Are these emails from the same thread?"" Data Sampling Although we had 413,814 positive instances available in the Enron Threads Corpus, we found that classifier performance was unaffected by the amount of training data, down to very low levels (see Figure 1 ). However, because the standard deviation in the data did not level out until around 1,200 class-balanced training instances 5 , we used this number of positive instances (600) in each of our experiments. In order to estimate effectiveness of features for different data distributions, we used three different subsampled datasets. Random Balanced (RB) Dataset. The first dataset is class-balanced and uses 1200 training instances. Minimum email length is one word. For every positive instance we used, we created a negative email pair by taking the first email from the positive pair and pseudo-randomly pairing it with another email from a different thread that was assigned to the same training, development, or test set. However, the probability of semantic similarity between two emails in a positive instance is much greater than the probability of semantic similarity between two emails in a randomly-created negative instance. The results of experiments on our first dataset reflect both the success of our text similarity metrics and the semantic similarity (i.e., topical distribution) within our dataset. The topical distribution will vary immensely between different email corpora. To investigate the performance of our features in a more generaliable environment, we created a subsample dataset that con- trolls for semantic similarity within and outside of the email thread. Semantically Balanced (SB) Dataset. The second dataset combines the same positive instances as the first set with an equal number of semantically-matched negative instances for a training size of 1200 instances, and a minimum email length of one word. For each positive instance, we measured the semantic similarity within the email pair using Cosine Similarity and then created a negative instance with the same (±.005) similarity. Emails had an average of 96±287 tokens and 5±11sentences, and a similar token size distribution as SB. Random Imbalanced (RI) Dataset. However, both the RB and SB datasets use a class-balanced distribution. To see if our features are still effective in a class-imbalanced environment, we created a third dataset with a 90% negative, 10% positive distribution for both the training and test sets 6 . Specifically, we used the first dataset and then added an extra 8 negative instances for each positive instance. Experiments with this dataset use 10-fold cross validation, where each fold has 6000 training and 750 test instances. No minimum email length was used, similar to a more natural distribution. Results Our results are shown in Table 3 . Since we aim to detect pairs of emails belonging to the same thread rather than unrelated emails, we measure the system performance on the positive class. We use the standard F-measure of F 1 = 2×P (pos)×R(pos) P (pos)+R(pos) . As a measure to show performance on both positive and negative classes, we provide a standard accuracy measure of Acc= T P +T N T P +F N +T N +F P . Feature groups are shown in isolation as well as the complete set of features minus one group. 7  With the RB corpus, the best performing single feature configuration, content features group (P=.83 ±.04), matches the human upper bound precision(P=.84). The benefit of content features is confirmed by the reductions in complete feature set performance when they are left out. The content features group was the only group to perform significantly above the Dice baseline. Adding the other feature groups does not significantly improve the overall results. Further leave-one-out experiments revealed no single high performing feature within the content features group. Structural features produced low performance, failing to beat the Chance baseline. Structural similarity from rhetorical strategy is rare in an email conversational setting. Any structural benefits are likely to come from sources unavailable in a disguised email situation, such as auto-signatures identifying senders as the same person. The low results on structural features show that we are not relying on such artifacts for classification. Style features were also unhelpful, failing to significantly beat the Dice baseline. The features failed to identify communication accomodation within the thread. Results on the SB dataset show that there is a noticeable drop in classification for all feature groups when negative instances have a similar semantic similarity as positive instances. The configuration with all features showed a 15 percentage point drop in precision, and a 12 percentage point drop in accuracy. However, content features continues to be the best performing feature group with semantically similar negative instances, as with random negative instances, and outperformed the Dice baseline. Adding the additional feature groups does not significantly improve overall performance. The results on the RI corpus mirror results from the balanced (RB) corpus. 2011 ) use coherence models to disentangle chat, using some features (entity grid, topical entity grid) which correspond to the information in our content features group. They also found these content-based features to be helpful. Inherent limitations Certain limitations are inherent in email thread disentanglement. Some email thread relations cannot be detected with text similarity metrics, and require extensive discourse knowledge, such as the emails below. Email1: Can you attend the Directors Fund Equity Board Meeting next Wednesday, Nov 5, at 3pm? Email2: Yes, I will be there. Several other problems in email thread disentanglement cannot be solved with any discourse knowledge. One problem is that some emails are identical or near-identical; there is no way to choose between textually identical emails. Table 4 shows some of the most common email texts in our corpus, based on a <.05 similarity value from Jaro Second String similarity, as described in Section 3. However, near identical texts make up only a small portion of the emails in our corpus. In a sample of 5,296 emails, only 3.6% of email texts were within a .05 Jaro Second String similarity value of another text. Another problem is that some emails are impossible to distinguish without world and domain knowledge. Consider a building with two meeting rooms: A101 and A201. Sometimes A101 is used, and sometimes A201 is used. In response to the question, Which room is Monday's meeting in?, there may be no way to choose between A101 and A201 without further world knowledge. Another problem is topic overlap. For example, in a business email corpus such as the EEC, there are numerous threads discussing Monday morning 9am meetings. The more similar the language used between threads, the more difficult the disentanglement becomes, using text similarity. This issue is addressed with the SB dataset. Finally, our classifier cannot out-perform humans on the same task, so it is important to note human limitations in email disentanglement. Our human upper bound is shown in Table 3 . We will further address this issue in Sections 4.4. Error Analysis We inspected 50 email pairs each of true positives, false positives, false negatives, and true negatives from our RB experiments 8 . We inspected for both technical details likely to affect classification, and for linguistic features to guide future research. Technical details included small and large text errors (such as unidentified email headers or incorrect email segmentation), custom and noncustom email signatures, and the presense of large signatures likely to affect classification. Linguistic features included an appearance of consecutivity (emails appear in a Q/A relation, or one is informative and one is 'please print', etc.), similarity of social style (""Language vocab level, professionalism, and social address are a reasonable match""), and the annotator's perception that the emails could be from the same thread. An example of a text error is shown below. Names and dates occur frequently in legitimate email text, such as meeting attendance lists, etc., which makes them difficult to screen out. Emails from false positives were less likely to contain these small errors (3% versus 14%), which implies that the noise introduced from the extra text has more impact than the false similarity potentially generated by similar text errors. Large text errors (such as 2 emails labelled as one) occurred in only 1% of emails and were too rare to correlate with results. Autosignatures, such as the examples below, mildly impacted classification. Instances classified as negative (both FN and TN) were marginally more likely to have had one email with a non-customized autosignature (3% versus 1.5%) or a customized auto-signature (6.5% versus 3.5%). Autosignatures were also judged likely to affect similarity values more often on instances classified as negative (20% of instances). The presence of the autosignature may have introduced enough noise for the classifier to decide the emails were not similar enough to be from the same thread. We define a non-custom auto-signature as any automatically-added text at the bottom of the email. We did not see enough instances where both emails had an autosignature to evaluate whether similarities in autosignatures (such as a common area code) impacted results. Some email pair similarities, observable by humans, are not being captured by our text similarity features. Nearly all (98%) positive instances were recognized by the annotator as potential consecutive emails within a thread, or non-consecutive emails but still from the same thread, whereas only 46% of negative instances were similarly (falsely) noted. Only 2% of negative instances were judged to look like they were consecutive emails within the same thread. The following TP instance shows emails that look like they could be from the same thread but do not look consecutive. Below is a TN instance with emails that look like they could be from the same thread but do not look consecutive. Email1: i do but i havent heard from you either, how are things with wade Email2: rumor has it that a press conference will take place at 4:00 -more money in, lower conversion rate. The level of professionalism (""Language vocab level, professionalism, and social address are a reasonable match"") was also notable between class categories. All TP instances were judged to have a professionalism match, as well as 94% of FN's. However, only 64% of FP's and 56% of TN's were judged to have a professionalism match. Based on a review of our misclassified instances, we are surprised that our classifier did not learn a better model based on style features (F 1 =.60). Participants in an email thread appear to echo the style of emails they reply to. For instance, short, casual, all-lowercase emails are frequently responded to in a similar manner. Conclusion In this paper, we have described the creation of the Enron Threads Corpus, which we made available online. We have investigated the use of text similarity features for the pairwise classification of emails for thread disentanglement. We have found that content similarity features are more effective than style or structural features across class-balanced and class-imbalanced environments. There appear to be more stylistic features uncaptured by our similarity metrics, which humans access for performing the same task. We have shown that semantic differences between corpora will impact the general effectiveness of text similarity features, but that content features remain effective. In future work, we will investigate discourse knowledge, highly-tuned stylistic features, and other email-specific features to improve headerless, quoteless email thread disentanglement. Acknowledgments This work has been supported by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under grant No. I/82806, and by the Center for Advanced Security Research (www.cased.de).",16703040,3eb736b17a5acb583b9a9bd99837427753632cdb,10,https://aclanthology.org/R13-1042,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Jamison, Emily  and
Gurevych, Iryna","Headerless, Quoteless, but not Hopeless? Using Pairwise Email Classification to Disentangle Email Threads",327--335,,,,,,,inproceedings,jamison-gurevych-2013-headerless,,,Computational Social Science and Social Media
3,W05-0819,"In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on ""Building and using parallel texts: data driven machine translation and beyond"". Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data.","In this paper, we describe a word alignment algorithm for English-Hindi parallel data. The system was developed to participate in the shared task on word alignment for languages with scarce resources at the ACL 2005 workshop, on ""Building and using parallel texts: data driven machine translation and beyond"". Our word alignment algorithm is based on a hybrid method which performs local word grouping on Hindi sentences and uses other methods such as dictionary lookup, transliteration similarity, expected English words and nearest aligned neighbours. We trained our system on the training data provided to obtain a list of named entities and cognates and to collect rules for local word grouping in Hindi sentences. The system scored 77.03% precision and 60.68% recall on the shared task unseen test data. Introduction This paper describes a word alignment system developed as a part of shared task on word alignment for languages with scarce resources at the ACL 2005 workshop on ""building and using parallel texts: data driven machine translation and beyond"". Participants in the shared task were provided with common sets of training data, consisting of English-Inuktitut, Romanian-English, and English-Hindi parallel texts and the participating teams could choose to evaluate their system on one, two, or all three language pairs. Our system is for aligning English-Hindi parallel data at the word level. The word-alignment algorithm described here is based on a hybridmulti-feature approach, which groups Hindi words locally within a Hindi sentence and uses dictionary lookup (DL) as the main method of aligning words along with other methods such as Transliteration Similarity (TS), Expected English Words (EEW) and Nearest Aligned Neighbors (NAN). We used the training data supplied to derive rules for local word grouping in Hindi sentences and to find Named Entities (NE) and cognates using our TS approach. In the following sections we briefly describe our approach. Training Data The training data set was composed of approximately 3441 English-Hindi parallel sentence pairs drawn from the EMILLE (Enabling Minority Language Engineering) corpus (Baker et al., 2004) . The data was pre-tokenized. For the English data, a token was a sequence of characters that matches any of the ""Dr."", ""Mr."", ""Hon."", ""Mrs."", ""Ms."", ""etc."", ""i.e."", ""e.g."", ""[a-zA-Z0-9]+"", words ending with apostrophe and all special characters except the currency symbols £ and $. Similarly for the Hindi, a token consisted of a sequence of characters with spaces on both ends and all special characters except the currency symbols £ and $. Word Alignment Given a pair of parallel sentences, the task of word alignment can be described as finding one-to-one, one-to-many, and many-to-many correspondences between the words of source and target sentences. It becomes more complicated when aligning phrases of one language with the corresponding words or phrases in the target language. For some words, it is also possible not to find any translation in the target language. Such words are aligned to null. The algorithm presented in this paper, is a blend of various methods. We categorize words of a Hindi sentence into one of four different categories and use different techniques to deal with each of them. These categories include: 1) NEs and cognates 2) Hindi words for which it is possible to predict their corresponding English words 3) Hindi words that match certain pre-specified regular expression patterns specified in a rule file (explained in section 3.3.) and finally 4) words which do not fit in any of the above categories. In the following sections we explain different methods to deal with words from each of these categories. Named Entities and Cognates According to WWW1, the Named Entity Task is the process of annotating expressions in the text that are ""unique identifiers"" of entities (e.g. Organization, Person, Location etc.). For example: ""Mr. Niraj Aswani"", ""United Kingdom"", and ""Microsoft"" are examples of NEs. In most text processing systems, this task is achieved by using local pattern-matching techniques e.g. a word that is in upper initial orthography or a Title followed by the two adjacent words that are in upper initial or in all upper case. We use a Hindi gazetteer list that contains a large set of NEs. This gazetteer list is distributed as a part of Hindi Gazetteer processing resource in GATE (Maynard et al., 2003) . The Gazetteer list contains various NEs including person names, locations, organizations etc. It also contains other entities such as time units -months, dates, and number expressions. Cognates can be defined as two words having a common etymology and thus are similar or identical. In most cases they are pronounced in a similar way or with a minor change. For example ""Bungalow"" in English is derived from the word ""बं गला"" in Hindi, which means a house in the Bengali style (WWW2). We use our TS method to locate such words. Section 3.2 describes the TS approach. Transliteration Similarity For the English-Hindi alphabets, it is possible to come up with a table consisting of correspondences between the letters of the two alphabets. This table is generated based on the various sounds that each letter can produce. For example a letter ""c"" can be mapped to two letters in Hindi, ""क"" and ""स"". This mapping is not restricted to one-to-one but also includes many-tomany correspondences. It is also possible to map a sequence of two or more characters to a single character or to a sequence two or more characters. For example ""tio"" and ""sh"" in English correspond to the character ""श"" in Hindi. Prior to executing our word alignment algorithm, we use the TS approach to build a table of NEs and cognates. We consider one pair of parallel sentences at a time and for each word in a Hindi sentence, we generate different English words using our TS table. We found that before comparing words of two languages, it is more accurate to eliminate vowels from the words except those that appear at the start of words. We use a dynamic programming algorithm called ""edit-distance"" to measure the similarity between these words (WWW3). We calculate the similarity measure for each word in a Hindi sentence by comparing it with each and every word of an English sentence. We come up with an m x n matrix, where m and n refer to the number of words in Hindi and English respectively. This matrix contains a similarity measure for each word in a Hindi sentence corresponding to each word in a parallel English sentence. From our experiments of comparing more than 100 NE and cognate pairs, we found that the word pairs should be considered valid matches only if the similarity is greater than 75%. Therefore, we consider only those pairs which have the highest similarity among the other pairs with similarity greater than 75%. The following example shows how TS is used to compare a pair of English-Hindi words. For example consider a pair ""aswani अ।सवानी"" and the TS table entries as shown below: A अ, S स, SS स, V व, W व and N न We remove vowels from both words: ""aswn असवन"", and then convert the Hindi word into possible English words. This gives four different combinations: ""asvn"", ""assvn"", ""aswn"" and ""asswn"". These words are then compared with the actual English word ""aswn"". Since we are able to locate at least one word with similarity greater than 75%, we consider ""aswani अ।सवानी"" as a NE. Once a list of NEs and cognates is ready, we switch to our next step: local word grouping, where all words in Hindi sentences, either those available in the gazetteer list or in the list derived using TS approach, are aligned using TS approach. Local Word Grouping Hindi is a partially free order language (i.e. the order of the words in a Hindi sentence is not fixed but the order of words in a group/phrase is fixed). Unlike English where the verbs are used in different inflected forms to indicate different tenses, Hindi uses one or two extra words after the verb to indicate the tense. Therefore, if the English verb is not in its base form, it needs to be aligned with one or more words in a parallel Hindi sentence. Sometimes a phrase is aligned with another phrase. For example ""customer benefits"" aligns with "" ाहक के फायदे "". In this example the first word ""customer"" aligns with the first word "" ाहक"" and the second word ""benefits"" aligns with the third word ""फायदे "". Considering ""customer satisfaction"" and "" ाहक के फायदे "" as phrases to be aligned with each other, ""के "" is the word that indicates the relation between the two words "" ाहक"" and ""फायदे "", which means the ""benefits of customer"" in English. These words in a phrase need to be grouped together in order to align them correctly. In the case of certain prepositions, pronouns and auxiliaries, it is possible to predict the respective Hindi postpositions, pronouns and other words. We derived a set of more than 250 rules to group such patterns by consulting the provided training data and other grammar resources such as Bal Anand (2001) . The rule file contains the following information for each rule: 1) Hindi Regular Expression for a word or phrase. This must match one or more words in the Hindi sentence. 2) Group name or a part-of-speech category. 3) Expected English word(s) that this Hindi word group may align to. 4) In case a group of one or more English words aligns with a group of one or more Hindi words, information about the key words in both groups. Key words must match each other in order to align English-Hindi groups. 5) A rule to convert Hindi word into its base form. We list some of the derived rules below: 1) Group a sequence of [X + Postposition], where X can be any category in the above list except postposition or verb. For example: ""For X"" = ""X के िलये "", where ""For"" = ""के िलये "". 2) Root Verb + (रहा, रही or रहे ) + (PH). Present continuous tense. We use ""PH"" as an abbreviation to refer to the present/past tense conjunction of the verb ""होना"" -ं , ह , है , हो, etc. 3) Group two words that are identical to each other. For example: ""अलग अलग"", which means ""different"" in English. Such bi-grams are common in Hindi and are used to stress the importance of a word/activity in a sentence. Once the words are grouped in a Hindi sentence, we identify those word groups which do not fit in any of the TS and EEW categories. Such words are then aligned using the DL approach. Dictionary lookup Since the most dictionaries contain verbs in their base forms, we use a morphological analyzer to convert verbs in their base forms. The English-Hindi dictionary is obtained from (WWW4). The dictionary returns, on average, two to four Hindi words referring to a particular English word. The formula for finding the lemma of any Hindi verb is: infinitive = root verb + ""ना"". Since in most cases, our dictionary contains Hindi verbs in their infinitive forms, prior to comparing the word with the unaligned words, we remove the word ""ना"" from the end of it. Due to minor spelling mistakes it is also possible that the word returned from dictionary does not match with any of the words in a Hindi sentence. In this case, we use edit-distance algorithm to obtain similarity between the two words. If the similarity is greater than 75%, we consider them similar. We use EEW approach for the words which remain unaligned after the DL approach. Expected English words Candidates for the EEW approach are the Hindi word groups (HWG) that are created by our Hindi local word grouping algorithm (explained in section 3.3). The HWGs such as postpositions, number expressions, month-units, day-units etc. are aligned using the EEW approach. For example, for the Hind word ""बावन"" in a Hindi sentence, which means ""fifty two"" in English, the algorithm tries to locate ""fifty two"" in its parallel English sentence and aligns them if found. For the remaining unaligned Hindi words we use the NAN approach. Nearest Aligned Neighbors In certain cases, words in English-Hindi phrases follow a similar order. The NAN approach works on this principle and aligns one or more words with one of the English words. Considering one HWG at a time, we find the nearest Hindi word that is already aligned with one or more English word(s). Aligning a phrase ""customer benefits"" with "" ाहक के फायदे "" (example explained in section 3.3) is an example of NAN approach. Similarly consider a phrase ""tougher controls"", where for its equivalent Hindi phrase ""अिधक िनयं ण"", the dictionary returns a correct pair ""controls िनयं ण"", but fails to locate ""tougher अिधक"". For aligning the word ""tougher"", NAN searches for the nearest aligned word, which, in this case, is ""controls"". Since the word ""controls"" is already aligned with the word ""िनयं ण"", the NAN method aligns the word ""tougher"" with the nearest unaligned word ""अिधक"". Test Data results We executed our algorithm on the test data consisting of 90 English-Hindi sentence pairs. We obtained the following results for non-null alignment pairs.",1215281,b20450f67116e59d1348fc472cfc09f96e348f55,15,https://aclanthology.org/W05-0819,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Aswani, Niraj  and
Gaizauskas, Robert",Aligning Words in {E}nglish-{H}indi Parallel Corpora,115--118,,,,,,,inproceedings,aswani-gaizauskas-2005-aligning,,,
4,L02-1309,,,18078432,011e943b64a78dadc3440674419821ee080f0de3,12,http://www.lrec-conf.org/proceedings/lrec2002/pdf/309.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Suyaga, Fumiaki  and
Takezawa, Toshiyuki  and
Kikui, Genichiro  and
Yamamoto, Seiichi",Proposal of a very-large-corpus acquisition method by cell-formed registration,,,,,,,,inproceedings,suyaga-etal-2002-proposal,,,
5,R13-1044,"The paper 1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: first the system recognizes word pairs and triples, and then it classifies the relations. Evaluation was performed on random samples from two balanced Polish corpora.","The paper 1 presents a rule-based approach to semantic relation recognition within the Polish noun phrase. A set of semantic relations, including some thematic relations, has been determined for the need of experiments. The method consists in two steps: first the system recognizes word pairs and triples, and then it classifies the relations. Evaluation was performed on random samples from two balanced Polish corpora. Introduction Semantic relation recognition is a well-known task in natural language processing. Although the relation recognition within noun phrase and between nominals was studied intensely, the task is still challenge for semantic analysis of Polish. We are aware of few papers and projects dealing with Semantic Role Labelling between predicates and their arguments, cf. (Gołuchowski and Przepiórkowski, 2012) or (Lun, 2009) , but of none concerning semantic relation recognition inside Polish noun phrase. Related work In (Nastase et al., 2006) authors classify semantic relations between a head and a modifier of a noun phrase. Number of all relation types was equal to 30. These relations were grouped into 5 more general groups. The authors experimented with decision trees, instance-based learning and Support Vector Machines. For each relation they learnt the binary classifier; as the baseline for F-measure they used the model with all of examples classified as positive and recall being equal to 100%. With regard to the semantic relation the baseline ranged between 17.78% and 60.35%. Identifying the semantic relations inside compound nouns was presented in (Uchiyama et al., 2008) . The authors used SVM classifier and in the best configuration of features, they achieved accuracy of about 84%. In (Rosario and Hearst, 2001) authors used neural networks to determine 20 semantic relationssimilarily to (Nastase et al., 2006) -between a head and a modifier of noun phrase. They used a domain-specific lexical hierarchy of medicine. The authors achieved accuracy of about 60%. The workshop SemEval-2010 (task 8) concerned the recognition of semantic relations between nominals. In (Tratz and Hovy, 2010) the authors developed a system based on the Maximum Entropy classifier, able to detect 10 bidirectional semantic relations Achieved F-measures depended on the system configuration and lay between 66, 68% and 77, 75%. The same set of semantic relations was used in (Rink and Harabagiu, 2010) . The authors used Support Vector Machines classifier and a very rich set of features (i.e., part of speech for all constituents of a semantic relation pair, number of words between the nominals, features based on paths in the dependency tree from Stanford dependency parser). F-measure of this approach was 82.19%. Authors in (Tymoshenko and Giuliano, 2010 ) used shallow syntactic parsing and semantic information from ResearchCyc (Lenat, 1995) in the same task of recognizing semantic relations. They used liner combination of kernels (semantic and syntactic) using Support Vector Machines classifier. For the best combination of kernels, they obtained F-measure equal to 77.62%. There are some works, where rule-based approaches were used. In (Huang, 2009) there has been proposed an approach for automatic construction of rules identifying ten types of seman-tic relations, using five types of input informations. The relation instances were extracted from Modern Chinese Standard Dictionary. The authors achieved very high precision (range from 0, 81 to 0.99), but recall was low -about 0, 2. In (Hearst, 1992) authors used set of manually written rules for identification of hyperonymy relations. (Ben Abacha and Zweigenbaum, 2011) used linguistic patterns (built semi-automatically from corpora) to identify semantic relatios in medical texts. In this domain-specific task they achieved 75.72% precision and 60, 46% recall. Recognized semantic relation types We seek for semantic relations within nominal phrases. The relation set consists of 12 semantic relations, of which 5 are thematic (semantic) roles 2 . Definitions of our semantic relations are based on works of (Kearns, 2011), (Palmer et al., 2010) , (Van Valin, 2004) , (Larson, 1996) , (Dowty, 1991) , (Jędrzejko, 1993) , (Laskowski and Wróbel, 1997) . We tried to select relations that are very frequent or frequent in Polish texts. 3 The relation set is following (thematic roles are marked with theta, other relations -with rho): Proto-Agent θ -it is an instigator of an action or an entity that is in a particular state, it may undergoe change of state not caused by another participant; for predicates denoting relations -it is the first element of the relation: (człowiek) wykształcony przez Jana θ '(man) educated by John θ ', wyj ący wilk θ 'howling wolf θ '. The Proto-Agent macrorole covers subroles of Agent, Causer and nonagentive non-causative Actor (cf. Actor macrorole in (Kearns, 2011) ). Proto-Patient θ is the second macrorole -it is an entity undergoing action, event or change of state caused by another participant; for predicates denoting relations -it is the second element of a given relation: wykształcenie kogoś θ 'educating someone θ ', (Jan) posiadaj ący maj ątek θ '(John) possessing an estate θ '. According to (Dowty, 1991) 2 In Polish, as in other Indo-European languages, verbs could be nominalized during a process of syntactic transformation (Jędrzejko, 1993) , (Kolln, 1990) . Such nominalized predicates could be linked with nouns by thematic relations. 3 Rationale for selection of the presented semantic relation types was their frequencies in a four-text sample taken from a Polish corpus KPWr. Together chosen relations account for ca 80% of all semantic relation occurrences in these texts. Most of our relation types could be found on the list of the most frequent relation types in the English noun phrase (Moldovan et al., 2004, Tab. 1) . many thematic roles come down to the macroroles of Proto-Patient and Proto-Agent. Instrument θ is a tool, a device or means used by someone in order to cause something, it is sometimes regarded as a secondary cause of situation or change of state: przeszyty włóczni ą 'speared with a spear', lina θ cumownicza adjective 'a hawser, lit. mooring rope θ '. Material θ is an entity that is used by someone to produce something from it, material undergoes change of state resulting in its disappearance and emerging of a result: zrobiony z mosi ądzu θ 'made out of brass θ ', mosiężna θ figurka 'brass θ statuette'. Purpose θ -an entity or a situation toward which the event is directed or an individual which benefits from the event (purpose combines goal, beneficiary and recipient roles): wręczenie (medali) olimpijczykom 'giving (medals) to Olympians θ , sala koncertowa θ 'a concert θ hall'. Location is a physical place at which a given event is localised, a place being destination of an event, a path or a source of motion, or simply a place at which a particular individual is situated: wręczenie (medali) w auli 'giving (medals) at the lecture theatre ', przedzieranie się przez moczary 'struggling through the swamp '. Time is a particular moment or a duration of an event -it localises a situation within the flow of events or gives its duration: przedzieranie się przez godzinę /w środę 'struggling for an hour /on Wednesday '. Temporal/spatial meronymy -these relations point onto a spatial or temporal part of a place/location/time/period): poniedziałkowy poranek 'Monday morning ', środek zimy 'middle of the winter', koniec drogi 'end of the road', stolica kraju 'capital of the country'. Attribute is a property of an individual or an event, such as colour, size, weigth, intensity, duration etc., which might be expressed with a qualitative adjective: czerwony samochód 'red car', głośna muzyka 'loud music'. Family (member) is a relative or an in-law to someone, the relation is bidirectional and reflexive: syn króla 'king's son ', moja żona 'my wife ' (I am a relative to my wife). Order gives a position of an entity or an event in an ordered sequence/chain: druga odpowiedź '2nd answer', lata 80 . 'eighties, lit. eightieth years'. Quantity is an amount of something or a cardinality of a given set: pięciu panów 'five men', kieliszek wina 'glass of wine'. Semantic relation recognition rule-based algorithm Our rule-based system proceeds in two steps 4 : first it recognizes word pairs and triples, then operators classifying relations enter. Recognizing word pairs and triples Since we consider relations within noun phrases, we must identify them correctly. We made use of a CRF shallow parser (Radziszewski and Pawlaczek, 2012) trained on an annotated corpus of Polish (KPWr) (Broda et al., 2012) which comprises shallow syntactic annotation level (Radziszewski et al., 2012) . KPWr contains 326 annotated text samples representing different genres and styles: blogs, press articles, official and legal texts and Polish Wikipedia articles, it comprises 106358 annotations (phrases and phrase heads, and predicateargument relations). Noun and preposition phrases (NPs/PPs) from the corpus correspond to arguments of predicateargument structure. Each such NP/PP constists of one or several smaller phrases based on agreement (AgPs, for details, please look at cited works). Here is an example NP from the corpus (a head of the phrase is boldfaced, AgP heads are underlined): [[samolot wyprodukowany] AgP [przez PZL] AgP [w roku 1938] AgP [w Łodzi] AgP ] N P 'aircraft made by PZL in (year) 1938 in Łódź (city)' There is no reliable deep parser for Polish (Gołuchowski and Przepiórkowski, 2012) , thus we decided to construct a simple rule-based algorithm for deepened shallow parsing of Polish NPs/PPs. The algorithm works on tagged texts -we used (Radziszewski, 2013) tagger. Parsing rules make use of an output from the CRF shallow parser (Radziszewski and Pawlaczek, 2012) , in particular: borders of whole NPs/PPs, and of their constituents (i.e., phrases based on agreement, AgPs). Found pairs and triples are directly connected within a syntactic structure. Hand-written rules act like a partial dependency parser. The pairs consist of one subordinate and 4 Similarly to system presented in (Gamallo et al., 2002) . one superordinate token, the triples comprise one superordinate token and a subordinate preposition phrase (preposition + governed nominal head of a subordinate noun phrase). The whole algorithm runs in a main loop which iterates AgP i heads. We start from the first AgP 0 head to the left, then we proceed to the right, jumping from AgP i head to the closest AgP i+1 head to the right. For every AgP i head we run a cascadelike chain of rules (numbered from 1 to 7) for genetives, nominatives, small preposition phrases (being a part of larger NPs or PPs), coordination, other known to the tagger tokens, other unknown to the tagger tokens and for modifiers. The algorithm in pseudocode was shown in Algorithm 1 The algorithm gives following description for just analysed phrase, ""R + number"" denotes the number of a rule in the Algorithm 1 activated on the word pair or triple (for instance, R3 means that the rule number 3 was activated): R7: samolot ← wyprodukowany 'plane made', R3: wyprodukowany ← przez PZL 'by PZL',R3: wyprodukowany ← w roku 'in year', R3: wyprodukowany ← w Łodzi 'in Łódź' . Such simple shallow parsing algorithm operates quite well on an annotated part of KPWr with Fmeasure equal to 84%, P = 88%, R = 80%. 5 Applying WCCL operators Having identified pairs and triples we run on them operators written in a constraint language WCCL (Radziszewski et al., 2011) . The operators are language-specific and utilize morphosyntactic features (POS, case, number and gender), domains of Polish WordNet lexical units (word-sense pairs (Maziarz et al., 2012) ), thousands of derivational relation instances between nouns, adjectives and verbs from the wordnet 6 and information about syntactic frames of nominalized predicates, taken from Polish valence dictionary (Dębowski and Woliński, 2007) . Each of written operators refers to one semantic relation. In other words, each semantic relation is described by one or by many WCCL operators. If an operator is successfully applied to a pair (or a For example, our Proto-Patient relation was described by the 6 WCCL operators. One of them is presented in Listing 1. This operator uses two dictionaries with valence frames (acc -a list of verbs possessing any accusative frame, frames -a list of verbs described in the Polish valence dictionary (Dębowski, 2013) ) and morphosyntactic information about part of speech (class) and case. This operator PROTO-PATIENT-acc captures pairs like dręcz ący pact Janka noun.acc−θ 'tormenting John θ ' with a noun playing a Proto-Patient role of the predicate dręcz ący. The operator first checks whether a predicate (active participle) has an accusative frame or is outside the dictionary of Dębowski (""frames""). Since dręczyć 'to torment' is in acc dictionary and since Janek 'John' has subst class and acc case -the boolean operator returns 'true'. Let us present another example: the Proto-Agent macrorole is recognized by 5 operators, in Listing 2 was shown one of them. The PROTO-AGENT-ger-przez-acc operator is written for triples, i.e., for a triple wydanie pact przez pron wydawcę noun.acc−θ 'publishing by the publisher θ '. The first element in the triple is a gerund form of verb wydać 'to publish'. The operator checks whether the verb wydać has in its frame accusative/genetive or whether it cannot be found in Dębowski's dictionary (position 0 in the triple, frames). Listing 1: One of the WCCL operators describing Proto-Patient relation. Language details has been described in (Radziszewski et al., 2011) , abbreviations for grammatical categories has been explained in (Przepiórkowski et al., Next the operator seeks for the preposition przez 'by' at position 1. Then it tests if the first meaning of the lemma wydawca 'publisher' does not belong to the domain 'time' (= Polish czas) in Polish WordNet (position 2). Indeed, the first meaning of wydawca is in the domain 'person' (that iformation is avaiable in the dictionary noun_domain). At the end, we check whether the last token of our triple is in accusative. Because all of these conditions are fulfilled, the operator returns 'true', and we may assume that the last token takes the role of Proto-Agent. In Listing 3 one operator for family ralation was shown. FAMILY-agpp used to recognize this relation for word pairs. The operator, inter alia, uses semantic dictionary of kinship names built on the basis of Polish WordNet (the dictionary kinship), lammas of possessive pronouns (e.g., mój 'my', twój 'yours'). Listing 3: Two WCCL operators describing Family relation @b:""FAMILY-agpp"" ( and( // agreement agrpp(0,1, {nmb, gen, cas}), // position 0 in(base[0], [""moj"", ""twoj"", ""swoj"", ""nasz"", ""wasz""]) // position 1 equal(lex(base[1], ""kinship""), [""1""]), equal(lex( base[1], ""noun_domain""), [""os""]), in(class[1], {ger, subst, depr}), ) ) Results and conclusions Evaluation of the presented semantic relation recognition algorithm was performed in three steps. First experiment (labelled kpwr) was performed on a random sample of the KPWr corpus (26 out of 326 texts, aproximately one thirteenth of the corpus). In this experiment we made use of syntactic annotations from KPWr (cf. Tab. 1). Second experiment was performed on a random sample of 100 texts taken from yet another Polish corpus, called NKJP (Przepiórkowski et al., 2012, nkjp, approximately one tenth of the corpus) 7 . Since NKJP lacked syntactic annotations of KPWr style, we were forced to run on it the CRF shallow parser (described in Sec. 4.1) . This experiment gave us information about performance of our algorithm on a 'bare' text (see Tab. 2). Evaluation in the experiments was done by a professional linguist. At last, four baseline models were constructed and evaluated on the two corpora (Tab. 3). We created baselines similar to that presented in (Uchiyama et al., 2008) , which was majority model. We chose the most frequent relation, which in the sample from KPWr was Proto-Patient (with the number of 113 instances out of 268 relation instances), this relation type was also the most frequent in the sample of NKJP (411 out of 1950 relation instances). For each corpora two baselines were calculated: in Baseline #1 we assumed that we had perfectly recognized all occurences of semantic relations (of any type), in Baseline #2 we simply signed with 'Proto-Patient' label every recognized by our system semantic relation instance. Baseline #2 is realistic, while #1 is idealistic, since to obtain #1 we should be able to recognize every single relation instance within a corpus. Baselines #1 are upper limits for all majority models (including #2). Our two idealistic baselines are higher than the realistic baselines (see Tab. 3). Percentile bootstrap methods (DiCiccio and Efron, 1996), (DiCiccio and Romano, 1988) were applied to statistical significance and confidence interval (CI) analysis of the data. 8 We took 10000 bootstrap resamplings for each measure (P, R, F1), α was equal to 0.05 for each one-tailed test and CI (a percentile CI need not be symmetrical). In nkjp we have beaten both idealistic and realistic baselines. Precision, recall and F1 for kpwr are higher than Baseline #2. Only idealistic Baseline #1 for the KPWr corpus has overtaken our rule-based algorithm with regard to recall (42.2% vs. 32.8%), while its precision is lower and F1's are statistically indistinguishable. Results are promising, precisions go above 50% (the lower endpoint for the kpwr confidence intervel), for nkjp we may assess it even more precisely as 60%-65%. Some semantic relations are recognized with higher precision: Proto-Agent (nkjp: 89-100%, kpwr: 90-100%, α = 0.05), Proto-Patient (nkjp: 88-95%, kpwr: 83%-98%), family (nkjp: 90-100%) and order (nkjp: 91-100%). Our system is thus comparable in this aspect to the systems described in Sec. 2. 9  Overall recall is low, but higher than realistic baselines. In kpwr we obtained R = 27-38%, while for nkjp we got statistically higher interval of 37-41%. It seems that recall was not affected by lack of marked NP/PP borders in the corpus (these should have been brought out by the CRF shallow parser). F-measures calculated on our both corpora are also much higher than realistic baselines #2. We can already conclude that our preliminary experiments turned successful. Now we are aiming at improving our operators to raise their recall and at expanding the semantic role set (e.g., for Agent, Causer, Experiencer, Possessor or Result). Parallel, we start work on construction of automatic algorithms for relation recognition. not avaiable. 9 Not directly, of course.",2491460,c0f1047fe0f95c367184d494e78bb07b11ee3608,2,https://aclanthology.org/R13-1044,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"K{\k{e}}dzia, Pawe{\l}  and
Maziarz, Marek",Recognizing semantic relations within {P}olish noun phrase: A rule-based approach,342--349,,,,,,,inproceedings,kedzia-maziarz-2013-recognizing,,,"Semantics: Sentence-level Semantics, Textual Inference and Other areas"
6,W05-0818,"In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and languageindependent heuristics to find links between single words and multiword units in sentence-aligned parallel texts. The method has achieved an alignment error rate of 22.72% and 44.49% on English-Inuktitut and Romanian-English parallel sentences, respectively.","In this paper we describe LIHLA, a lexical aligner which uses bilingual probabilistic lexicons generated by a freely available set of tools (NATools) and languageindependent heuristics to find links between single words and multiword units in sentence-aligned parallel texts. The method has achieved an alignment error rate of 22.72% and 44.49% on English-Inuktitut and Romanian-English parallel sentences, respectively. Introduction Alignment of words and multiword units plays an important role in many natural language processing (NLP) applications, such as example-based machine translation (EBMT) (Somers, 1999) and statistical machine translation (SMT) (Ayan et al., 2004; Och and Ney, 2000) , transfer rule learning (Carl, 2001; Menezes and Richardson, 2001) , bilingual lexicography (Gómez Guinovart and Sacau Fontenla, 2004) , and word sense disambiguation (Gale et al., 1992) , among others. Aligning two (or more) texts means finding correspondences (translation equivalences) between segments (paragraphs, sentences, words, etc.) of the source text and segments of its translation (the target text). Following the same idea of many recently proposed approaches on lexical alignment (e.g., Wu and Wang (2004) and Ayan et al. (2004) ), the method described in this paper, LIHLA (Language-Independent Heuristics Lexical Aligner) starts from statistical alignments between single words (defined in bilingual lexicons) and applies languageindependent heuristics to them, aiming at finding the best alignments between words or multiword units. Although the most frequent alignment category is 1 : 1 (in which one source word is translated exactly as one target word), other categories such as omissions (1 : 0 or 0 : 1) or those involving multiword units (n : m, with n and/or m ≥ 1) are also possible. This paper is organized as follows: section 2 explains how LIHLA works; section 3 describes some experiments carried out with LIHLA together with their results and, in section 4, some concluding remarks are presented. How LIHLA works As the first step, LIHLA uses alignments between single words defined in two bilingual lexicons (source-target and target-source) generated from sentence-aligned parallel texts using NATools. 1  Given two sentence-aligned corpus files, the NA-Tools word aligner -based on the Twenty-One system (Hiemstra, 1998) -counts the co-occurrences of words in all aligned sentence pairs and builds a sparse matrix of word-to-word probabilities (Model A) using an iterative expectation-maximization algorithm (5 iterations by default). Finally, the elements with higher values in the matrix are chosen to compose two probabilistic bilingual lexicons (source-target and target-source) (Simões and Almeida, 2003) . For each word in the corpus, each bilingual lexicon gives: the number of occurrences of that word in the corpus (its absolute frequency) and its most likely translations together with their probabilities. The construction of the bilingual lexicons is an independent prior step for the alignment performed by LIHLA and the same bilingual lexicons can be used several times to align parallel sentences. So, using the two bilingual lexicons generated by NATools and some language-independent heuristics, LIHLA tries to find the best alignment between source and target tokens (words, numbers, special characters, etc.) in a pair of parallel sentences. For each source token s j in source sentence S, LIHLA will look for the best token t i in the target parallel sentence T applying these heuristics in sequence: 1. Exact match LIHLA creates a 1 : 1 alignment between s j and t i if they are identical. This heuristic stays for exact matches, for instance, between proper names and numbers. Best candidate according to the bilingual lexicon LIHLA looks for possible translations of s j in the source-target bilingual lexicon (B S ) and makes an intersection between them and the words in T . In this intersection, if no candidate word identical to those in B S is found, then LIHLA tries to look for cognates for those words using the longest common subsequence ratio (LCSR). 2 By doing this, LIHLA can deal with small changes in possible translations such as different forms of the same verb, changes in gender and/or number of nouns, adjectives, and so on. Then, LIHLA selects the best target candidate word t i for s j -the best candidate word according to B S among those in a position which is favorably situated in relation to s j -and looks for multiword units involving s j and t i -those words that occur immediately before and/or after s j (for source multiword units) or 2 The LCSR of two words is computed by dividing the length of their longest common subsequence by the length of the longer word. For example, the LCSR of Portuguese word alinhamento and Spanish word alineamiento is 10 12 0.83 as their longest common subsequence is a-l-i-n-a-m-e-n-t-o. t i (for target multiword units) and are not possible translations for other words in T and S, respectively. According to the multiword units that have (or not) been found, a 1 : 1, 1 : n, m : 1 or m : n alignment is established. An omission alignment for s j (1 : 0) can also be established if no target candidate word t i that satisfies this heuristic is available. Cognates If no possible translation for s j is found in the bilingual lexicon and the target sentence (T ) at the same time, LIHLA uses the LCSR to look for cognates for s j in T and sets a 1 : 1 alignment between s j and its best cognate or a 1 : 0 alignment if there is no cognate available. These heuristics are applied while alignments can still be produced and a maximum number of iterations is not reached (see section 3 for the number of iterations performed in the experiments described in this paper). Furthermore, at the first iteration, all words with a frequency higher than a set threshold are ignored to avoid erroneous alignments since all subsequent alignments are based on the previous ones. In its last step (which is optional and has not been performed in the experiments described in this paper), LIHLA aligns the remaining unaligned source and target tokens between two pairs of already aligned tokens establishing several 1 : 1 alignments when there are the same number of source and target tokens, or just one alignment involving all source and target tokens if they exist in different quantities. The decision of creating n 1 : 1 alignments in spite of just one n : n alignment when there is the same number of source and target tokens is due to the fact that a 1 : 1 alignment is more likely to be found than a n : n one. Experiments In this section we present the experiments carried out with LIHLA for the ""Shared task on word alignment"" in the Workshop on Building and Using Parallel Texts during ACL2005. Systems participating in this shared task were provided with training data (consisting of sentence-aligned parallel texts) for three pairs of languages: English-Inuktitut, Romanian-English and English-Hindi. Furthermore, the systems would choose to participate in one or both subtasks of ""limited resources"" (where systems were allowed to use only the resources provided) and ""unlimited resources"" (where systems were allowed to use any resources in addition to those provided). The system described in this paper, LIHLA, participated in the subtask of limited resources aligning English-Inuktitut and Romanian-English test sets. The training sets -composed of 338,343 English-Inuktitut aligned sentences (omission cases were excluded from the whole set of 340,526 pairs) and 48,478 Romanian-English aligned ones-were used to build the bilingual lexicons. Then, without changing any default parameter (threshold for LCSR, maximum number of iterations, etc.), LIHLA aligned the 75 English-Inuktitut and the 203 Romanian-English parallel sentences on test sets. The whole alignment process (bilingual lexicon generation and alignment itself) did not take more than 17 minutes for English-Inuktitut (3 iterations per sentence, on average) and 7 minutes for Romanian-English (4 iterations per sentence, on average). The evaluation was run with respect to precision, recall, F -measure, and alignment error rate (AER) considering sure and probable alignments but not NULL ones (Mihalcea and Pedersen, 2003) . Tables 1 and 2 present metric values for English-Inuktitut and Romanian-English alignments, respectively, as provided by the organization of the shared task. The results obtained in these experiments were not so good as those achieved by LIHLA on the language pairs for which it was developed, that is, 92.48% of precision and 88.32% of recall on Portuguese-Spanish parallel texts and 84.35% of precision and 76.39% of recall on Portuguese-English ones. 3  The poor performance in the English-Inuktikut task may be partly due to the fact that Inuktikut is a polysynthetic language, that is, one in which, unlike in English, words are formed by long strings of concatenated morphemes. This makes it difficult for NATools to build reasonable dictionaries and lead to a predominance of n : 1 alignments, which are harder to determine -this fact can be confirmed by the better precision of LIHLA when probable alignments were considered (see table 1 ). The performance in the English-Romanian task, not very far from the English-Portuguese task used to tune up the parameters of the algorithm, is harder to explain without further analysis. Metric The difference in precision and recall between the two language pairs is due to the fact that on the English-Inuktitut reference corpus in addition to sure alignments the probable ones were also annotated while in Romanian-English only sure alignments are found. This indicates that evaluating alignment systems is not a simple task since their performance depends not only on the language pairs and the quality of parallel corpora (constant criteria in this shared task) but also the way the reference corpus is built. So, at this moment, it would be unfair to blame the worse performance of LIHLA on its alignment methodology since it has been applied to the new language pairs without changing any of its default parameters. Maybe a simple optimization of parameters for each pair of languages could bring better results and also the impact of size and quality of training and reference corpora used in these experiments should be investigated. Then, the only conclusion that can be taken at this moment is that LIHLA, with its heuristics and/or default parameters, can not be indistinctly applied to any pair of languages. Despite of its performance, LIHLA has some advantages when compared to other lexical alignment methods found in the literature, such as: it does not need to be trained for a new pair of languages (as in Och and Ney (2000) ) and neither does it require pre-processing steps to handle texts (as in Gómez Guinovart and Sacau Fontenla ( 2004 )). Furthermore, the whole alignment process (bilingual lexical generation and alignment itself) has proved to be very fast as mentioned previously. Concluding remarks This paper has presented a lexical alignment method, LIHLA, which aligns words and multiword units based on initial statistical word-to-word correspondences and language-independent heuristics. In the experiments carried out at the ""Shared task on word alignment"" which took place at the Workshop on Building and Using Parallel Texts during ACL2005, LIHLA has been evaluated on English-Inuktitut and Romanian-English parallel texts achieving an AER of 22.72% and 44.49%, respectively. As future work, we aim at investigating the impact of using additional linguistic information (such as part-of-speech tags) on LIHLA's performance. Also, as a long-term goal, LIHLA will be part of a system implemented to learn transfer rules from sequences of aligned words. Acknowledgments We thank FAPESP, CAPES, CNPq and the Spanish Ministry of Science & Technology (Project TIC2003-08681-C02-01) for financial support.",15322146,ff3f05120d24e5dac2879f25402993bc6355f780,5,https://aclanthology.org/W05-0818,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Caseli, Helena M.  and
Nunes, Maria G. V.  and
Forcada, Mikel L.",{LIHLA}: Shared Task System Description,111--114,,,,,,,inproceedings,caseli-etal-2005-lihla,,,
7,L02-1313,,,649937,c5c1643517ee6646c47b4ee2b8443d4f62ee1ae5,4,http://www.lrec-conf.org/proceedings/lrec2002/pdf/313.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Baldwin, Timothy  and
Bilac, Slaven  and
Okumura, Ryo  and
Tokunaga, Takenobu  and
Tanaka, Hozumi",Enhanced {J}apanese Electronic Dictionary Look-up,,,,,,,,inproceedings,baldwin-etal-2002-enhanced,,,
8,R13-1045,"We describe an approach to building a morphological analyser of Arabic by inducing a lexicon of root and pattern templates from an unannotated corpus. Using maximum entropy modelling, we capture orthographic features from surface words, and cluster the words based on the similarity of their possible roots or patterns. From these clusters, we extract root and pattern lexicons, which allows us to morphologically analyse words. Further enhancements are applied, adjusting for morpheme length and structure. Final root extraction accuracy of 87.2% is achieved. In contrast to previous work on unsupervised learning of Arabic morphology, our approach is applicable to naturally-written, unvowelled Arabic text.","We describe an approach to building a morphological analyser of Arabic by inducing a lexicon of root and pattern templates from an unannotated corpus. Using maximum entropy modelling, we capture orthographic features from surface words, and cluster the words based on the similarity of their possible roots or patterns. From these clusters, we extract root and pattern lexicons, which allows us to morphologically analyse words. Further enhancements are applied, adjusting for morpheme length and structure. Final root extraction accuracy of 87.2% is achieved. In contrast to previous work on unsupervised learning of Arabic morphology, our approach is applicable to naturally-written, unvowelled Arabic text. Introduction The number and diversity of human languages makes it impractical to manually craft lexicons and morphological processors for more than a very small proportion of them. Further challenges are posed by the need to deal with dialects and colloquial forms of languages. This has motivated recent increased interest in approaches to morphological analysis based on unsupervised learning. Inspired by competitions such as the Morpho Challenge, many techniques have been proposed for unsupervised morphology learning. Although these techniques are often intended to be language independent, they are often directed to a specific group of languages. Most work has aimed at sequential separation or segmentation of morphemes concatenated together in a surface word form. This type of analysis, outputting stems and appended morphemes aims to identify some kind of border between the different morphemes. However, another type of word formation consists of the interdigitation of a root morpheme with an affix or pattern template; in this case there is no boundary between morphemes, since they are rather intercalated with each other. This type of non-concatenative morphology, which is characteristic of the Semitic group of languages, has attracted far less interest for unsupervised learning. In this paper we present an approach to unsupervised learning of non-concatenative morphology, applying it to Arabic. We describe an approach to learning tri-literal roots and affix template of Arabic by first inducing root and affix lexicons. Our approach uses Maximum Entropy modelling to obtain clusters 1 of words based on concatenative and non-concatenative orthographic features, and induces the lexicons from these clusters. Our data is an undiacritized version of the Quranic Arabic Corpus since we assume a realistic setting of unvowelled text, as most Arabic text is written without vowels; we chose this corpus since correct roots of each word are available, facilitating the evaluation process. The fact that the corpus contains a relatively small vocabulary of around 7000 words also simulates the scenario for most of the world's languages of scarcity of linguistic resources and data. This paper is structured as follows: Section 2 surveys previous related work. Section 3 provides an introduction to Arabic root and pattern morphology. Our approach to unsupervised lexicon induction based on Maximum Entropy (ME) modelling is explained in section 4. Section 5 describes the procedure for performing morphological analysis of words, followed by evaluation in section 6 and conclusions in section 7. 2 Related Work An active current area of natural language processing research is applying statistical and information-theoretic approaches to unsupervised learning of morphology and grammar. A common starting point is raw (unannotated) text corpora, inducing the target knowledge from word forms and their patterns of usage. Information theoretic approaches, particularly Minimum Description Length (MDL) as investigated by Goldsmith (2000 Goldsmith ( , 2006) ) and others (Cruetz and Lagus, 2005, 2007) , have brought a theoretical perspective considering input data to be 'compressed' into a morphologically analysed representation. This optimization scheme has achieved good results, and is amongst the most effective approaches for unsupervised morphological analysis. Most work on unsupervised learning of morphology has focused on concatenative morphology (De Pauw and Wagacha 2007; Hammarström and Borin 2011) . Another perspective adopted by Schone and Jurafsky (2001) incorporates orthographic and phonological features, and induces semantic relatedness between word pairs using Latent Semantic Indexing. Their work shows comparable performance to Goldsmith's (2000) Linguistica system. Yarowsky and Wicentowski (2000) experiment with learning irregular mnaturaorphology using a lightly supervised technique to align irregular words to their lemmas by estimating the distribution of ratios over part-of-speech classes of inflected words to lemmas. More recently, researchers have addressed nonconcatenative morphology, such as for Semitic languages, using a variety of empirical approaches. Daya et al. (2008) learn Semitic roots using supervised learning, building a multi-class classifier for individual root radicals. Clark (2007) uses Arabic as a test-bed to study semi-supervised learning of complex broken plural structure modelled using memory-based algorithms, with the aim of gaining insights into human language acquisition. Most work on unsupervised learning of morphology has focused on concatenative morphology (Hammarström and Borin 2011) . The few studies that have focussed on nonconcatenative morphology, such as for Semitic languages, have not used naturally written text. For example, Rodriguez and Ćavar (2005) learn roots using a number of orthographic heuristics and then apply constraint-based learning to improve the quality of roots. Xanthos (2008) works on phonetic transcriptions of Arabic text to decipher roots and patterns. The approach is to initially create crude Root and Pattern (RP) transcriptions from words based on vowel-consonant distinctions, and then to apply an MDL approach similar to Goldsmith's (2006) in order to refine the RP structures. In contrast to previous work, we learn intercalated morphology, identifying the root and transfixes/ incomplete pattern for words from 'natural' text without short vowels or diacritical markers. Root and Pattern Morphology Words in Arabic are formed through three morphological processes. The first (i) is the fusion of a root form and pattern template to derive a base word, which can be a noun, verb or adjective, all of which are semantically related to the root. The second (ii) is affixation, by means of prefixes, suffixes or infixes, including inflectional morphemes marking gender, plurality and/or tense, resulting in a stem. Thirdly (iii) a final layer of clitics may be attached to a word, including a subset of prepositions, conjunctions, determiners and pronouns; these appear at the beginning (proclitics) or end (enclitics) of a word but never in the middle. Since techniques for concatenative morphology learning are fairly advanced we have focused on using stemmed words, computable through such approaches. We used the QAC stem vocabulary where appended morphemes of type (iii) are mostly absent 2 and hence ignored from analysis. Most of type (ii) are present as part of the stem. In the case of (i), most derived forms consist of short vowels and occasional long vowels or a consonant interdigitated with the root. In unvowelled text the short vowels are ignored, so derived words have at most single letter affixation. Table 1 shows two example words with their roots and affix pattern templates. The 'y' and 't' in the respective words are clitic/inflectional markers, which are part of the affix template. 'A' is the derivational infix marker for nouns. Word Root Pattern ktAby Ktb --A-y tEArf Erf t-A-- For analysis, each word, ‫ݓ‬ , is decomposed, using a decomposition function, into a set of tuples encoding all ݊ possible combinations of a root (of at least 3 letters) and associated pattern: ‫ݓ(݀‬ ) → ‫ݎ〈{‬ ௫ , ‫‬ ௫ 〉} (Eq. 1) where ‫ݔ‬ ranges from 1 to ݊. For example, the decomposition of the word 'yErf', is shown in Figure 1 . ‫݂ݎܧݕ‬ → ⎩ ⎪ ⎨ ⎪ ⎧ ‫ݕ〈‬ ‫ܧ‬ ‫,ݎ‬ − − −݂ 〉, ‫ݕ〈‬ ‫ܧ‬ ݂, − − ‫,〉−ݎ‬ ‫ݕ〈‬ ‫ݎ‬ ݂, ‫ܧ−‬ − −〉, ‫ܧ〈‬ ‫ݎ‬ ݂, ‫ݕ‬ − − −〉, ‫ݕ〈‬ ‫ܧ‬ ‫ݎ‬ ݂, − − − −〉 ⎭ ⎪ ⎬ ⎪ ⎫ Using Maximum Entropy Modelling for Unsupervised Learning In this study we apply an supervised machine learning technique, Maximum Entropy (ME) modelling, in a completely unsupervised way, taking our inspiration from the work of De Pauw and Wagacha ( 2007 ), who applied the approach for extracting prefixes in an African language. Unlike for supervised learning, no annotated text is used. Instead we simply derive features automatically from the vocabulary words of the dataset. Each word is represented as an output class mapped to by the corresponding features of the words. These word-features are used to train a classifier. Rather than applying the classifier to classify unseen data, we apply the model back to the 'training data' to obtain, not the classification but the proximities of each word/class with every other word/class. These proximities are then utilized to root and pattern lexicons. The advantage of this approach to gauge relatedness of words over other approaches, such as minimum edit distance, is the ability to better capture morpheme dependencies between words with common roots which may be orthographically quite different due to substantial affixing. Building the Lexicons We derive two lexicons: a root lexicon and an affix or pattern lexicon. We do this by training ME classifiers on orthographic features computed from each word in the corpus dataset. The classifiers are then applied to the same data to obtain word clusters relating each word to every other word with respect to either common roots or common patterns. Thus, for the root lexicon we obtain neighbours of words that have the same or similar patterns. Conversely, for the pattern lexicon we obtain neighbours of words that have common root radicals. Modelling Orthographic Features We first extract orthographic features for obtaining word clusters with similar roots (i.e. for pattern lexicon acquisition). We then construct the inverse of these features for obtaining word clusters with similar patterns (i.e. for root lexicon acquisition). In the former case, feature extraction proceeds as follows: we first enclose each word with beginning and end boundary markers, '@' and '#' respectively. (This is in order to provide context information for the first and last characters of a word). We next compute the power-set of all the character combinations in a word, and then exclude features where the first and last letter of the word appear without the boundary markers (to give emphasis to word boundary features). The final set of these features for the word 'yErf' is shown in the first column of Table 2 . In the latter case, pattern features are obtained such that corresponding to each root feature, we replace root radicals with a placeholder; characters between root radicals that are omitted from the root features appear as potential affix characters in the pattern template. These inverse features are shown in the second column of Table 2 . Root Features (for Pattern Lexicon) Pattern features (for Root Lexicon) @y, @yE, @yEr, @yErf#, @yEr#, @yEf#, @yE#, @yr, @yrf#, @yr#, @yf#, @y#, @E, @Er, @Erf#, Er#, @Ef#, @E#, @r, @rf#, @r#, @f#, E, Er, Erf#, Er#, Ef#, E#, r, rf#, r#, f# @-, @--, @---, @----#, @---f#, @--r-#, @--rf#, @-E-, @-E--#, @-E-f#, @-Er-#, @-Erf#, @y-, @y--, @y---#, @y--f#, @y-r-#, @y-rf#, @yE-, @yE--#, @yE-f#, @yEr-#, -, --, ---#, --f#, -r-#, -rf#, -, --#, -f#, -# Word Nearest Neighbors The classifier is trained using Limited Variable LBFGS optimization method. The number of iterations for training is stopped automatically when 100% accuracy on the training data is achieved. Each trained classifier is reapplied to its respective training data features to get proximity values between each word and every other word. Sorting the list gives us the most related word in terms of root based or pattern based proximity values, with the highest value (≈ 1) for the headword, ℎ, i.e. the word's own features. Table 3 shows an example of the closest neighbours in a cluster, along with their headword. Using these words and proximity measures we next apply a strategy to induce the morpheme. Not all words in the list of N elements for each word are relevant to us since the proximity value starts to drop rapidly towards zero as we go down the ranked list. With each headword we choose a 500 nearest neighbours cluster for each type of morpheme as a sufficient number beyond which we expect no gain in efficiency is expected. Head-Word, h Proximity Dictionary Induction Using the respective word clusters we create dictionaries for two types of morphemes, roots and patterns, such that we score the morphemes thus: Higher scoring morphemes are more plausible and ranked higher in the lexical list than lower ones. The procedure for scoring is adapted and amended from the work of De Pauw and Wagacha (2007). For the pattern lexicon, we score each pattern in the following manner: for each headword, h i (having probability value ≈ 1) in cluster c i (with each of the i = 1,2,…N words in the vocabulary), we obtain all possible decompositins(equation 1) into template patterns ‫‬ ௫ (shown in column 1 of Table 4 ) and roots, ‫ݎ‬ ௫ (column 2 of Table 4 ) with respect to the headword, ℎ . Each pattern is scored with a function ܵ( ‫‬ ௫ ) (equation 2) which aggregates the Logarithmically Scaled ( ‫ܵܮ‬ ) probability value, ܲ of words k j (j = 1,2,…500 words in each cluster), such that ‫ݎ‬ ௫ matches any of the roots in word k, ‫ݎ‬ ௬ (y=1,2,…m root combinations in k). This aggregation is not only local to each cluster but covers all occurrences of the pattern in each of the N clusters. ܵ( ‫‬ ௫ ) = ቀ‫ܵܮ‬൫ܲ ൯× ‫|(ܣܮ‬ ௫ |)ቚ ‫ݎ‬ ௫ = ‫ݎ‬ ௬ ቁ ହ ୀଵ ே ୀଵ (Eq. 2) Logarithmic scaling is necessary since the probability drops too rapidly and too low in order to provide a feasible ratio between words. After taking the log of the probability the resulting ratios are negative which are then adjusted by subtracting the log of a base probability value, ܲ , thus linearly inverting the ratios (equation 3). ܲ is hence chosen to be small enough to ensure the resulting logarithmic score is positive. We chose the smallest occurring probability value in our clusters as the value for ܲ . ‫ܵܮ‬൫ܲ ൯= log ܲ(݇ ) − log ܲ (Eq. 3) The score is also exponentially Length Adjusted ‫)ܣܮ(‬ for each pattern, ‫,‬ according to the length of the pattern, ‫,||‬ in terms of the number of affix charaters in ‫.‬ This boosts the score for lengthier morphemes which are relatively infrequent. The intuition for adjustment formula comes from the work of (Chung and Gildea, 2009) and (Liang and Klein, 2009) , who use a exponential Length Penalty measure to adjust their model for morpheme length. ‫)||(ܣܮ‬ = ݁ || (Eq. 4) Thus the pattern is scored according to the score of words containing plausible roots. Commonly occurring patterns such as 'y---' gather weight and ascend the list of the most frequent (and hence potentially sound) affix templates. Table 4 shows how each pattern for the headword 'yErf' is scored, aggregating the logarithmic score over words (in column 4 of Similarly, we score the root, ܵ( ‫ݎ‬ ௫ ), with respect to the pattern occurrence in each word k of cluster c i : ܵ( ‫ݎ‬ ௫ ) = ቀ‫ܵܮ‬൫ܲ ൯ቚ ‫‬ ௫ = ‫‬ ௬ ቁ ହ ୀଵ ே ୀଵ (Eq. 5) The scoring aggregates over the log scaled probability of words in the affix-based clusters having pattern occurrences in a word in each cluster. There is no need for length adjustment to these ratios since we are considering only three letter roots. Table 5 exemplifies this for scoring roots with words (in column 3 of Table 5 ) that have corresponding patterns (in column 2 of Table 5 ). Root Pattern Word Table 6 shows the top lexicon entries for roots and patterns along with their respective scores. The top entries in the lexicon would plausibly be correct morphemes while lower entries would be not so plausible. Root Lexicon Morphological Analysis A word is analysed into its root and pattern template by considering every possible combination of trilateral root and corresponding pattern pairs, ‫ݎ〈‬ ௫ , ‫‬ ௫ 〉 , as defined in equation 1 for the word, w i , in the vocabulary, scoring each analysis with the sum of the scores for the root, ‫ݎ‬ ௫ , and pattern, ‫‬ ௫ , in the root lexicon and pattern lexicon, respectively. Due to the different ranges of scores for root and pattern, the score for the former is scaled with respect to the latter, as in equation 6, in order to guarantee equal contributions. ‫)ݎ(ܵܵ‬ = ‫)ݎ(ܵ‬ × max(ܵ(‫))‬ max(ܵ(‫))ݎ‬ (Eq. 6) The analysis, x, with the highest score is selected as the output, as illustrated in equation 7. max ௫ୀଵ.. ( ܵ( ‫ݎ‬ ௪ ௫ ) + ‫(ܵܵ‬ ௪ ௫ ) ) (Eq. 7) Since we are considering text without diacritics, due to absence of short vowels, we only expect words to contain single letter infixes. Hence we experiment with an alternative configuration of the word decomposition, ‫ݎ〈‬ ௭ , ‫‬ ௭ 〉: non-contiguous root radicals formed with more than one intervening character are dropped; correspondingly patterns with more than one consecutive character between radical place holder markers are dropped. Evaluation We carry out our evaluation using the Quranic Arabic Corpus (QAC) 3 , since it identifies the root of each word, facilitating the evaluation. In this section, we first detail some information about our dataset before going onto evaluation of the analyses for correct root extraction. Data The QAC consists of approximately 77,900 word tokens, with a total of around 19,000 unique tokens. Since we are interested in investigating learning from undiacritized text, we removed all short vowels and diacritical markers. The size of the resulting vocabulary, after removal of vowels, is approximately 14,850. We take as input lightly stemmed text, with clitics removed, but with most inflectional markers attached. We assume that stemmed words are obtainable using existing tools for unsupervised concatenative morphology learning. For example, the technique of Poon et al (2009) could be used to obtain accurate stems for each word. The stemmed unvowelled vocabulary size is around 7370. The original corpus is annotated with roots for all derived and inflected words. More than 95% of words are tagged with their root forms since the Quran consists mostly of words of derivable forms, with very few proper nouns. There are 7192 stemmed words with available roots. In Arabic, sometimes alterations in root radicals take place; for example, in hollow roots, when moving from a root containing a long vowel to the surface word, the long vowel might change its form to another type or get dropped. Such words with hollow roots or reduplicated radicals, whose characters do not match every radical of the root, were removed from the evaluation as they are beyond the scope of the learning algorithm to identify. Leaving aside these word and root evaluation pairs we evaluated with 5468 stemmed types. Baseline As a baseline for evaluation, we derived lexicons in a similar manner to procedure for derivation from clusters (section 5.3). Instead of using clusters we simply scored patterns that matched the largest number of vocabulary words having corresponding roots. Likewise, the root score was obtained by counting the number of words with corresponding patterns. Comparing our system to the baseline is meant to elucidate the advantage of using the machine learning technique to enhance our lexicons. In the baseline we do not have the ME based word clusters with proximities to the target word; only one cluster exist: the vocabulary set with unit promitiy of 1. Evaluation of Lexicons In this section we compare our lexicons, built using maximum entropy modeling approach, (ME), to the baseline(BL). We evaluated the effect of logarithmic scaling (ME_LS) comparing it to using raw probability values(ME_RW). Also we gauged the performance improvement with Length Adjustment (ME_LS_LA) for morphemes. Finally, we evaluated morphological analysis restricted to patterns with single affixes which correspond to roots with single non-contiguous characters from words (ME_NC1). We evaluate morphological analysis through correct identification of the root. The accuracy is measured in terms of percentage of the roots that are correctly identified. As stated above, we evaluate on a total of 5468 words. The results for the different configuration evaluations is given in table 7 . The accuracy of 74% shows a sound and competitive baseline. The low results for ME_RW highlights the weakness of considering raw probability values which are too low to provide adequate weightage to morphemes. Hence the dismal performace. The true value for the ME based processing is realized in ME_LS, where the probabilities have been logarithmically scaled be summing. We see an accuracy gain of 6% over the baseline which is quite significant and encouraging. Further improvements can be seen when the score has been adjusted for morpheme length, ME_LS_LA, with performance increase by further 5%. Still more improvement is seen using knowledge of word structure of undiacritized text, ME_LS_LS_NC1, with further accuracy gain of 2.25 %. The final result for ME based analysis with further enhancements gives an promising accuracy result of 87.20%. Configuration Conclusion and Future directions In this paper we have presented an approach to solve the problem of learning intercalated morphology in an unsupervised manner with no parameter settings and minimal linguistic knowledge. We applied the machine learning based techniques to learn clusters of words related on basis of either root or pattern morpheme. Thereafter, plausible morphemes are extracted using a scoring method which takes advantage of knowledge of word proximities from clusters built using a maximum entropy classifier. We further apply enhancements to the procedure by accommodating for length and structure of morphemes. The finalized procedure offers significant boost in performance. The dynamicity of the technique allows its applicability to other types of morphological structures. Also, the system can easily be extended to cater to roots beyond tri-literals by adapting the soring function to accommodate for morpheme length.",690455,0b125557ba23075532380e88fb990933838975b7,2,https://aclanthology.org/R13-1045,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Khaliq, Bilal  and
Carroll, John",Unsupervised Induction of {A}rabic Root and Pattern Lexicons using Machine Learning,350--356,,,,,,,inproceedings,khaliq-carroll-2013-unsupervised,,,"Phonology, Morphology and Word Segmentation"
9,W05-0821,"Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model.","Statistical machine translation systems use a combination of one or more translation models and a language model. While there is a significant body of research addressing the improvement of translation models, the problem of optimizing language models for a specific translation task has not received much attention. Typically, standard word trigram models are used as an out-of-the-box component in a statistical machine translation system. In this paper we apply language modeling techniques that have proved beneficial in automatic speech recognition to the ACL05 machine translation shared data task and demonstrate improvements over a baseline system with a standard language model. Introduction Statistical machine translation (SMT) makes use of a noisy channel model where a sentence ē in the desired language can be conceived of as originating as a sentence f in a source language. The goal is to find, for every input utterance f, the best hypothesis ē * such that ē * = argmax ēP (ē| f ) = argmax ēP ( f |ē)P (ē) (1) P ( f |ē) is the translation model expressing probabilistic constraints on the association of source and target strings. P (ē) is a language model specifying the probability of target language strings. Usually, a standard word trigram model of the form P (e 1 , ..., e l ) ≈ l i=3 P (e i |e i−1 , e i−2 ) (2) is used, where ē = e 1 , ..., e l . Each word is predicted based on a history of two preceding words. Most work in SMT has concentrated on developing better translation models, decoding algorithms, or minimum error rate training for SMT. Comparatively little effort has been spent on language modeling for machine translation. In other fields, particularly in automatic speech recognition (ASR), there exists a large body of work on statistical language modeling, addressing e.g. the use of word classes, language model adaptation, or alternative probability estimation techniques. The goal of this study was to use some of the language modeling techniques that have proved beneficial for ASR in the past and to investigate whether they transfer to statistical machine translation. In particular, this includes language models that make use of morphological and part-of-speech information, so-called factored language models. Factored Language Models A factored language model (FLM) (Bilmes and Kirchhoff, 2003) is based on a representation of words as feature vectors and can utilize a variety of additional information sources in addition to words, such as part-of-speech (POS) information, morphological information, or semantic features, in a unified and principled framework. Assuming that each word w can be decomposed into k features, i.e. w ≡ f 1:K , a trigram model can be defined as p(f 1:K 1 , f 1:K 2 , ..., f 1:K T ) ≈ T t=3 p(f 1:K t |f 1:K t−1 , f 1:K t−2 ) (3) Each word is dependent not only on a single stream of temporally preceding words, but also on additional parallel streams of features. This representation can be used to provide more robust probability estimates when a particular word n-gram has not been observed in the training data but its corresponding feature combinations (e.g. stem or tag trigrams) has been observed. FLMs are therefore designed to exploit sparse training data more effectively. However, even when a sufficient amount of training data is available, a language model utilizing morphological and POS information may bias the system towards selecting more fluent translations, by boosting the score of hypotheses with e.g. frequent POS combinations. In FLMs, word feature information is integrated via a new generalized parallel backoff technique. In standard Katz-style backoff, the maximum-likelihood estimate of an n-gram with too few observations in the training data is replaced with a probability derived from the lower-order (n − 1)gram and a backoff weight as follows: p BO (w t |w t−1 , w t−2 ) (4) = d c p M L (w t |w t−1 , w t−2 ) if c > τ α(w t−1 , w t−2 )p BO (w t |w t−1 ) otherwise where c is the count of (w t , w t−1 , w t−2 ), p M L denotes the maximum-likelihood estimate, τ is a count threshold, d c is a discounting factor and α(w t−1 , w t−2 ) is a normalization factor. During standard backoff, the most distant conditioning variable (in this case w t−2 ) is dropped first, followed by the second most distant variable etc., until the unigram is reached. This can be visualized as a backoff path (Figure 1(a) ). If additional conditioning variables are used which do not form a temporal sequence, it is not immediately obvious in which order they should be eliminated. In this case, several backoff paths are possible, which can be summarized in a backoff graph (Figure 1 is also possible to choose multiple paths and combine their probability estimates. This is achieved by replacing the backed-off probability p BO in Equation 2 by a general function g, which can be any non-negative function applied to the counts of the lower-order n-gram. Several different g functions can be chosen, e.g. the mean, weighted mean, product, minimum or maximum of the smoothed probability distributions over all subsets of conditioning factors. In addition to different choices for g, different discounting parameters can be selected at different levels in the backoff graph. One difficulty in training FLMs is the choice of the best combination of conditioning factors, backoff path(s) and smoothing options. Since the space of different combinations is too large to be searched exhaustively, we use a guided search procedure based on Genetic Algorithms (Duh and Kirchhoff, 2004) , which optimizes the FLM structure with respect to the desired criterion. In ASR, this is usually the perplexity of the language model on a held-out dataset; here, we use the BLEU scores of the oracle 1-best hypotheses on the development set, as described below. FLMs have previously shown significant improvements in perplexity and word error rate on several ASR tasks (e.g. (Vergyri et al., 2004) ). W 1 t W − 2 t W − 3 t W − t W 1 t W − 2 t W − t W 1 t W − t W (a) F 1 F 2 F 3 F F F 1 F 2 F F 1 F 3 F F 2 F 3 F F 1 F F 3 F F 2 F (b) Baseline System We used a fairly simple baseline system trained using standard tools, i.e. GIZA++ (Och and Ney, 2000) for training word alignments and Pharaoh (Koehn, 2004) for phrase-based decoding. The training data was that provided on the ACL05 Shared MT task website for 4 different language pairs (translation from Finnish, Spanish, French into English); no additional data was used. Preprocessing consisted of lowercasing the data and filtering out sentences with a length ratio greater than 9. The total number of training sentences and words per language pair ranged between 11.3M words (Finnish-English) and 15.7M words (Spanish-English). The development data consisted of the development sets provided on the website (2000 sentences each). We trained our own word alignments, phrase table, language model, and model combination weights. The language model was a trigram model trained using the SRILM toolkit, with modified Kneser-Ney smoothing and interpolation of higher-and lowerorder ngrams. Combination weights were trained using the minimum error weight optimization procedure provided by Pharaoh. We use a two-pass decoding approach: in the first pass, Pharaoh is run in N-best mode to produce N-best lists with 2000 hypotheses per sentence. Seven different component model scores are collected from the outputs, including the distortion model score, the first-pass language model score, word and phrase penalties, and bidirectional phrase and word translation scores, as used in Pharaoh (Koehn, 2004 ). In the second pass, the N-best lists are rescored with additional language models. The resulting scores are then combined with the above scores in a log-linear fashion. The combination weights are optimized on the development set to maximize the BLEU score. The weighted combined scores are then used to select the final 1-best hypothesis. The individual rescoring steps are described in more detail below. Language Models We trained two additional language models to be used in the second pass, one word-based 4-gram model, and a factored trigram model. Both were trained on the same training set as the baseline system. The 4-gram model uses modified Kneser-Ney smoothing and interpolation of higher-order and lower-order n-gram probabilities. The potential advantage of this model is that it models n-grams up to length 4; since the BLEU score is a combination of n-gram precision scores up to length 4, the integration of a 4-gram language model might yield better results. Note that this can only be done in a rescoring framework since the first-pass decoder can only use a trigram language model. For the factored language models, a feature-based word representation was obtained by tagging the text with Rathnaparki's maximum-entropy tagger (Ratnaparkhi, 1996) and by stemming words using the Porter stemmer (Porter, 1980) . Thus, the factored language models use two additional features per word. A word history of up to 2 was considered (3gram FLMs). Rather than optimizing the FLMs on the development set references, they were optimized to achieve a low perplexity on the oracle 1-best hypotheses (the hypotheses with the best individual BLEU scores) from the first decoding pass. This is done to avoid optimizing the model on word combinations that might never be hypothesized by the firstpass decoder, and to bias the model towards achieving a high BLEU score. Since N-best lists differ for different language pairs, a separate FLM was trained for each language pair. While both the 4-gram language model and the FLMs achieved a 8-10% reduction in perplexity on the dev set references compared to the baseline language model, their perplexities on the oracle 1-best hypotheses were not significantly different from that of the baseline model. N-best List Rescoring For N-best list rescoring, the original seven model scores are combined with the scores of the secondpass language models using the framework of discriminative model combination (Beyerlein, 1998) . This approach aims at an optimal (with respect to a given error criterion) integration of different information sources in a log-linear model, whose combination weights are trained discriminatively. This combination technique has been used successfully in ASR, where weights are typically optimized to minimize the empirical word error count on a heldout set. In this case, we use the BLEU score of the N-best hypothesis as an optimization criterion. Optimization is performed using a simplex downhill method known as amoeba search (Nelder and Mead, 1965) Results The results from the first decoding pass on the development set are shown in Conclusions We have demonstrated improvements in BLEU score by utilizing more complex language models in the rescoring pass of a two-pass SMT system. We noticed that FLMs performed worse than wordbased 4-gram models. However, only trigram FLM were used in the present experiments; larger improvements might be obtained by 4-gram FLMs. The weights assigned to the second-pass language models during weight optimization were larger than those assigned to the first-pass language model, suggesting that both the word-based model and the FLM provide more useful scores than the baseline language model. Finally, we observed that the overall improvement represents only a small portion of the possible increase in BLEU score as indicated by the oracle results, suggesting that better language models do not have a significant effect on the overall system performance unless the translation model is improved as well. Acknowledgements This work was funded by the National Science Foundation, Grant no. IIS-0308297. We are grateful to Philip Koehn for assistance with Pharaoh.",1966857,2a05c9c5373a3e1e01b8161e6687b960ab3d2ff5,55,https://aclanthology.org/W05-0821,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Kirchhoff, Katrin  and
Yang, Mei",Improved Language Modeling for Statistical Machine Translation,125--128,,,,,,,inproceedings,kirchhoff-yang-2005-improved,,,
10,L02-1314,,,849597,98c7bd487d5f9239d4e654a348fad31f30eae138,15,http://www.lrec-conf.org/proceedings/lrec2002/pdf/314.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Besan{\c{c}}on, Romaric  and
Rajman, Martin",Evaluation of a Vector Space Similarity Measure in a Multilingual Framework,,,,,,,,inproceedings,besancon-rajman-2002-evaluation,,,
11,R13-1046,"We improve upon a previous line of work for parsing web data, by exploring the impact of different decisions regarding the training data. First, we compare training on automatically POS-tagged data vs. gold POS data. Secondly, we compare the effect of training and testing within sub-genres, i.e., whether a close match of the genre is more important than training set size. Finally, we examine different ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways (sentence length, perplexity). In general, we find that approximating the in-domain data has a positive impact on parsing.","We improve upon a previous line of work for parsing web data, by exploring the impact of different decisions regarding the training data. First, we compare training on automatically POS-tagged data vs. gold POS data. Secondly, we compare the effect of training and testing within sub-genres, i.e., whether a close match of the genre is more important than training set size. Finally, we examine different ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways (sentence length, perplexity). In general, we find that approximating the in-domain data has a positive impact on parsing. Introduction and Motivation Parsing data from the web is notoriously difficult, as parsers are generally trained on news data (Petrov and McDonald, 2012) . The problem, however, varies greatly depending upon the particular piece of web data: what is often termed web data is generally a combination of different sub-genres, such as Facebook posts, Twitter feeds, YouTube comments, discussion forums, blogs, etc. The language used in such data does not follow standard conventions in various respects (see Herring, 2011) : 1) The data is edited to varying degrees, with Twitter on the lower end and professional emails and blog on the upper end of the scale. 2) The sub-genres often display characteristics of spoken language, including sentence fragments and colloquialisms. 3) Some web data, especially social media data, typically contains a high number of emoticons and acronyms such as LOL. At the same time, there is a clear need to develop basic NLP technology for a variety of types of web data. To perform tasks such as sentiment analysis (Nakagawa et al., 2010) or information extraction (McClosky et al., 2011) , it helps to partof-speech (POS) tag and parse the data, as a step towards providing a shallow semantic analysis. We continue our work (Khan et al., 2013) on dependency parsing web data from the English Web Treebank (Bies et al., 2012) . We previously showed that text normalization has a beneficial effect on the quality of a parser on web data, that we can further improve the parser's accuracy by a simple, n-gram-based parse revision method, and that having a balanced training set of out-ofdomain and in-domain data provides the best results when parsing web data. The current work extends this previous work by more closely examining the data given as input for training the parser. Specifically, we take the following directions: 1. All previous experiments were carried out on gold part of speech (POS) tags. Here, we investigate using a POS tagger trained on outof-domain data, thus providing a more realistic setting for parsing web data. We specifically test the impact of training the parser on automatic POS tags (section 4). 2. The web data provided in the English Web Treebank (EWT) is divided into five different sub-genres: 1) answers to questions, 2) emails, 3) newsgroups, 4) reviews, and 5) weblogs. Figure 1 shows examples from the different sub-genres. So far, we used the whole set across these genres, which raises questions about whether a closer match of the genre is more important than the data size, and we thus investigate parsing results within  each sub-genre, and whether adding easy-toparse data to training improves performance for the difficult sub-genres (section 5). 3. Finally, from our previous work, we know that combining the EWT training set with sentences from the Penn Treebank is beneficial. However, we do not know how to best select the out-of-domain sentences. Should they be drawn randomly; should they match in size; should the sentences match in terms of parsing difficulty (cf. perplexity)? We explore different ways to match the in-domain data (section 6). Related Work There is a growing body of work on parsing web data, as evidenced by the 2012 Shared Task on Parsing the Web (Petrov and McDonald, 2012) . There have been many techniques employed for improving parsing models, including normalizing the potentially ill-formed text (Foster, 2010; Gadde et al., 2011; Øvrelid and Skjaerholt, 2012) and training parsers on unannotated or reannotated data, e.g., self-training or uptraining, (e.g., Seddah et al., 2012; Roux et al., 2012; Foster et al., 2011b,a) . Less work has gone into investigating the impact of different genres or on specific details of the sentences given to the parser. Indeed, Petrov and McDonald (2012) mention that for the shared task, ""[t]he goal was to build a single system that can robustly parse all domains, rather than to build several domain-specific systems."" Thus, parsing results were not obtained by genre. However, Roux et al. (2012) demonstrated that using a genre classifier, in order to employ specific sub-grammars, helped improve parsing performance. Indeed, the quality and fit of data has been shown for in-domain parsing (e.g. Hwa, 2001) , as well as for other genres, such as questions (Dima and Hinrichs, 2011) . One common, well-documented ailment of web parsers is the effect of erroneous tags on POS accuracy. Foster et al. (2011a,b) , e.g., note that propagation of POS errors is a serious problem, especially for Twitter data. Researchers have thus worked on improving POS tagging for web data, whether by tagger voting (Zhang et al., 2012) or word clustering (Owoputi et al., 2012; Seddah et al., 2012) . There are no reports about the impact of the quality of POS tags for trainingi.e., whether worse, automatically-derived tags might be an improvement over gold tags-though Søgaard and Plank (2012) note that training with predicted POS tags improves performance. Researchers have trained parsers using additional data which generally fits the testing domain, as mentioned above. There has been less work, however, on extracting specific types of sentences which fit the domain well. Bohnet et al. (2012) noticed a problem with parsing fragments and so extracted longer NPs to include in training as standalone sentences. From a different perspective, Søgaard and Plank (2012) weight sentences in the training data rather than selecting a subset, to better match the distribution of the target domain. In general, identifying sentences which are similar to a particular domain is a concept familiar in active learning (e.g., Mirroshandel and Nasr, 2011; Sassano and Kurohashi, 2010) , where dissimilar sentences are selected for hand-annotation to improve parsing. Experimental Setup Data For our experiments, we use two main resources, the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993) and the English Web Treebank (EWT) (Bies et al., 2012) . The EWT is comprised of approx. 16 000 sen-tences from weblogs, newsgroups, emails, reviews, and question-answers. Note that our data sets are different from the ones in Khan et al. (2013) since in the previous work we had removed sentences with POS labels AFX and GW. To create training and test sets, we broke the data into the following sets: • WSJ training: sections 02-22 (42 009 sent.) • WSJ testing: section 23 (2 416 sent.) • EWT training: 80% of the data, taking the first four out of every five sentences (13 298 sent.) • EWT testing: 20% of the data, taking every fifth sentence (3 324 sent.) The two corpora were converted from PTB constituency trees into dependency trees using the Stanford dependency converter (de Marneffe and Manning, 2008). 1 Since the EWT uses data that shows many of the characteristics of non-standard language, we decided to normalize the spelling of the EWT training and the test set. For the normalization, we reduce all web URLs to a single token, i.e., each web URL is replaced with the place-holder URL. Similarly, all emoticons are replaced by a single marker EMO. Repeated use of punctuation, e.g., !!!, is reduced to a single punctuation token. POS Tagger We use TnT (Brants, 2000) , a Markov model POS tagger using a trigram model. It it is fast to train and has a state-of-the-art model for unknown words, using a suffix trie of hapax legomena. Parser We use MSTParser (McDonald and Pereira, 2006) , 2 a freely-available parser that reaches stateof-the-art accuracy in dependency parsing for English. MST is a graph-based parser which optimizes its parse tree globally (McDonald et al., 2005) , using a variety of feature sets, i.e., edge, Evaluation For parser evaluation, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS), the percentage of dependencies which are attached correctly or attached and labeled correctly (Kübler et al., 2009) . Parser evaluation is carried out with MSTParser's evaluation module. For POS tagger evaluation, we report accuracy based on TnT's evaluation script. Significance testing was performed using the CoNLL 2007 shared task evaluation using Dan Bikel's Randomized Parsing Evaluation Comparator. 3 The effect of POS tagging We here explore the effect of POS tagging on parsing web data, to see how closely the conditions for training should match the conditions for testing. However, first we need to gauge the effect of using the TnT POS tagger out of domain. For this reason, we conducted a set of experiments, training and testing TnT in different conditions. The results are shown in table 1. They show that TnT reaches an accuracy of 96.7% when trained and tested on the WSJ. This corroborates findings by Brants (2000) . When we train TnT on EWT training data, running it on the EWT testing data delivers an accuracy of 94.28%, already 2-3% below performance on news data. However, note that the EWT is much smaller than the full WSJ. In contrast, if we train TnT on WSJ and then use it for POS tagging EWT data, we only reach an accuracy of 88.73%. Even if we balance the source and target domain data, which proved beneficial in our previous experiments on parsing (Khan et al., 2013) , we reach an accuracy of 93.48%, well below the in-domain tagging result for the EWT. This means that in contrast to parsing, the POS tagger requires less training data and profits more Given this degree of error in tagging, a parser trained with similar noise in POS tags may outperform one which is trained on gold tags. Thus, we run TnT on the training data, using a 10-fold split of the training set: each tenth of the training corpus is tagged using a POS tagger trained on the other 9 folds. Then we use the combination of all the automatically POS tagged folds and insert those POS tags into the gold standard dependency trees before we train the parser. The three conditions for POS tagging are shown in table 2. The first point to note is the impact of switching from gold to automatic POS tags: testing on TnT tags results in a degradation of about 4.5-5.5% in LAS, as compared to gold standard POS tags in the test set, consistent with typical drops in performance (e.g., Rehbein et al., 2012) . More to the point for our purposes, we see in table 2 that training a parser on automaticallyassigned POS tags outperforms a parser trained on gold POS tags. LAS increases from 77.69% to 78.54%. This supports the notion that training data should match testing closely. However, it also shows that we need to investigate methods for improving POS tagger accuracy. The effect of domain As mentioned, the EWT contains subcorpora from five different genres, and, while they share many common features (misspellings, unknown words), they have many unique properties, as illustrated in the examples in figure 1 . In terms of sentence length, domains such as weblogs lend themselves more easily to longer, more well-edited sentences, matching news data better. Reviews, on the other hand, often have shorter sentences-similar to, e.g., email greetings. Run-ons are common across genres, but we see them here in the answer and news sub-genres. The example for the answer sub-corpus shows some of the difficult challenges faced by a parser, as it contains a declarative sentence embedded within the question, where the final word (please) attaches back to the question. To gauge the effect of different sub-genres, we trained and tested the parser within each subgenre. In order to concentrate on the differences in parsing, we used gold POS tags for these experiments. Results for the five individual sub-corpora are given in the first five rows of table 3. It is noteworthy that there is nearly a 5% difference in LAS between the best sub-genre (EWT email ) and the worst (EWT answer ). We also show various properties of the sub-corpora, including number of tokens (Tokens), the average sentence length (Sen-Len), and the number of finite verbal roots (Fin-Root) 4 in training; and also the percentage of unknown word tokens in the test corpus, as compared to the training corpus (Unk.) In general, emails and reviews fare the best, likely due to a combination of shorter sentences (11.84 and 14.58, respectively) and text that tends to follow grammatical conventions. Blogs and newsgroups are in the middle, with longer, harderto-parse sentences (18.17 and 22.07, respectively) and higher levels of unknown words in testing (12.2% and 10.2%), but being consistently fairly well-edited. While it might be surprising that the results for these two sub-genres are lower than emails and reviews, note that the training for both domains is significantly lower, on the order of 10,000 words less than the other corpora. It is possible that with more data, these well-edited domains would see improved parser performance. On the lower end of the parsing spectrum is the domain of answers, which is a curious trend. There is nearly as much training data as with emails and reviews, and the average sentence length is comparable. If we look at the number of non-finite sentence roots-as a way to approximate the number of non-fragment sentences-it is nearly identical to the email sub-genre. We suspect that the fragments are not as systematic as greetings and that users may post replies quickly, leading to less well-formed text, but this deserves future consideration. Given the poor performance on the answer domain and the higher performance of the parser on Table 3 : The effect of domain on parser performance, using gold POS tags (** = sig. at the 0.01 level, testing all conditions below the line, as compared to the first row Train=EWT answer ) emails, we decided to see whether parsing could be improved by adding data to the small answer training set 1) from the domain that is easiest to parse: emails, 2) from the news domain because of its similar average sentence length, and 3) from the blog domain because it has the longest sentences. We compare these configurations with one where we add the same number of sentences, but sampled from all four remaining domains (balanced) and one where we add all the training data from all other genres (rest). We see a clear improvement for all settings, in comparison with using only the answer data for training. The best results are obtained by using all other genres as additional training data, showing that the size of the training set is the most important variable. The results also show that the sampling from all remaining sub-genres results in higher parsing accuracy than just using the easiest to parse data set, illustrating that we should not look for data which is generally easy to parse, but data which is the best fit for the test data. The effect of sentence selection In our previous work (Khan et al., 2013) , we showed that we obtain the best results when we use a balanced training corpus with the same number of sentences from the EWT and the WSJ. On the one hand, these results show that in-domain data is critical for the success of the parser; on the other hand, out-of-domain data is important to increase the size of the training set. It is thus important to find a good balance between using more training data and not overpowering the indomain data. This leads to the question of whether it is possible to choose sentences from an out-of- domain data set that are similar to the sentences in the target domain rather than just selecting a portion of consecutive sentences. In other words, can we identify sentences from the WSJ that will have the best impact on a parser for web data? In the first set of experiments, we investigate simple heuristics to choose a good set of training sentences from the WSJ: In the first experiment, we use the full WSJ (EWT+WSJ). Then we restrict the WSJ part to match the number of sentences from the EWT (EWT+WSJSent). However, since WSJ sentences are longer on average than EWT sentences, we repeat the experiment but choose the WSJ subset so that it matches the number of words in the EWT training set (EWT+WSJToken). Finally, we choose the WSJ sentences so that they match the distribution of sentence lengths in EWT (EWT+WSJDist). For example, if EWT has 100 sentences with 10 words, we select 100 sentences of length 10 from the WSJ. All of these experiments are again carried out with gold POS tags. The results of these experiments are shown in the first two parts of table 4. The results for the selection methods show that selecting the WSJ part based on the number of words results in the lowest parsing accuracy. Choosing the WSJ part based on the number of sentences or the distribution of sentence length results in the same unlabeled accuracy (UAS) of 86.34%, as compared to 86.26% for the word based selection. However, the selection based on the number of sentences results in a higher labeled accuracy of 83.83%, as opposed to 83.73% for the distribution of sentence length. We suspect that the random selection of sentences gives more variety, which is beneficial for training. However, note that the difference in the number of words in the training set across these three methods is minimal: they vary only by 41 words. In a second set of experiments, we decided to use a more informed method for choosing similar sentences: perplexity. Thus, we trained a language model on the (stemmed) words of the test set based on a 5-gram word model, and then calculated perplexity for each sentence in the WSJ, normalized by the length of the sentence. We used the CMU-Cambridge Statistical Language Modeling Toolkit 5 for calculating perplexity. Perplexity should give an approximation of distance between sentences in the two corpora. We experimented with different selection strategies: 1. Low Perplexity (LowP): We select the sentences with the lowest perplexity, i.e., the most similar ones to the test set; we restricted the number of sentences from the WSJ to match the size of the EWT training set. 2. All Low Perplexity (AllLowP): Here, we also selected sentences with low perplexity, but this time used all sentences below the median, i.e. half the WSJ sentences. 3. Low Perplexity close to the median (Med-LowP): Here, we investigate the effect of choosing sentences that are less similar to the test sentences: we select the same number of sentences as with LowP, but this time from the median down. In other words, the sentences with the lowest perplexity, i.e., the most similar sentences, are excluded. This is based on the assumption that if the chosen sentences are too similar, it will not have much effect on the trained model. 4. Mid-range Perplexity (MidP): In this set, we choose sentences that are even less similar to the test sentences. We again choose the same number of sentences as in the EWT training set, but half of them from the median and down and half from the median up. The results are in the final four rows of table 4. Interestingly, the best-performing method adds low-perplexity data to training. Thus, selecting data which is more similar to the domain helps the most. Furthermore, once the data is farther away, it starts to harm parsing performance, as can be seen in the (albeit minimal) difference between the EWT+LowP and EWT+AllLowP models. Summary and Outlook Exploring the parsing of web data, we have investigated different decisions that go into the training data, demonstrating how the better the fit of the training data to the testing data-in properties ranging from the nature of the POS tags to which sentences go into the data-the better performance the parser will have. We first compared training on automatically POS-tagged data vs. gold POS tag data, showing that performance improves by automatically tagging the training data. Next, we compared the effect of training and testing within sub-genres and saw that features such as sentence length have a strong effect. Finally, we examined ways to select out-of-domain parsed data to add to training, attempting to match the in-domain data in different shallow ways, and we found that matching training sentences to a language model improves parsing. In short, fitting the training data to the in-domain data, in even fairly superficial ways, has a positive impact on parsing results. There are several directions to take this work. First, the sentence selection methods, for example, can be combined with self-training techniques to not only increase the training data size, but to only add sentences which fit the test domain well. Secondly, the work on understanding sub-genres of web parsing deserves more thorough treatment in the future to tease apart which components are most problematic (e.g., sentence fragments), how they can be automatically identified, and how the parser can be adjusted to accommodate them.",7403488,778382f4099a5d478c1f68e1bb46fc7c26013a5c,11,https://aclanthology.org/R13-1046,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Khan, Mohammad  and
Dickinson, Markus  and
K{\""u}bler, Sandra",Towards Domain Adaptation for Parsing Web Data,357--364,,,,,,,inproceedings,khan-etal-2013-towards,,,"Syntax: Tagging, Chunking and Parsing"
12,W05-0820,"Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis.","Eleven groups participated in the event. This paper describes the goals, the task definition and resources, as well as results and some analysis. Statistical machine translation is currently the dominant paradigm in machine translation research. Annual competitions are held for Chinese-English and Arabic-English by NIST (sponsored by the US military funding agency DARPA), which creates a forum to present and compare novel ideas and leads to steady progress in the field. One of the advantages of statistical machine translation is that the currently applied methods are fairly language-independent. Building a new machine translation system for a new language pair is not much more than a matter of running a training process on a training corpus of parallel text (a text in one language paired with a translation in another). It is therefore possible to hold a competition where research groups have only a few weeks to build machine translation systems for language pairs that they have not previously worked on. We effectively demonstrated this with our shared task. For instance, seven teams built Finnish-English machine translation systems, a language pair that was certainly not of their immediate concern before. In contrast to the bigger NIST competition, we wanted to keep the barrier of entry as low as possible. We provided not only training data from the Europarl corpus (Koehn, 2005) , but also additional resources: sentence and word alignments, the decoder Pharaoh 1 (Koehn, 2004b) , and a language model, so that participation was feasible even as a graduate level class project. Using about 15 million words of translated text, participants were asked to build a phrase-based statistical machine translation system. The focus of the task was to build a probabilistic phrase translation table, since most of the other resources were provided -for more on phrase-based statistical machine translation, refer to Koehn et al. (2003) . The participants' systems were compared by how well they translated 2000 previously unseen test sentences from the same domain. The shared task operated within an extremely short timeframe. The workshop and hence the shared task was accepted on February 22, 2005 and announced on March 3. The official test data was made available on April 3, results were due one week later. Despite this tight schedule, eleven research groups participated and built a total of 32 machine translation systems for the four language pairs. Goals When setting up this competition, we were motivated by a number of goals. We set out to: Create a platform to demonstrate the effectiveness of novel ideas: The research community is easily balkanized, where different groups work on different data sets and under different conditions, so that it becomes often hard to assess, how effective a novel method is. By creating an environment with common test and training sets, language model, preprocessing, and even decoder, the effect of other model choices can be more easily demonstrated. Work on new language pairs, new problems: Different language pairs pose different challenges. We picked Finnish-English and German-English for the special problems of rich morphology, word order, which are a challenge to current phrase-based SMT methods. Enable more researchers to get engaged in SMT research: One of our main goals with providing as many resources as possible was to keep the barrier of entry low. Participants could use the word alignment and other resources and focus on phrase extraction. We hoped to attract researchers that are relatively new to the field. We were satisfied to learn that many entries are by graduate students working on their own. Promote and create free resources: Academic research thrives on freely available resources. The field of statistical machine translation has been blessed with a long tradition of freely available software tools -such as GIZA++ (Och and Ney, 2003) -and parallel corpora -such as the Canadian Hansards 2 . Following this lead, we made word alignments and a language model available for this competition in addition to our previously published resources (Europarl and Pharaoh). The competition created resources as well. Most teams agreed to share system output and their model files. You can download them from the competition web site 3 . Promote work on European language pairs: Finally, we wanted to promote work on European languages. The increasing economic and political ties within the European Union create a huge need for translation services. We would like to see researchers rise to the challenge of creating high quality machine translation systems to fill these needs. We are very grateful for the strong participation, especially by researchers who are relatively new to the field. Rules of Engagement We set up a machine translation competition for four language pairs. We chose Spanish-English and French-English, because many researchers would be familiar with these languages. We chose German-English for its special problems with word order (such as nested constructions and split verb groups) and morphology. Finally, we picked Finnish-English for the rich agglutinative morphology of Finnish. Statistical machine translation systems are typically trained on sentence-aligned parallel corpora. We selected Europarl 4 , a freely available parallel corpus in eleven languages. In addition, we also made a word alignment available, which was derived using a variant of the current default method for word alignment -Och and Ney (2003)'s refined method. Figure 1 details some properties of the parallel corpora. The training corpus is most of the Europarl corpus, only the text of sessions from last quarter of the year 2000 was reserved for testing. The corpus has the size of roughly 15 million English words in 700,000 sentences -these numbers differ for each of the four parallel corpora due to the different number of discarded sentences during sentence alignment and after enforcing a 40 word length limit for sentences. The number of foreign words differs even more dramatically. The effect of Finnish morphology manifests itself in a low number of words (just over 11 million), but a high number of distinct words (more than 5 times as many as in the English half). The test corpus consists of 2000 sentences aligned across all five languages. Note that the output of each system is compared against the same English references for all source languages. The number of total words, distinct words, and words not seen in the training data reflects again the morphology effect. For researchers willing to create their own word alignment, we suggested the use of GIZA++ 5 , an implementation of the IBM word-based machine translation models, which also assisted the creation of the provided word alignments. We trained a language model on the English part Spanish-English French-English Finnish of the Europarl corpus using the SRI language modeling toolkit (Stolke, 2002) . Finally, we suggested the use of Pharaoh (Koehn, 2004b) , a phrase-based machine translation decoder. How well does this setup match the state of the art? The MIT system using the Pharaoh decoder (Koehn, 2004a) proved to be very competitive in last year's NIST evaluation. However, the field is moving fast, and a number of steps help to improve upon the provided baseline setup, e.g., larger language models trained on general text (up to a billion words have been used), better reodering models (e.g., suggested by Tillman (2004) and Och et al. (2004) ), better language-specific preprocessing (Koehn and Knight, 2003) and restructuring (Collins et al., 2005) , additional feature functions such as word class language models, and minimum error rate training (Och, 2003) to optimize parameters. Some of these steps (e.g., improved reordering models) go beyond the current capabilities of Pharaoh. However, we are hopeful that freely available software continues to match or at least follow closely the state of the art. We announced the shared task on March 3, and provided all the resources mentioned above (also a development test corpus to track the quality of systems being developed). The test schedule called for the translation of 2000 sentence for each of the four language pairs in the week between April 3-10. We allowed late submissions up to April 17. Results Eleven teams from eight institutions in Europe and North America participated, see Figure 2 for a complete list. The figure also indicates, if a team used the Pharaoh decoder (eight teams), the provided language model (seven teams) and the provided word alignment (four did, three of those with additional preprocessing or additional data). Translation performance was measured using the BLEU score (Papineni et al., 2002) , which measures n-gram overlap with a reference translation. In our case, we only used a single reference translation, since the test set was taken from a held-out portion of the Europarl corpus. On the other hand we used a relatively large number of test sentences to guarantee that the BLEU results are stable despite the fact that we used only one reference translation for each sentence. Shared tasks like this one, of course, bring out the competitive spirit of participants and can draw criticisms about being a horse race. From an outside perspective, however, it is far more interesting to learn which methods and ideas proved to be successful, than who won the competition. Taking stock of the results -see Figure 3 -one observes a very packed field at the top. While the participants from the University of Washington produced the best translations for every single language pair, the distance to many other participant scores ID Team Pharaoh is within a BLEU percentage point or two. As one might have expected, the scores are best for Spanish and French, and worst for Finnish. Figure 4 shows some typical output of the submitted systems. The proceedings to the workshop include detailed system descriptions of all participants. Novel phrase extraction approaches were proposed, along with better preprocessing, language modeling, rescoring, and other ideas. We are certain that better performance can be achieved by combining some of the methods used by different participants. And hence, we would like to pose the challenge to the research community to build and test better systems using the provided resources. We will gladly list additional results on the competition web site. Survey Following the end of the competition, we sent out a questionnaire to the participants. One of the questions what they would like to see different in a potential future competition. We listed four potential changes: 70% of the respondends checked translation from English, 50% checked out of domain test data, 40% checked more language pairs, 0% checked fewer language pairs. Additional suggestions were: alternatives to the BLEU scoring method (maybe human judgment by participants themselves), transitive translation using pivot languages, translation of resource-poor languages, and more time to prepare for the task. Outlook Given the short timeframe, one should view the system performances (albeit very competitive with the state of the art) as a baseline effort on the task of open domain text translation between European languages. We hope that future researchers will use the provided environment as a test bed for their machine translation systems. We will continue to publish any scores reported to us. Since we placed much of the systems' output online, the interested reader may be inspired to more closely explore the quality and shortcomings. Even some of the model files have been made available, so it is even possible to download and install some of the systems. Spanish-English Reference We know all too well that the present Treaties are inadequate and that the Union will need a better and different structure in future , a more constitutional structure which clearly distinguishes the powers of the Member States and those of the Union . Input Spanish Sabemos muy bien que los Tratados actuales no bastan y que , en el futuro , será necesario desarrollar una estructura mejor y diferente para la Unión Europea , una estructura más constitucional que también deje bien claras cuáles son las competencias de los Estados miembros y cuáles pertenecen a la Unión . Best system (Spanish-English) we all know very well that the current treaties are not enough and that , in the future , it will be necessary to develop a structure better and different for the european union , a structure more constitutional also make it clear what the competences of the member states and what belongs to the union . Worst System (Spanish-English) we know very well that the current treaties not enough and that , in the future , will be necessary develop a better structure and different to the european union , a structure more constitutional that also be well clear the powers of the member states and what belong to the union . Input French Nous savons très bien que les Traités actuels ne suffisent pas et qu ' il sera nécessaire à l ' avenir de développer une structure plus efficace et différente pour l ' Union , une structure plus constitutionnelle qui indique clairement quelles sont les compétences des états membres et quelles sont les compétences de l ' Union . Best system (French-English) we know very well that the current treaties are not enough and that it will be needed in the future to develop a structure more effective and different for the union , a structure more constitutional which clearly indicates what are the competence of member states and what are the powers of the union . Input Finnish Tiedämme oikein hyvin , että nykyiset perustamissopimukset eivät ole riittäviä ja että tulevaisuudessa on tarpeen kehittää unionille parempi ja toisenlainen rakenne , siis perustuslaillisempi rakenne , jossa mys ilmaistaan selkeämmin , mitä jäsenvaltioiden ja unionin toimivaltaan kuuluu Best system (Finnish-English) we know very well that the existing founding treaties do not need to be developed for the union and a different structure , therefore perustuslaillisempi structure , which also expresses clearly what the member states and the union 's competence is not sufficient and that better in the future . Input German Uns ist sehr wohl bewusst , dass die geltenden Verträge unzulänglich sind und künftig eine andere , effizientere Struktur für die Union entwickelt werden muss , nämlich eine stärker konstitutionell ausgeprägte Struktur mit einer klaren Abgrenzung zwischen den Befugnissen der Mitgliedstaaten und den Kompetenzen der Union . Best system (German-English) the union must be developed , with a major institutional structure with a clear demarcation between the powers of the member states and the competences of the union is well aware that the existing treaties are inadequate and in the future , a different , more efficient structure for us .",9271055,5025a4b2b724867f7b6b24f3e9253a61b76a3406,83,https://aclanthology.org/W05-0820,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Koehn, Philipp  and
Monz, Christof",Shared Task: Statistical Machine Translation between {E}uropean Languages,119--124,,,,,,,inproceedings,koehn-monz-2005-shared,,,
13,L02-1315,,,781377,35f7addff6dc85673b2d0b467e83dfd38c515952,5,http://www.lrec-conf.org/proceedings/lrec2002/pdf/315.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Yablonsky, Serge A.",Corpora as Object-Oriented System. From {UML}-notation to Implementation,,,,,,,,inproceedings,yablonsky-2002-corpora,,,
14,R13-1047,"In this work, we present a new task for testing compositional distributional semantic models. Recently, there has been a spate of research into how distributional representations of individual words can be combined to represent the meaning of phrases. Vecchi et al. (2011) have shown that some compositional models, including the additive and multiplicative models of Mitchell and Lapata (2008; 2010)   and the linear map-based model of Baroni and Zamparelli (2010) , can be applied to detect semantically anomalous adjectivenoun combinations. We extend their experiments and apply these models to the combinations extracted from texts written by learners of English. Our work contributes to the field of compositional distributional semantics by introducing a new test paradigm for semantic models and shows how these models can be used for error detection in language learners' content word combinations.","In this work, we present a new task for testing compositional distributional semantic models. Recently, there has been a spate of research into how distributional representations of individual words can be combined to represent the meaning of phrases. Vecchi et al. (2011) have shown that some compositional models, including the additive and multiplicative models of Mitchell and Lapata (2008; 2010)   and the linear map-based model of Baroni and Zamparelli (2010) , can be applied to detect semantically anomalous adjectivenoun combinations. We extend their experiments and apply these models to the combinations extracted from texts written by learners of English. Our work contributes to the field of compositional distributional semantics by introducing a new test paradigm for semantic models and shows how these models can be used for error detection in language learners' content word combinations. Introduction Vector-based (distributional) models are widely used for representing the meaning of single words. They rely on the assumption that word meaning can be learned from the linguistic environment and can be approximated by a word's distribution across contexts. Words are represented as vectors in a high-dimensional space, with vector dimensions encoding word co-occurrence with contextual elements -other words within a local window, words linked by specific dependencies to the target word, and so forth. Distributional models provide a clear basis for interpreting word meaning, as well as a simple means for measuring semantic similarity. These properties have been exploited in many NLP tasks, including automatic thesaurus extraction (Grefenstette, 1994) , word sense induction (Schütze, 1998) and disambiguation (McCarthy et al., 2004) , collocation extraction (Schone and Jurafsky, 2001) and others. In contrast to single words, the distribution of phrases cannot be used as a reliable approximation of their meaning, as phrase vectors are much sparser. Irrespective of the size of the corpus considered, some content word combinations will remain unattested as a consequence of their Zipf-like distributions. For example, Vecchi et al. (2011) have shown that both semantically acceptable and semantically deviant word combinations will be absent from large English corpora. A promising alternative is to use compositional models which combine distributional vectors for the component words in some way, for example, using a direct vector combination function (Kintsch, 2001; Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) or linear transformations on vectors (Baroni and Zamparelli, 2010) . In spite of the spate of recent work in this area, the question of how to combine word representations is far from answered. Compositional models can be assessed by their ability both to provide a solid theoretical basis for meaning composition and to represent composite meaning for relevant practical tasks. Promising results have been shown with such models on similarity detection and paraphrase ranking (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2010) , adjectivenoun vector prediction (Baroni and Zamparelli, 2010) and semantic anomaly detection (Vecchi et al., 2011) . Of these tasks, the latter appears to be particularly challenging since it addresses the ability of compositional models to account for linguistic productivity. No corpus can effectively sample all possible content word combinations. On the other hand, some corpus-attested word combinations may appear semantically deviant when considered out of context (for example, when they are used metaphorically). Vecchi et al. (2011) have focused on unattested adjective-noun (AN) combinations and noted that if a combination does not occur in a corpus, it may be due to various reasons including data sparsity as well as nonsensicality. The task of distinguishing between the two cases is challenging. Vecchi et al. use the following examples: (1) a. blue rose b. residential steak Whereas both may well be unattested in a corpus, the concept of blue rose is perfectly conceivable while that of residential steak is nonsensical and only interpretable in specifically-constructed discourse contexts. Vecchi et al. argue that there should be a detectable difference between the model-generated representations for the semantically deviant combinations and those for the acceptable ones, and assess compositional models by their ability to capture this difference. Vecchi et al. have created a set of corpus-unattested AN combinations, annotated them as semantically acceptable or deviant, and applied the additive (add) and multiplicative (mult) models of Mitchell and Lapata (2008) and adjective-specific linear maps (alm) of Baroni and Zamparelli (2010) . Given that promising results have been obtained in their experiments, we propose that a useful extension of this task is to test the compositional models on errors in content word combinations extracted from texts written by learners of English. This task provides a natural setting for testing semantic models on genuine examples and is a potential practical application for such models. Language learners' errors are diverse, but many of them can naturally be explained in terms of nonproductive, semantically anomalous combination of content words (Leacock et al., 2010) . Learners may lack robust intuitions about words' selectional preferences and subtle differences in meaning, so they may confuse near-synonyms, overuse words with broad meaning, and otherwise choose words inappropriately. Consider the following examples extracted from our data: (2) a. These examples illustrate that learner errors can often be explained by confusions stemming from similar meaning (2a) or form (2b). When a word combination appears to be nonsensical as in 2c, the words chosen might still be related to the appropriate ones in the learner's mental lexicon. We recognise that although error detection in learners' content word combinations is a natural extension to semantic anomaly detection, it also poses additional difficulties that semantic models might not be able to deal with. For example, some erroneous word combinations may not be completely devoid of compositional meaning, while violating language conventions. However, semantic models might still be able to capture some of these conventions. Another challenge is that some expressions cannot be unambiguously classified as either correct or incorrect, as their interpretation depends on the context of use: best moment (2d) is appropriate when used to denote a short period of time, but it is often incorrectly used by learners instead of best time. To make our work comparable with previous work on semantic anomaly, we investigate AN combinations extracted from texts written by nonnative speakers of English, and apply the add, mult and alm models of semantic composition. The main contributions of this work are to show that error detection in content word combinations provides a natural testbed and useful application for the compositional distributional models, and that the results obtained on this task provide a more natural estimate of the models' performance than ones based on artificially constructed examples. If the compositional distributional models can distinguish between correct and incorrect content word combinations, these models can then be used for writing or pedagogical assistance. To the best of our knowledge, this is the first attempt to handle learner errors in the choice of content words using compositional distributional semantics. Plan of the paper. We overview related work on error detection and discuss the three models of semantic composition in Section 2. Section 3 presents the data and experimental setup. We discuss the results of our experiments in Section 4 and conclude in Section 5. 2012). Such errors are more frequent, but they are also more systematic which makes them easier to detect. Function words constitute a closed class, so the set of possible corrections is also limited. By comparison, errors in content word combinations pose a bigger challenge. Since content words primarily express meaning rather than encode syntax, detection and correction of such errors depend on a system's ability, in the limit, to recognise the communicative intent of the writer. Moreover, the set of possible corrections is much larger than for function words. Previous work has either focused on correction alone assuming that errors are already detected (Liu et al., 2009; Dahlmeier and Ng, 2011) , or has reformulated the task as writing improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al., 2008; Yi et al., 2008) . In the former case error detection, which is a difficult task in itself, is not addressed, while in the latter case it is integrated into that of suggesting alternatives according to some metric (for example, frequency or mutual information). In some cases, a database of typical errors in word combinations is collected from learner texts and suggestions are only made for these errorprone combinations. Otherwise suggestions will be made for many acceptable phrases. In this work, we treat error detection in the choice of content words as an independent task and assess the ability of compositional distributional models to discriminate incorrect from correct AN combinations -a frequent source of error in learner texts. Composition by Component-wise Operations In the additive and multiplicative compositional models of Mitchell and Lapata (2008; 2010) , the components of the composite vector are obtained by component-wise operations applied to the word vectors. If c is a word combination vector and a and b are word vectors, then c's i-th component is the sum of the i-th components of a and b for the add model: c i = a i + b i (1) and the product of the corresponding components for the mult model: c i = a i b i (2) An advantage of using these models is that they provide a clear and simple interpretation of vector composition, requiring no training or tuning. They have also been shown to be promising models of composition in a number of NLP tasks, including semantic anomaly detection (Vecchi et al., 2011) . However, the principal weakness of these models is that they use commutative operations, and therefore fail to represent the difference in the grammatical function of the component words, their order, and ""headedness"". For example, these models would produce the same composite vectors for component vector and vector component. In addition, the add model does not take ""incompatibility"" of constituent vectors along individual dimensions into account. If one vector has a high value in its i-th dimension while another vector has 0, the composed vector will receive the high value from the first input vector, even though, intuitively, this dimension should get 0 or near-0 value. This problem does not arise with the mult model. On the other hand, the mult model is heavily biased towards dimensions with high values in both input vectors (Baroni et al., 2012) . Distributional Functions and Linear Maps The adjective-specific linear maps of Baroni and Zamparelli (2010) take the grammatical functions of the words within a combination into account. Focusing on AN combinations, they try to model the fact that adjectives modify nouns and the resulting combination is nominal. They note that the meaning of nouns can be represented with their distributional vectors, but the meaning of attributive adjectives cannot be fully captured by their distribution alone: for example, new in new friend is not the same as new in new shoes. The meaning of the adjective new is defined through its application to the denotations of the nouns. Therefore, Baroni and Zamparelli (2010) suggest treating adjectives as distributional functions that map between semantic vectors representing nouns to ones representing AN combinations. Within this approach, adjectives are represented with weight matrices. The composition is defined by matrix-by-vector multiplication as follows: f (noun) = def F × a = b (3) where F is the matrix representing an adjective and encoding function f, which maps the input noun vector a to the output AN vector b. The ij-th cell of the matrix contains the weight determining how much the component corresponding to the jth context element in the noun vector contributes to the value assigned to the i-th context element in the AN vector (Baroni et al., 2012) . These weights are estimated separately for each adjective from all corpus-observed noun-AN vector pairs using (multivariate) partial least squares regression. 3 Experimental Setup Test Data We have extracted a set of AN combinations from the publicly available CLC-FCE dataset (Yannakoudakis et al., 2011) , a subset of the Cambridge Learner Corpus (CLC), 1 which is a large corpus of texts produced by English language learners sitting Cambridge Assessment's examinations. 2  These texts have been manually errorcoded (Nicholls, 2003) . Using the error annotation, we have divided extracted ANs into two subsets -correctly used ANs and those that are annotated with error codes due to inappropriate choice of an adjective or/and noun. 3 For the ANs that are used correctly in some contexts and incorrectly in others we use the most frequent annotation from the data. Our test set contains 4681 correct and 530 incorrect combinations. In contrast to Vecchi et al. (2011) , who have used a limited set of constituent adjectives and nouns and an approximately equal number of semantically acceptable and deviant combinations, our test set is more skewed towards correct combinations and consists of a wider range of constituent words. It also includes ANs occurring in the BNC 4 -3294 of the correct test ANs and 256 of the incorrect ones are corpus-attested. The set of corpus-attested ANs annotated as incorrect in our data includes lowfrequency combinations from the BNC, as well as combinations whose error-annotation depends on context. We believe that this test set reflects practical applications of semantic anomaly detection more closely. Semantic Space Construction In constructing the semantic space we follow the procedure outlined in Vecchi et al. (2011) . We populate the semantic space with a large number of distributional vectors for the target elements -constituent nouns and adjectives from the test ANs, and the most frequent nouns and adjectives from a corpus of English as well as AN combinations of these words. To estimate the frequency rankings, we use a concatenation of two wellformed English corpora -the 100M word BNC and the Web-derived 2B word ukWaC corpus. 6  The semantic space is represented by a matrix encoding word co-occurrences, with the rows representing the target elements and the columns representing a set of 10K context words consisting of 6,590 nouns, 1,550 adjectives and 1,860 verbs most frequent in the combined corpus. The ijth cell of the original matrix contains a sentenceinternal co-occurrence count of the i-th target element with the j-th context word. The raw sentence-internal co-occurrence counts from the original matrix have been transformed into Local Mutual Information scores (Baroni and Zamparelli, 2010; Evert, 2005) . An interesting research question is how much data are needed to obtain reliable word cooccurrence counts. We estimate the word cooccurrence statistics using the BNC only, and leave it for future research to explore the impact of estimating them from larger corpora, for example, the ukWaC or the concatenated corpus mentioned above. We lemmatise, tag and parse the data with the RASP system (Briscoe et al., 2006; Andersen et al., 2008) , and extract all statistics at the lemma level. The target elements are selected as follows: we first select the 4K adjectives and 8K nouns which are most frequent in the concatenated corpus. In each case, we exclude the top 50 most frequent words since those may have too general meanings. Next, we extract the constituent adjectives and nouns from our test data and populate the semantic space with the words not yet contained in it. As a result, our semantic space contains 8,364 nouns. Since we aim at investigating AN behaviour in a highly-populated semantic space, we add more AN combinations to that. We select 218 very frequent adjectives (occurring more than 100K but We perform all operations on vectors in the full semantic space, using a 76,053 × 10K matrix. We leave it for future research to perform dimensionality reduction (for example, using Singular Value Decomposition) and to compare the results with the ones reported here. Composition Methods For the add and mult models, the AN vectors are obtained by component-wise addition and multiplication without normalisation. For the alm model, the weight coefficients are estimated with multivariate partial least squares regression using the R pls package (Mevik and Wehrens, 2007) , using the leave-one-out training regime. This model is computationally expensive since a separate weight matrix must be learned for each adjective and since we use the non-reduced semantic space. Therefore, for the experiments presented here we limit the number of test adjectives to 38. The selected adjectives are, on the one hand, frequently misused by language learners, and, on the other, have a manageable number of training examples. The reduced set of test ANs consists of 347 combinations. The number of latent variables used by the training algorithm depends on the number of available noun-AN training pairs. We have gradually changed this number from 3 to 20 depending on the adjective and the number of available training pairs with the aim of keeping the independentvariable-to-training-item ratio stable. However, we have not optimised this number and leave it for future research. Measures of Semantic Anomaly Once the composite vectors are obtained, the next question is how to distinguish between the vectors for correct and anomalous combinations. Vecchi et al. (2011) propose three simple measures for distinguishing between the two sets of vectors: 1. Vector Length (VLen): they hypothesise that vectors for anomalous ANs are shorter than those for acceptable ones. Since the distributional vectors encode word occurrence, words that do not ""match"" semantically should have their co-occurrence counts distributed differently along the dimensions, and their composition is expected to have many near-0 values. 2. Cosine with the Noun Vector (CosN): they hypothesise that in nonsensical ANs the meaning of the input nouns is degraded and their model-generated vectors are situated further away from the original noun vectors. For example, since a big dog is still a dog and an *extensive dog is less clearly so, in the semantic space the vector for big dog would be closer to that of dog than the vector for *extensive dog to dog. Semantically deviant ANs are expected to have lower cosine between their vectors and the original noun vectors. Density of the AN Neighbourhood (Dens): it is hypothesised that deviant ANs will have fewer close neighbours and be more ""isolated"" in the semantic space. This is measured by the average cosine with the top 10 nearest neighbours, which is assumed to be lower for anomalous ANs. We hypothesise that some cues alternative to the ones already proposed may also be effective: The models can be assessed by their ability to place the AN vector in the neighbourhood populated by similar words and combinations. We measure this as the proportion of nearest neighbours containing same constituent words as in the tested ANs. Results We use the measures described above and compute the difference between the mean values for the correct and incorrect model-generated ANs. We apply the unpaired t-test, assuming a twotailed distribution, to assess the statistical significance of the difference between these values. In Tables 1 to 3 we report p values estimating statistical significance at the 0.05 level, and statistical significance is marked with an asterisk ( * ). We assume that there might be a difference between the corpus-attested and corpus-unattested Vecchi et al. (2011) . We report the results on the full set of test ANs, as well as on each of the two subgroups separately. Our goals are to: • comparatively evaluate performance of the three composition models; • assess the appropriateness of the proposed metrics; • investigate models' performance on the corpus-attested and corpus-unattested combinations. Comparative Performance of the Models Of the three composition models, the mult model (Table 2 ) shows the best results overall. The alm model (Table 3 ) shows statistically significant difference between the model-generated vectors for the correct and incorrect combinations with the cosines and component overlap, but it does not detect the difference on the corpusunattested subset with any of the metrics. The add model (Table 1 ) shows statistically significant differences only with the cosine measures on the corpus-unattested subset. The poor performance of this model may be due to its weaknesses outlined in Section 2.2. Also, Baroni and Zamparelli (2010) note that normalisation may help improving its performance. Appropriateness of the Metrics Cosines to the original input vectors show promising results with all three models. In contrast to the results reported by Vecchi et al. (2011) With COver, the alm model, followed by the mult model, produce sensible results. Table 4 shows the top 3 nearest neighbours found by the models for the correct AN bad intention and the incorrect * bad information. The latter is annotated as incorrect since its meaning is quite vague and a possible correction is inaccurate information. Note that only the alm model is able to discriminate between the correct and the incorrect word combinations suggesting sensible nearest neighbours for bad intention and less sensible ones for *bad information. Attested vs Unattested Combinations Our results show that the models perform differently on the two subsets and somewhat better on corpus-attested ANs. However, the results also confirm that appropriate models and metrics can be found to distinguish between correct and incorrect ANs in both subsets. Conclusion In this paper we have introduced a new task on which compositional distributional semantic models can be tested. Our results support the hypothesis that semantic models can be applied to detect errors in the choice of content words by English language learners. The original contribution of our paper is to show how compositional and distributional semantics can be linked to error detection to provide a solution to a practical task. Our results suggest that with the metrics considered it is easier to detect the difference between the model-generated vectors for the correct and incorrect word combinations with the multiplicative model. On the other hand, qualitative analysis suggests that the adjective-specific linear maps of Baroni and Zamparelli (2010) are superior, since they place the model-generated vectors in semantically sensible neighbourhoods. We plan to investigate further whether the use of a bigger corpus for collecting word co-occurrence statistics provides more reliable counts, and whether dimensionality reduction and/or normalisation of the models improves the results. We also plan to apply the alm model to a larger number of examples. Some other models such as the ones by Erk and Padó (2008) and Thater et al. (2010) which take selectional preferences and context into account may yield better results on this task, and we plan to test this experimentally in the future. Finally, since these models can discriminate between correct and anomalous combinations, the next step is to incorporate them into an error detection classifier. Acknowledgments We are grateful to Cambridge ESOL, a division of Cambridge Assessment, and Cambridge University Press for supporting this research and for granting us access to the CLC for research purposes. We would like to thank Helen Yannakoudakis, Øistein Andersen and the anonymous reviewers for their valuable comments.",2465888,268c6aa93f799472085c1efb976e18878ba12a97,13,https://aclanthology.org/R13-1047,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Kochmar, Ekaterina  and
Briscoe, Ted",Capturing Anomalies in the Choice of Content Words in Compositional Distributional Semantic Space,365--372,,,,,,,inproceedings,kochmar-briscoe-2013-capturing,,,Semantics: Lexical Semantics
15,W05-0823,"This work discusses translation results for the four Euparl data sets which were made available for the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"". All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model.","This work discusses translation results for the four Euparl data sets which were made available for the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"". All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model. Introduction During the last decade, statistical machine translation (SMT) systems have evolved from the original word-based approach (Brown et al., 1993) into phrase-based translation systems (Koehn et al., 2003) . Similarly, the noisy channel approach has been expanded to a more general maximum entropy approach in which a log-linear combination of multiple models is implemented (Och and Ney, 2002) . The SMT approach used in this work implements a log-linear combination of feature functions along with a translation model which is based on bilingual n-grams. This translation model was developed by de Gispert and Mariño (2002) , and it differs from the well known phrase-based translation model in two basic issues: first, training data is monotonously segmented into bilingual units; and second, the model considers n-gram probabilities instead of relative frequencies. This model is described in section 2. Translation results from the four source languages made available for the shared task (es: Spanish, fr: French, de: German, and fi: Finnish) into English (en) are presented and discussed. The paper is structured as follows. Section 2 describes the bilingual n-gram translation model. Section 3 presents a brief overview of the whole SMT procedure. Section 4 presents and discusses the shared task results and other interesting experimentation. Finally, section 5 presents some conclusions and further work. Bilingual N-gram Translation Model As already mentioned, the translation model used here is based on bilingual n-grams. It actually constitutes a language model of bilingual units which are referred to as tuples (de Gispert and Mariño, 2002) . This model approximates the joint probability between source and target languages by using 3grams as it is described in the following equation: p(T, S) ≈ N n=1 p((t, s) n |(t, s) n−2 , (t, s) n−1 ) (1) where t refers to target, s to source and (t, s) n to the n th tuple of a given bilingual sentence pair. Tuples are extracted from a word-to-word aligned corpus according to the following two constraints: first, tuple extraction should produce a monotonic segmentation of bilingual sentence pairs; and second, the produced segmentation is maximal in the sense that no smaller tuples can be extracted without violating the previous constraint (Crego et al., 2004) . According to this, tuple extraction provides a unique segmentation for a given bilingual sentence pair alignment. Figure 1 illustrates this idea with a simple example. Two important issues regarding this translation model must be mentioned. First, when extracting tuples, some words always appear embedded into tuples containing two or more words, so no translation probability for an independent occurrence of such words exists. To overcome this problem, the tuple 3-gram model is enhanced by incorporating 1-gram translation probabilities for all the embedded words (de Gispert et al., 2004) . Second, some words linked to NULL end up producing tuples with NULL source sides. This cannot be allowed since no NULL is expected to occur in a translation input. This problem is solved by preprocessing alignments before tuple extraction such that any target word that is linked to NULL is attached to either its precedent or its following word. SMT Procedure Description This section describes the procedure followed for preprocessing the data, training the models and optimizing the translation system parameters. Preprocessing and Alignment The Euparl data provided for this shared task (Euparl, 2003) was preprocessed for eliminating all sentence pairs with a word ratio larger than 2.4. As a result of this preprocessing, the number of sentences in each training set was slightly reduced. However, no significant reduction was produced. In the case of French, a re-tokenizing procedure was performed in which all apostrophes appearing alone were attached to their corresponding words. For example, pairs of tokens such as l ' and qu ' were reduced to single tokens such as l' and qu'. Once the training data was preprocessed, a wordto-word alignment was performed in both directions, source-to-target and target-to-source, by using GIZA++ (Och and Ney, 2000) . As an approximation to the most probable alignment, the Viterbi alignment was considered. Then, the intersection and union of alignment sets in both directions were computed for each training set. Feature Function Computation The considered translation system implements a total of five feature functions. The first of these models is the tuple 3-gram model, which was already described in section 2. Tuples for the translation model were extracted from the union set of alignments as shown in Figure 1 . Once tuples had been extracted, the tuple vocabulary was pruned by using histogram pruning. The same pruning parameter, which was actually estimated for Spanish-English, was used for the other three language pairs. After pruning, the tuple 3-gram model was trained by using the SRI Language Modeling toolkit (Stolcke, 2002) . Finally, the obtained model was enhanced by incorporating 1-gram probabilities for the embedded word tuples, which were extracted from the intersection set of alignments. Table 1 presents the total number of running words, distinct tokens and tuples, for each of the four training data sets. The second feature function considered was a target language model. This feature actually consisted of a word 3-gram model, which was trained from the target side of the bilingual corpus by using the SRI Language Modeling toolkit. The third feature function was given by a word penalty model. This function introduces a sentence length penalization in order to compensate the sys-tem preference for short output sentences. More specifically, the penalization factor was given by the total number of words contained in the translation hypothesis. Finally, the fourth and fifth feature functions corresponded to two lexicon models based on IBM Model 1 lexical parameters p(t|s) (Brown et al., 1993) . These lexicon models were calculated for each tuple according to the following equation: p lexicon ((t, s) n ) = 1 (I + 1) J J j=1 I i=0 p(t i n |s j n ) (2) where s j n and t i n are the j th and i th words in the source and target sides of tuple (t, s) n , being J and I the corresponding total number words in each side of it. The forward lexicon model uses IBM Model 1 parameters obtained from source-to-target alignments, while the backward lexicon model uses parameters obtained from target-to-source alignments. Decoding and Optimization The search engine for this translation system was developed by Crego et al. (2005) . It implements a beam-search strategy based on dynamic programming and takes into account all the five feature functions described above simultaneously. It also allows for three different pruning methods: threshold pruning, histogram pruning, and hypothesis recombination. For all the results presented in this work the decoder's monotonic search modality was used. An optimization tool, which is based on a simplex method (Press et al., 2002) , was developed and used for computing log-linear weights for each of the feature functions described above. This algorithm adjusts the log-linear weights so that BLEU (Papineni et al., 2002) is maximized over a given development set. One optimization for each language pair was performed by using the 2000-sentence development sets made available for the shared task. Shared Task Results Table 2 presents the BLEU scores obtained for the shared task test data. Each test set consisted of 2000 sentences. The computed BLEU scores were case insensitive and used one translation reference. es -en fr -en de -en fi -en 0.3007 0.3020 0.2426 0.2031 As can be seen from Table 2 the best ranked translations were those obtained for French, followed by Spanish, German and Finnish. A big difference is observed between the best and the worst results. Differences can be observed from translation outputs too. Consider, for example, the following segments taken from one of the test sentences: es-en: We know very well that the present Treaties are not enough and that , in the future , it will be necessary to develop a structure better and different for the European Union... fr-en: We know very well that the Treaties in their current are not enough and that it will be necessary for the future to develop a structure more effective and different for the Union... de-en: We very much aware that the relevant treaties are inadequate and , in future to another , more efficient structure for the European Union that must be developed... fi-en: We know full well that the current Treaties are not sufficient and that , in the future , it is necessary to develop the Union better and a different structure... It is evident from these translation outputs that translation quality decreases when moving from Spanish and French to German and Finnish. A detailed observation of translation outputs reveals that there are basically two problems related to this degradation in quality. The first has to do with reordering, which seems to be affecting Finnish and, specially, German translations. The second problem has to do with vocabulary. It is well known that large vocabularies produce data sparseness problems (Koehn, 2002) . As can be confirmed from Tables 1 and 2 , translation quality decreases as vocabulary size increases. However, it is not clear yet, in which degree such degradation is due to monotonic decoding and/or vocabulary size. Finally, we also evaluated how much the full feature function system differs from the baseline tuple 3-gram model alone. In this way, BLEU scores were computed for translation outputs obtained for the baseline system and the full system. Since the English reference for the test set was not available, we computed translations and BLEU scores over de-velopment sets. Table 3 presents the results for both the full system and the baseline. 1   Table 3 : Baseline-and full-system BLEU scores (computed over development sets). language pair baseline full es -en 0.2588 0.3004 fr -en 0.2547 0.2938 de -en 0.1844 0.2350 fi -en 0.1526 0.1989 From Table 3 , it is evident that the four additional feature functions produce important improvements in translation quality. Conclusions and Further Work As can be concluded from the presented results, performance of the translation system used is much better for French and Spanish than for German and Finnish. As some results suggest, reordering and vocabulary size are the most important problems related to the low translation quality achieved for German and Finnish. It is also evident that the bilingual n-gram model used requires the additional feature functions to produce better translations. However, more experimentation is required in order to fully understand each individual feature's influence on the overall loglinear model performance. Acknowledgments This work has been funded by the European Union under the integrated project TC-STAR -Technology and Corpora for Speech to Speech Translation -(IST-2002-FP6-506738, http://www.tc-star.org). The authors also want to thank José A. R. Fonollosa and Marta Ruiz Costa-jussà for their participation in discussions related to this work.",8303276,bd500b912faec09be996f76ab49e33eb5a8b5f8f,22,https://aclanthology.org/W05-0823,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Banchs, Rafael E.  and
Crego, Josep M.  and
de Gispert, Adri{\`a}  and
Lambert, Patrik  and
Mari{\~n}o, Jos{\'e} B.",Statistical Machine Translation of {E}uparl Data by using Bilingual N-grams,133--136,,,,,,,inproceedings,banchs-etal-2005-statistical,,,
16,2009.mtsummit-posters.23,"Rule based machine translation methods require a set of sophisticated transfer rules for good accuracy. To manually build such a bilingual resource, one requires many man-years of work performed by linguistic specialists. This cost is too high, especially in case of less represented language pairs, such as Hungarian and Japanese. This paper proposes a simple and robust method to automatically build a large coverage transfer rule set for the Hungarian-Japanese language pair. Our method uses a small parsed bilingual corpus and a bilingual dictionary of the selected languages. We concentrate on accurately inducing the most frequent target language translation rules from all instances of a source language rule. We achieved good accuracy especially for low level rules, which are especially important in case of agglutinative languages.","Rule based machine translation methods require a set of sophisticated transfer rules for good accuracy. To manually build such a bilingual resource, one requires many man-years of work performed by linguistic specialists. This cost is too high, especially in case of less represented language pairs, such as Hungarian and Japanese. This paper proposes a simple and robust method to automatically build a large coverage transfer rule set for the Hungarian-Japanese language pair. Our method uses a small parsed bilingual corpus and a bilingual dictionary of the selected languages. We concentrate on accurately inducing the most frequent target language translation rules from all instances of a source language rule. We achieved good accuracy especially for low level rules, which are especially important in case of agglutinative languages. Introduction Creating a set of transfer rules for a rule-based or pattern-based system could take many man-years of work (Prószéky and Tihanyi, 2002) ; we attempt to simplify this process by automatically generating these rules in form of transfer rules, including word-level rule correspondences, such as inflection and conjugation rules. This is particularly crucial with agglutinative languages, such as Hungarian or Japanese. Both languages manifest a high degree of verb and adjective inflection, governed by grammatical rules for which bilingual transfer rule implementation can be very costly. Moreover, both languages present specific linguistic features that are again very costly to organize in a bilingual environment. For example, Hungarian has one of the most grammatical cases, estimated to be between 17 and 24 (László, 1977) (Table 1 ). In Japanese particles before the respective nouns are used instead of grammatical cases. We attempt to generate the transfer rules using a small or medium sized parallel corpus and a bilingual dictionary, concentrating mainly on wordlevel, inflectional correspondences. Because of the limited bilingual resources, our transfer rules will be incorporated into a rule based machine translation framework for assimilation purposes. Thus we target the most simple and general transfer rules that cover most of the language. In this stage we do not attempt to create rules for grammatical exceptions or idiomatic expressions. This paper is structured as follows: first we discuss the most significant related studies, after which we focus on the problems of current translation template generation methods, followed by a brief description of our method. Finally we evaluate our method and conclude with our findings. Related work There are numerous relatively successful examples of shallow translation template extraction methods for closely related languages (Altintas and Güvenir, 2003; Cicekli, 2005) . Initial in-depth structure alignment methods attempt to identify complex, hierarchical structures such as phrase structures (Kaji et al., 1992) or dependency structures (Watanabe et al., 2000) . Other methods include the Translation Template Learner (TTL) algorithm, which analyzes similarities and differences between translation pairs (Cicekli and Güvenir, 2003; Öz and Cicekli, 1998; Ong et al., 2007) . Most of these heuristic methods attempt to generate transfer rules from each sentence pair. With distant languages they notoriously fail, because after recognizing partial matches, these methods estimate that the remaining, unmatched fragments are also equivalent, producing many erroneous, useless and even contradictory results. As a possible solution to the drawbacks of the pure statistical machine translation (weak on reordering; lack of target language fluency), syntactic approaches were proposed that work with traditional statistic models: syntax-based statistical machine translation (Yamada and Knight, 2001) ; string-based (Galley et al., 2004; Galley et al., 2006) ; tree-based (Lin, 2004; Liu et al., 2006) forest-based (Mi et al., 2008) ; forest pruning based systems (Mi et al., 2008) . These methods perform better than the non-statistical ones, but require large bilingual corpora. One other major problem of statistical methods is their difficulty in applicability with agglutinative languages. For example, in Hungarian one noun can have more than 2000 possible forms (combi-nations of number, case, number or person of grammatical possessors or possessed, etc), thus simply collecting enough statistical data is an enormous task. Lemmatizers could facilitate this task, but because of the complexity of the inflection rules and the high number of exceptions from the rule, efficient Hungarian lemmatizers are not yet available. Proposed method In order to achieve high precision, our method analyzes all instances of a certain rule, attempting to extract the most frequent, and thus the most suitable transfer rules. During this process, it looks for the most general rule as possible, subcategorizing or exemplifying only when needed. Our method follows a bottom-to-top mechanism, looking to identify not only general translation templates, but also partial rules, or frequent subsequences of a certain pattern, mainly targeting inflections and conjugations. Resource details To generate the transfer rules, the proposed method uses an automatically generated bilingual dictionary (Varga and Yokoyama, 2007) and a parsed bilingual corpus. There is no known large digital bilingual corpus for Hungarian and Japanese, therefore we needed to improvise with a small, manually created corpus using bilingual language books for Japanese or Hungarian learners that has only about 5000 sentence pairs. Although the sentences are short, it might be suitable for transfer rule extraction, since its data is grammatically rich and well prepared due to its initial educational purpose. For Hungarian we used MetaMorph (Prószéky and Novák, 2005) and for Japanese Cabocha (Kudo and Matsumoto, 2000) parsers. Our method's bilingual corpus requires a specific format. To ensure robustness, we opted for label-bracketed phrase structure rules, since these retain a relatively detailed syntax of the language. For example, the format for the Japanese sentence 夜はオペラに行った [Yoru ha opera ni itta 'Last night I went to the opera'] is (S (PP (N 夜) (Part は)) (VP (NP (N オペラ)(Part に)) (V 行った)))). Both parsers needed minor modifications to accommodate the output. Moreover, the Japanese parser had to be adapted to the Yamada-grammar (Moriyama, 2000) , so that one word should represent one concept, similarly with Hungarian. To be able to generate low-level inflection rules, we extended both Hungarian and Japanese parses with additional, optional information for inflected or conjugated words. The additional information are: (1) information about the inflection or conjugation and (2) the stem of the inflected or conjugated word to be easily identified from the dictionary. For example, the additional information for the Japanese verb 行った [itta; 'went'], becomes (V<2p> 行った<行く>). The bracket after the part-of-speech (POS) information represents the grammatical category of the inflection (2: time; p: past, hence <2p>: past tense), while the bracket after the inflected word represents the stem of the word. Transfer rule generation Our method is composed of two steps: first we generate the language models for each language; next the transfer rules are generated. Both steps are entirely automated. Step 1: Language model generation In this step we are looking to build the language model of the two languages. We compute every sentence, saving all partial rules. We can distinguish four types of rules: 1. head rules: rules where the parent is the sentence itself. The number of terminal rules is equal with the number of words that the sentence contains (ex: V→sleep); 4. regular rules: every rule that is not head, lexical or terminal rule (ex: NP→Adj+NP). We count the frequency of each rule, saving also the sentences from which they were generated. Head, lexical and regular rules need to be solved (transfer rules need to be generated to each of them). We consider a rule to be solved, when there is a correspondence with it in the target language. The transfer rules for the terminal rules are the bilingual dictionaries, and thus they are already solved. Step 2: Transfer rule generation In this step we build the transfer rules between the two languages. Using a recursive algorithm (solve_rule) we build the transfer rule candidates followed by a noisy rule elimination process (clean_candidates). This is performed twice, once as Hungarian and once as Japanese set as source language. solve_rule() solve_rule() solve_rule() solve_rule() For each source head rule (S=s→children(s)), we retrieve all S[] instances (retrieve_instance) in which this rule appears (line 7). If there are S'[] children that are not solved (not_solved), we attempt to solve its children's rules (line 9). For example, in the case of S→PP+VP as a Japanese head rule, an instance where this rule appears is the PP(夜は)+VP(オペラに行った) [yoru ha + opera ni itta 'Last night + I went to the opera'] sentence. Since the PP→N(夜)+Part(は) and VP→PP( オ ペ ラ に )+V( 行 っ た ) are not solved yet, the algorithm attempts to solve these first (line 9). When and if these two rules will be solved, the parent rule (S→PP+VP) will be reattempted. If all children are solved, transfer rule candidates (generate_candidates) are generated (line 13) using all translations of the rule (re-trieve_translation) (line 11). identify_match identify_match identify_match identify_match() () () () If a source rule that is investigated has only solved children, we retrieve all instances of the rule, to- gether with their target language correspondences (TS[]). These target language correspondences are whole sentences, we need to identify which parts of these sentences correspond with the source rule. All rules are identified by their children's rules; therefore if the rule in question is a lexical rule, the identification is done using the bilingual dictionary. The stemmed expression is vital in this case. If no information about the stem is available, there is a risk that the word will not be retrieved from the dictionary. If the rule is a head rule, the identification is done using the already solved rules, while with regular rules both resources are needed. In case of lexical rules, we look up each lexical category's instance (stemmed word, if it is available) from the lexical rule and mark the eventual correspondences. After all such correspondences are marked, we investigate the lowest level phrasal categories in the target language, counting how many identified instances it has in its sub-tree. The node or nodes with the maximum value are selected together with the sub-tree(s) as a transfer rule candidate (Figure 1 ). For example, to identify the s 1 →s 2 +s 3 lexical rule from the t 1 →t 2 +t 3 subtree, the s 2 →w 4 and s 3 →w 5 terminal rules are looked up using the dictionary. The process can have multiple scenarios: the words correspond to the same sub-tree (scenario1), or to different subtree (scenario2). In the latter case, multiple candidates (t 2 →t 4 +t 5 and t 3 →t 6 +t 7 ) are saved. In case of head and regular rules the only difference is the number of children's children. While with lexical rules this was one (one instance for each PoS), for non-lexical rules this is generally at least two (Figure 2 ). If no correspondences are found, no transfer rule candidate is returned. instantiate instantiate instantiate instantiate() () () () In case of lexical rules there are cases when the translations are not registered in the dictionary. In these cases, assuming that the dictionaries are correct, these words have a grammatical, rather than a lexical function. In this case the corresponding lexical category is instantiated, being replaced by its instance. Instantiation is performed also when inflection or conjugation information are available.  For example, if our initial s 1 →s 2 (w 1 )+s 3 (w 2 ) rule's w 2 word did not have any correspondence, the rule becomes s 1 →s 2 (w 1 )+w 2 . For example, in case of the Japanese PP→N+Part, there is no regular rule for a noun plus a particle, therefore the method correctly makes the judgment that the particle needs to be instantiated and new rules are generated for each instance (Table 2 ). clean_ca clean_ca clean_ca clean_candidates ndidates ndidates ndidates() () () () If there are unmatched instances in the second language and their translation can not be found in the first language's rule, the transfer rule candidate is deleted. For example, none of the translations of japán (Japanese person; Japanese language) from example#4 could be found in the Japanese rule, thus the transfer rule was considered erroneous. On the other hand, definite articles (a, az) (English: the) also don't have translations in the Japanese rule, but it they have no translation in the dictionary either (there is no corresponding Japanese translation), therefore they were allowed. The remaining candidates are grouped by their common nodes and are saved with three values: total nr of candidates; total nr of transfer rule instances; nr of instances for the current rule. Since we do not use any thresholds within our method, these three numbers indicate the confidence level of the transfer rule. For example, for PP→N+は [ha] only one transfer rules could be generated: NP→DET+N<2s>. The corresponding values are (7, 2, 2). Evaluation For evaluation, we fragmented our corpus into 5 fragments. First we randomly separated 100 sentences that we used these as our evaluation data. Next we randomly separated 4 training corpora of 100, 500, 1000 and 2000 sentence pairs, to analyze the score differences across various sized corpora. Due to the small size of the available Hungarian-Japanese corpus, performing BLEU score was not adequate, since not enough statistical data was available. Instead, we performed automatic recall evaluation and a manual adequacy evaluation to evaluate our method. We used the rules whose number of instances for the current rule is at least 2. Recall evaluation We investigated to what percentage our method's output rules (R o ) manage to cover the training data's phrase structure rules (R T ). We performed a weighted recall evaluation, weighting each rule (r) with its frequency (frequency(r)) in the training corpus. Because of the instantiation feature many new rules are generated that are not part of the training data's phrase structure rules, during evaluation only we added these new rules to the training data. ( ( ) 100 ⋅ = ∑ ∑ ∈ ∈ T O R r R r r frequency r frequency recall (1) ) We analyzed the Japanese coverage, separately evaluating the head, regular and lexical rules. Lexical rules performed best, improving rapidly from 47.71% to 64.23% when the training data increased from 100 to 2000 sentence pairs (Table 3 ). Head rules performed worst, with only up to a third of them managing to move up the parse trees all the way to the root. Adequacy evaluation We automatically simulated a basic rule based machine translation system with 100 Japanese sentences whose rule head has a transfer rule. These sentences were randomly selected from the test data of our bilingual corpus. Our simple machine translation system exhaustively applied all suitable transfer rules, also performing lexical transfer, based on the bilingual dictionary. During this process, all intermediate data (lexical, regular and head rules) was separately saved. As a result, multiple Hungarian translations became available for a single Japanese sentence. Next, all Hungarian translations, together with the reference retrieved from the bilingual corpus, were manually checked by 3 independent, Hungarian native speakers. We used a 5 to 1 scoring criteria, where 5 is a perfect, 1 is a totally wrong output sentence. The interpretation of the intermediate scores was left to the judgment of each evaluator. We separately evaluated the head, regular and lexical rules. We performed the same evaluation on four training corpora: 100, 500, 1000 and 2000 samples. Assuming that a language model would correctly identify the best translation, we considered only the best scoring Hungarian translation for each Japanese source. Lexical items scored best, since understandably the errors on the lower level reflected within the regular and head rules as well. However, with the increase in size of the training data, the accuracy of the lexical rules increased faster than the other two types of rules. We could not observe any major difference in behaviour between the regular and head rules (Table 4 ). Discussions Our method showed its biggest weakness during recall evaluation. Many rules could not be identified in the transfer rules, especially the ones which direct over larger sub-trees. There are two major reasons for this: linguistic differences and resource issues. Regarding precision, the once recalled rules showed a surprising accuracy, especially lexical ones. Precision problems can be mainly attributed to resource issues. Linguistic differences Our first observation is that the biggest reasons for the low recall value are the linguistic differences between Hungarian and Japanese. Syntax is different, sentence construction is also different; therefore one sub-tree in a certain language does not necessarily match another sub-tree in the other parse. Other linguistic differences, such as expression of pronouns or the sentence topic manifest differently across languages, our method does not always generate the proper transfer rule in these cases. For example, with the 私[watashi]+は [ha] (me, myself) sub-tree rarely has any correspondence in Hungarian, since the agent (pronoun in this case) is expressed within the verb. Resource issues There are two types of resource issues. The first problem concerns the parsers and dictionary that we used. The parsers do not have a perfect accuracy, the noise produced by them reflected in the recall and accuracy results. The methodology of the parsers itself is different, with sub-trees not always matching a sub-tree in the other language, even when both parsers performed correctly. Our bilingual dictionary is noisy, because it was automatically generated using a pivot language (Varga and Yokoyama, 2007) . No manual cleaning was performed in order to raise its recall or accuracy; many translations could not be identified. The second problem concerns our corpus. Precision scores with smaller training data were low, because many erroneous transfer rules were generated besides the correct ones, but with the increase of the training data the frequency of correct rules also climbed rapidly. However, even with our biggest training data (2000 sentence pairs) the recall and precision values were not very high, but it is promising that from the second largest training data (1000 sentence pairs) the relative recall increase was between 3%-11%, the relative adequacy increase between 4%-13%. This significant increase shows that with an average sized corpus much better results can be achieved. Conclusions We presented a transfer rule generating method that uses a parsed bilingual corpus and a bilingual dictionary as resources. Although our biggest aim is low-cost in this research, during bilingual corpus acquisition we found ourselves in a contradictory situation: to generate low-cost transfer rules, we needed to manually create a small bilingual corpus. However, the cost of creating this corpus is insignificant when we think of the costs that a transfer rule system would require. As a compromise between having a small or medium sized corpus with noisy parses and the desire to achieve a good performance, we didn't handle grammatical exceptions or idiomatic expressions. As a result, with a small corpus we managed to achieve medium recall and good precision, with basic conjugation and inflectional rules being highly accurate. We showed that with the minimal increase in size of the bilingual corpus, overall adequacy, together with recall can quickly increase.",34218208,2e4ffd53b290960a18679c8d6f3db4e03bbacdc8,3,https://aclanthology.org/2009.mtsummit-posters.23,,"Ottawa, Canada",2009,August 26-30,Proceedings of Machine Translation Summit XII: Posters,"Varga, Istv{\'a}n  and
Yokoyama, Shoichi",Transfer rule generation for a {J}apanese-{H}ungarian machine translation system,,,,,,,,inproceedings,varga-yokoyama-2009-transfer,,,
17,L02-1316,,,3010546,8fbe98fa90a02026e090eb90eb0da8f4f79f1ad9,8,http://www.lrec-conf.org/proceedings/lrec2002/pdf/316.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Bartolini, Roberto  and
Lenci, Alessandro  and
Montemagni, Simonetta  and
Pirrelli, Vito",The Lexicon-Grammar Balance in Robust Parsing of {I}talian,,,,,,,,inproceedings,bartolini-etal-2002-lexicon,,,
18,R13-1048,"We present an incremental dependency parser which derives predictions about the upcoming structure in a parse-as-youtype mode. Drawing on the inherent strong anytime property of the underlying transformation-based approach, an existing system, jwcdg, has been modified to make it truly interruptible. A speed-up was achieved by means of parallel processing. In addition, MaltParser is used to bootstrap the search which increases accuracy under tight time constraints. With these changes, jwcdg can effectively utilize the full time span until the next word becomes available which results in an optimal quality-time trade-off.","We present an incremental dependency parser which derives predictions about the upcoming structure in a parse-as-youtype mode. Drawing on the inherent strong anytime property of the underlying transformation-based approach, an existing system, jwcdg, has been modified to make it truly interruptible. A speed-up was achieved by means of parallel processing. In addition, MaltParser is used to bootstrap the search which increases accuracy under tight time constraints. With these changes, jwcdg can effectively utilize the full time span until the next word becomes available which results in an optimal quality-time trade-off. Introduction Users prefer incremental dialogue systems to their non-incremental counterparts (Aist et al., 2007) . For a syntactic parser to contribute to an incremental dialogue system or any other incremental NLP application, it also needs to work incrementally. However, parsers usually operate on whole sentences only and few parsers exist that are capable of incremental parsing or are even optimized for it. This paper focuses on using a parser as part of an incremental pipeline that requires timely response to natural language input. In such a scenario, delay imposed by a parser's lookahead is more severe than delay caused by parsing speed since the parsing speed is capped by the user's input speed. Depending on the input method, the maximum typing speed varies between 0.75 seconds per word (qwerty keyboard) and 6 seconds per word (mobile 12-key multi-tap) (Arif and Stuerzlinger, 2009) and is usually lower if the sentence has to be phrased while typing. In such a scenario, the objective of the parser is to yield high quality results and produce them as soon as they are needed by a subsequent component. It is rarely known beforehand when the next word will be available for processing. Therefore, in an incremental pipeline a) computation should continue until the next word occurs if this might contribute to a better result, and b) a new word should be included immediately to avoid delays. A parser which works pull-based, i.e. processes one prefix until it is deemed finished and then pulls the next word, is insufficient under conditions, since it would require to determine the time used for processing before the processing can even start. Either the estimated processing time will be too short, violating a), or it will be too long, violating b). In contrast, push-based architectures can meet both requirements since the processing of the prefix can be interrupted when new input is available. Beuck et al. (2011) showed that Weighted Constraint Dependency Grammar-based parsers are capable of producing high-quality incremental results but neglected the processing time needed for each increment. In this paper, we will use jwcdg 1 , a reimplementation of the WCDG parser written in Java. jwcdg uses a transformation-based algorithm. It comes equipped with a strong anytime capability, causing the quality of the results to depend on the processing time jwcdg is allowed to consume. We will show that jwcdg can produce high quality results even if only granted fairly low amounts of processing time. Incremental Predictive Dependency Parsing Dependency parsing assigns every word to another word or NIL as its regent and the resulting edges are annotated with a label. If dependency analyses are used to describe the syntactic structure of sentence prefixes, different amounts of prediction can be provided. The interesting cases are those where either the regent or a dependent is not yet part of the sentence prefix. If the regent of a word w is not yet available, the parser can make a prediction about where and how w should be attached. One possibility is to simply state that the regent of w lies somewhere in the future without giving any additional information. This can be modelled by attaching w to a generic nonspec node (Daum, 2004) . Beuck et al. (2011) call this minimal prediction. However, it is usually possible to predict more: The existence of upcoming words can be anticipated and w can then be attached to one of these words. Of course, most of the time it will not be possible to predict exact words but abstract pseudo-words can be used instead that stand for a certain type such as nouns or verbs. Beuck et al. (2011) call these pseudo-words virtual nodes and the approach of using virtual nodes structural prediction (because the virtual nodes accommodate crucial aspects of the upcoming structure of the sentence). A virtual node can be included into an analysis to represent words that are expected to appear in later increments. As an example, ""Peter drives a red"" can be analyzed as Peter drives a red [nonspec] Su bj Det Ad j using minimal prediction or as Peter drives a red [VirtNoun] Su bj Det Ad j Obja using structural prediction. Minimal prediction leads to disconnected analyses while structural prediction allows for connected analyses which resemble the structure of whole sentences. Tn this case, the analysis includes the information that the regent of ""red"" is the object of ""drives"", which is missing in the analysis using minimal prediction. Challenges in Incremental Predictive Parsing The key difference between non-incremental and incremental parsing is the uncertainty about the continuation of the sentence. If a prediction about upcoming structure is being made, there is no guarantee that this prediction will be accurate. Using a beam of possible prefixes, as done in Input: ""Peter ..."" Make Initial Analysis Internal Analysis Peter--XY--> -1 Transform Partial Analysis Increment: ""... drives ..."" Extend Analysis Peter--XY--> 1 drives--S--> 0 Transform Internal Analysis Partial Analysis 2010 ), is a strategy to deal with this uncertainty. It guarantees that each new analysis is an extension of an old one. With this approach, the whole beam becomes incompatible with the observed sentence continuation if no fitting prediction is contained in the beam. Thus, such sentences can not be parsed. Minimal prediction, as another option, largely abstains from prediction. This allows for monotonous extensions even without a beam since the analysis of a prefix will not be incompatible with the continuation of the sentence. This approach is used by MaltParser (Nivre et al., 2007) . A transformation-based parser, finally, can also deal with non-monotonic extensions. In contrast to beam search, only a single analysis is generated for each prefix and there is no guarantee that the analysis of a prefix p n is a monotonic extension of p n−1 . Because the analysis of p n−1 is only used to initialize the transformation process, the search space is not restricted by the results that were obtained in former prefixes although they still guide the analysis. WCDG Parsing In the Weighted Constraint Dependency Grammar formalism, a grammar is used to judge the quality of analyses. The grammar consists of constraints that are evaluated on one or more edges of an analysis. If a constraint is violated, a penalty is computed. Constraints can incorporate more edges into their computation than the edges they are evaluated on (McCrae et al., 2008) . They can traverse the current analysis by using special predicates. This way, a constraint evaluated on one edge could, for example, check if that edge is adjacent to an edge with certain properties. If a constraint uses such predicates, it is considered context sensitive. In the WCDG formalism, the best analysis of a sentence is defined by ba = arg max a∈Analyses c∈Conflicts(a) penalty(c) where the conflicts are the parts of an analysis that stand in conflict with the grammar and penalty(c) is the penalty that the grammar assigns to the conflict c. The Frobbing Algorithm jwcdg tries to find an analysis by transforming a given one until it cannot be improved further. The algorithm employed for this purpose is called frobbing (Foth et al., 2000) . Frobbing consists of two phases: first, the problem is initialized. In this phase, all possible edges are constructed and the constraints defined for a single edge are evaluated on them. An initial analysis is constructed using the best-scored edge for every word, which is repeatedly transformed in the second phase. Frobbing is described as pseudocode in Algorithm 1. A set of conflicts (constraints that are violated on specific edges) is computed and the most severe of them is attacked by transforming the analysis (attackConflict, line 7). This either results in a (not necessarily better) analysis that does not have this conflict or in the insight that the conflict cannot be removed (line 12). The algorithm repeatedly tries to remove the most severe conflict. If in this process a new best analysis is found, it is marked as the new starting point. If the conflict can not be removed, the algorithm tracks back to the starting point. The procedure can be interrupted at any time and the best analysis that was found up to that point will be returned. In its incremental mode, jwcdg works as depicted in Figure 1 . The new word is added to the previous analysis using the best edge. The frobbing algorithm is then run until no better result can be found or the parser is interrupted. To allow for prediction, either a nonspec node (Daum, 2004) or a set of virtual nodes (Beuck et al., 2011) is added to the set of words. This way, all changes regarding incrementality are completely transparent to the frobbing algorithm, only the constraints in the grammar need to be aware of virtual nodes and nonspec. MaltParser is a shift-reduce based parser. It uses a classifier to determine locally optimal actions from a set of possible actions of a parsing algorithm. The classifier is trained using manually annotated sentences. MaltParser can use several different parsing algorithms. In this paper, the 2planar algorithm described in Gómez-Rodríguez and Nivre (2010) will be used, which is able to produce non-projective analyses. Related Work While MaltParser's processing is fast compared to jwcdg, a lookahead of one word already translates into a delay of one to three seconds, depending on the input speed of the user. Beuck et al. (2011) showed that, at least for German, Malt- Parser suffers from a fairly small decrease in accuracy if only features from the next two words instead of three are used. However, since the PoS tags are needed and a PoS tagger needs lookahead as well to achieve good accuracy, a lookahead of at least three words is needed for the whole taggerparser pipeline to achieve a high accuracy. The effect of this delay is illustrated in Figure 2 . In addition, MaltParser is not capable of producing structural predictions. Parallelizing jwcdg Among the two phases of frobbing, only the second one can be interrupted. Therefore, initialization needs to be faster than the shortest time limit we would like to impose. Initialization is mostly concerned with evaluating constraints on all edges that come from or point towards the new word. To make this judgement faster, the code has been changed so that it can be done in parallel, using worker threads instead of a sequential constraint evaluation. Table 1 shows the time needed to construct new edges and judge them while parsing a subset of the NEGRA corpus (Brants et al., 2003) . 2 Although the median time is already relatively good in the non-parallelized case, its maximum amounts to almost two seconds. Parallelization brings most benefits for the more complex initializations: the time needed for the 3rd quartile scales nearly linear up to eight cores. More than sixteen cores yield no further improvement. With the parallelized initialization, jwcdg can spend more time on transforming analyses. In addition, it is able to perform anytime parsing with a lower bound of about 200 ms per word on current hardware. The heart of the frobbing algorithm is the at-tackConflict method which, given an analysis a and a conflict c, systematically tries all changes of edges that are part of c. It then returns the best resulting analysis that does not violate the constraints constr (c). Since these transformations all work independently, they have been parallelized in the same manner as the initialization. The result can be seen in Table 2 . While the introduction of parallelized code causes a small overhead, using two cores already provides a noticeable benefit. The parallelized code can benefit from up to 32 cores, yet the overhead of managing more cores results in a sub-linear speedup. These optimizations have a noticeable impact on parsing performance under time pressure: With a time limit of two seconds per word, jwcdg base scores an unlabeled accuracy of 72.54% for the final analyses, while jwcdg parallel scores 76.29%. jwcdg base only reaches this accuracy with a time limit of four seconds. Unless otherwise noted, all evaluations have been carried out on sentences 18602 to 19601 of the NEGRA corpus. MaltParser as a Predictor for jwcdg When facing a tight time limit, jwcdg has only very little time to improve an analysis by transforming it. Therefore, with tighter time limits a good initial attachment becomes more important and a method which provides frobbing with a good initial analysis could help to achieve drastically better results. Foth and Menzel (2006) showed that WCDG can be augmented by trainable components to raise the accuracy of WCDG. The output of these predictors was converted into constraints to help WCDG finding a good analysis. One of the components was a shift-reduce parser modeled after Nivre (2003) , which was the first description of the MaltParser architecture. Although the shift-reduce parser was relatively simple compared to Malt-Parser and had a labeled accuracy of only 80.7 percent, it helped to raise the accuracy of WCDG from 87.5 to 89.8 percent. This approach has later been used by Khmylko et al. (2009) to integrate MST-Parser (McDonald et al., 2006 ) (which does not work incrementally) as an external data source for WCDG. We integrated MaltParser in a similar way. MaltParser consumes the input from left to right and, if using the 2planar algorithm, constructs edges as soon as possible: An edge can only be created between the word on top of the stack and the current input word. This means that every edge has to be constructed as soon as the second word number of threads used of the edge is the current input word. As soon as that word gets shifted onto the stack, the creation of the edge would no longer be possible. The parser works monotonically since edges are only added to but never removed from the set of edges. This means that all decisions by the parser are final; if word a is not attached to word b, we can be sure that a will never be attached to b in subsequent analyses. As a corollary, if MaltParser has an accuracy of X percent on whole sentences, the probability that a newly created edge is correct will also be X percent. As a delay is not acceptable for our application scenario, we will use MaltParser and the TnT tagger (Brants, 2000) without lookahead despite their inferior accuracy in that mode 3 . An Interface Between MaltParser and jwcdg A predictor (MaltPredictor) for jwcdg has been implemented that uses a newly written incremental interface for MaltParser so that the regents and labels predicted by MaltParser can be accessed by constraints as soon as they become available. MaltPredictor uses the PoS-tags that are provided 3 both were trained on sentences 1000 to 18000 of the negra corpus by the tagger predictor. Each time a new word w is pushed to jwcdg, MaltPredictor forwards w together with its PoS-tag onto MaltParsers input queue and runs MaltParser's algorithm until a shift operations occurs. With this shift operation, w is consumed from the input queue. If the sentence is marked as being finished, MaltParser is run until it has fully parsed the sentence. MaltPredictor then reads the state of MaltParser and stores for each word the regent it has been assigned to by Malt-Parser. If Maltparser did not assign a regent to a word, this fact is also stored. Since -as already mentioned -MaltParser works eagerly (i. e. constructs every edge as soon as possible), the regent of such a word must either lie in the future or be the root node. The three constraints that are used for accessing MaltParser's analyses are depicted as pseudocode in Figure 3 . If only these constraints and the tagger constraint (which selects the PoS-tag for every word) are used as a grammar, jwcdg will parse sentences exactly as MaltParser does. This way, jwcdg acts as an incremental interface to Malt-Parser. The first two constraints are only applicable if MaltPredictor has made a prediction for the word in question. The first constraint checks whether prediction_exists(word) -> regent_of(word) = predicted_regent(word) prediction_exists(word) -> label_of(word) = predicted_label(word) not prediction_exists(word) -> (regent(word) is virtual or regent(word) is nonspec or regent(word) is NIL or word is virtual) Figure 3 : Constraints for incorporating Malt-Parser's results into jwcdg the regent of a word is the one that has been selected by MaltParser. The second constraint checks that the predicted label matches the label of the edge in the analysis given that a label has been predicted. The third constraint is not as straightforward as the other two: Since we know that MaltParser creates edges as soon as possible and we know that MaltParser has not created an edge with this word as dependent, either the regent lies in the future (i. e. it should be a virtual node in jwcdg's analysis) or the regent is NIL (as MaltParser does not explicitly create edges to NIL while parsing). In the other possible case, the dependent of the current edge is a virtual node. In this case MaltParser cannot possibly predict an attachment. A parameter tuning on sentences 501 to 1000 of the NEGRA corpus has shown that a penalty of 0.9 works best for these constraints. When the input sentence is going to be extended, the new word is attached using the edge that violates the least unary, non context-sensitive constraints. The MaltParser constraints are unary and not context sensitive and therefore jwcdg will use the edge predicted by MaltParser if no other constraints prevent it from doing so. Comparison of MaltParser and jwcdg To compare MaltParser and jwcdg, the richer prediction of jwcdg has to be transformed into minimal prediction. In this mode, every attachment of a word to a virtual node is considered correct if the word is attached to an upcoming word in the gold standard. The two accuracies that are used for evaluation are initial attachment accuracy (how often is the newest word attached correctly?) and the final accuracy (how many attachments are correct in the parse trees for the whole sentences?). Figure 4 shows the accuracy for initial attachment and final accuracy as a function of a given time limit. Since MaltParser does not use an anytime algorithm, its results are the same for all time limits 4 . Note that the labeled initial attachment score is fairly low because MaltParser can not predict labels for edges that have nonspec as the regent. When ignoring labels, Maltparser's initial attachment accuracy is higher than its final accuracy since it does not change edges it has created (therefore the accuracy cannot rise) and words can be counted as correct initially but wrong in the final analysis: If the correct regent of a word lies somewhere in the future, the initial decision to not attach it is counted as correct. If it is later attached to a wrong word, it will be counted as wrong in the final accuracy. As can be seen, jwcdg outperforms MaltParser in most aspects when given enough time. The only exception is the unlabeled attachment score for the initial attachment. Here, however, one has to keep in mind that jwcdg produces more informative predictions with virtual nodes, which is not honored in this evaluation. Enhancing jwcdg with MaltParser jwcdg malt has been derived from jwcdg parallel by adding the MaltParser constraints discussed before to the grammar. The results (Figure 4 ) show that jwcdg malt has a considerably higher initial accuracy than jwcdg parallel , more than ten percentage points for a time limit of one second. The final accuracy is noticeably better as well. This shows that Malt-Parser's output helps jwcdg find a good initial analysis which can then be optimized by jwcdg. Evaluation on Additional Corpora The evaluations discussed so far have been carried out on the NEGRA corpus. NEGRA consists of newspaper texts and thus represents a very specific kind of text. However, most sentences from other text types (e.g. chat) are shorter and have a lower structural complexity. This section tries to measure the impact of these differences between different data sources. The results shown in Figure 5 confirm the assumption that jwcdg's accuracy increases when being evaluated only on short sentences. This difference could be due to the syntactic simplicity of short sentences and the shorter initialization time needed for short increments leaving more time for the transformation. In addition, jwcdg malt already approaches its best result with a time limit of four seconds. Evaluation on the creg-109 Corpus Since interactive Computer-Assisted Language Learning could be an interesting application domain for an incremental parser, an additional evaluation has been conducted on the creg-109 corpus, a set of 109 sentences of the corpus described in Meurers et al. (2010) . The creg-109 corpus ""consists of answers to German reading comprehension questions written by American college stu-dents learning German"" (Meurers et al., 2010) . Figure 6 shows the accuracy of the different parser versions on this corpus. The results have a different pattern than the ones for the NEGRA corpus: jwcdg parallel has nearly the same initial attachment accuracy as jwcdg malt and even slightly outperforms it in the final score. In addition to that, the result does not improve much with a time limit of more than two seconds. Both phenomena could be due to the lesser syntactic complexity of the sentences so that the MaltParser's analyses are not so beneficial for guiding jwcdg. Conclusion We have shown that it is possible to gain parse-asyou-type speed with good accuracy using a combination of different incremental approaches to dependency parsing. Our solution based on a parallelized version of jwcdg benefits from using up to 32 cores. In contrast to other approaches, which have to make algorithmic refinements, jwcdg can take advantage from the advances in processing speed due to its anytime property. MaltParser turned out to be a good predictor that helps jwcdg to produce good analyses earlier.",464028,e477ded457dc559b21d4f7f127e02aedcf58d76b,4,https://aclanthology.org/R13-1048,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"K{\""o}hn, Arne  and
Menzel, Wolfgang",Incremental and Predictive Dependency Parsing under Real-Time Conditions,373--381,,,,,,,inproceedings,kohn-menzel-2013-incremental,,,Dialogue and Interactive Systems
19,W05-0822,"This paper describes the participation of the Portage team at NRC Canada in the shared task 1 of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task.","This paper describes the participation of the Portage team at NRC Canada in the shared task 1 of ACL 2005 Workshop on Building and Using Parallel Texts. We discuss Portage, a statistical phrase-based machine translation system, and present experimental results on the four language pairs of the shared task. First, we focus on the French-English task using multiple resources and techniques. Then we describe our contribution on the Finnish-English, Spanish-English and German-English language pairs using the provided data for the shared task. Introduction The rapid growth of the Internet has led to a rapid growth in the need for information exchange among different languages. Machine Translation (MT) and related technologies have become essential to the information flow between speakers of different languages on the Internet. Statistical Machine Translation (SMT), a data-driven approach to producing translation systems, is becoming a practical solution to the longstanding goal of cheap natural language processing. In this paper, we describe Portage, a statistical phrase-based machine translation system, which we evaluated on all different language pairs that were provided for the shared task. As Portage is a very 1 http://www.statmt.org/wpt05/mt-shared-task/ new system, our main goal in participating in the workshop was to test it out on different language pairs, and to establish baseline performance for the purpose of comparison against other systems and against future improvements. To do this, we used a fairly standard configuration for phrase-based SMT, described in the next section. Of the language pairs in the shared task, French-English is particularly interesting to us in light of Canada's demographics and policy of official bilingualism. We therefore divided our participation into two parts: one stream for French-English and another for Finnish-, German-, and Spanish-English. For the French-English stream, we tested the use of additional data resources along with hand-coded rules for translating numbers and dates. For the other streams, we used only the provided resources in a purely statistical framework (although we also investigated several automatic methods of coping with Finnish morphology). The remainder of the paper is organized as follows. Section 2 describes the architecture of the Portage system, including its hand-coded rules for French-English. Experimental results for the four pairs of languages are reported in Section 3. Section 4 concludes and gives pointers to future work. Portage Portage operates in three main phases: preprocessing of raw data into tokens, with translation suggestions for some words or phrases generated by rules; decoding to produce one or more translation hypotheses; and error-driven rescoring to choose the best final hypothesis. (A fourth postprocessing phase was not needed for the shared task.) Preprocessing Preprocessing is a necessary first step in order to convert raw texts in both source and target languages into a format suitable for both model training and decoding (Foster et al., 2003) . For the supplied Europarl corpora, we relied on the existing segmentation and tokenization, except for French, which we manipulated slightly to bring into line with our existing conventions (e.g., converting l ' an into l' an). For the Hansard corpus used to supplement our French-English resources (described in section 3 below), we used our own alignment based on Moore's algorithm (Moore, 2002) , segmentation, and tokenization procedures. Languages with rich morphology are often problematic for statistical machine translation because the available data lacks instances of all possible forms of a word to efficiently train a translation system. In a language like German, new words can be formed by compounding (writing two or more words together without a space or a hyphen in between). Segmentation is a crucial step in preprocessing languages such as German and Finnish texts. In addition to these simple operations, we also developed a rule-based component to detect numbers and dates in the source text and identify their translation in the target text. This component was developed on the Hansard corpus, and applied to the French-English texts (i.e. Europarl and Hansard), on the development data in both languages, and on the test data. Decoding Decoding is the central phase in SMT, involving a search for the hypotheses t that have highest probabilities of being translations of the current source sentence s according to a model for P(t|s). Our model for P(t|s) is a log-linear combination of four main components: one or more trigram language models, one or more phrase translation models, a distortion model, and a word-length feature. The trigram language model is implemented in the SRILM toolkit (Stolcke, 2002) . The phrase-based translation model is similar to the one described in (Koehn, 2004) , and relies on symmetrized IBM model 2 word-alignments for phrase pair induction. The distortion model is also very similar to Koehn's, with the exception of a final cost to account for sentence endings. s To set weights on the components of the loglinear model, we implemented Och's algorithm (Och, 2003) . This essentially involves generating, in an iterative process, a set of nbest translation hypotheses that are representative of the entire search space for a given set of source sentences. Once this is accomplished, a variant of Powell's algorithm is used to find weights that optimize BLEU score (Papineni et al, 2002) over these hypotheses, compared to reference translations. Unfortunately, our implementation of this algorithm converged only very slowly to a satisfactory final nbest list, so we used two different ad hoc strategies for setting weights: choosing the best values encountered during , with the exception of a ch as the ability to decode either w ards. translarent language pairs of the sha d t hared t the iterations of Och's algorithm (French-English), and a grid search (all other languages). To perform the actual translation, we used our decoder, Canoe, which implements a dynamicprogramming beam search algorithm based on that of Pharaoh (Koehn, 2004) . Canoe is input-output compatible with Pharaoh few extensions su back ards or forw Rescoring To improve raw output from Canoe, we used a rescoring strategy: have Canoe generate a list of nbest translations rather than just one, then reorder the list using a model trained with Och's method to optimize BLEU score. This is identical to the final pass of the algorithm described in the previous section, except for the use of a more powerful loglinear model than would have been feasible to use inside the decoder. In addition to the four basic features of the initial model, our rescoring model included IBM2 model probabilities in both directions (i.e., P(s|t) and P(t|s)); and an IBM1-based feature designed to detect whether any words in one language seemed to be left without satisfactory tions in the other language. This missing-word feature was also applied in both directions. Experiments on the Shared Task We conducted experiments and evaluations on Portage using the diffe re ask. In addition to the provided data, a set of 6,056,014 sentences extracted from Hansard corpus, the official record of Canada's parliamentary debates, was used in both French and English languages. This c guage and translation models for use in decoding and rescoring. The development test data was split into two parts: The first part that includes 1,000 sentences in each language with reference translations into English served in the optimization of weights for both the decoding and rescoring models. In this study, number of n-best lists was set to 1,000. The second part, which includes 1,000 sentences in each language with referenc used in the evaluation of the performance of the translation models. Experiments on the French-English Task Our goal for this language pair was to conduct experiments on hniques: 1 indicates the method, the second column gives results for decoding with Canoe only, and the third column for decoding and rescoring with Canoe. For comparison between the four methods, there was an improvement in terms of BLEU scores when using two language models and two translation models generated from Europarl and Hansard corpora; however, parsing numbers and dates had a negative impact on the ranslation els. of increased trade within North merica but also functions as a good counterpoint for French-English. ble 1. BLEU scores for the French-English test sentences A noteworthy feature of these results is that the improvement given by the out-of-domain Hansard corpus was very slight. Although we suspect that somewhat better performance could have been achieved by better weight optimization, this result clearly underscores the importance of matching training and test domains. A related point is that our number and date translation rules actually caused a performance drop due to the fact that they were optimized for typographical conventions prevalent in Hansard, which are quite different from those used in Europarl. Our best result ranked third in the shared WPT05 French-English task , with a difference of 0.74 in terms of BLEU score from the first rank participant, and a difference of 0.67 in terms o BLEU score from the second ranked participant. Experiments on other Pairs of Languages The WPT05 workshop provides a good opportunity to achieve our benchmarking goals with corpora that provide challenging difficulties. German and Finnish are languages that make considerable use of compounding. Finnish, in addition, has a particularly complex morphology that is organized on principles that are quite different from any in English. This results in much longer word forms each of which occurs very infrequently. Our original intent was to propose a number of possible statistical approaches to analyzing and splitting these word forms and improving our results. Since none of these yielded results as good as the baseline, we will continue this work until we understand what is really needed. We also care very much about translating between French and English in Canada and plan to spend a lot of extra effort on difficulties that occur in this case. Translation between Spanish and English is also becoming more mportant as a result ble 2 BLEU scores for the Finnish-English, German-English and Spanish-English test sentences To establish our baseline, the only preprocessing we did was lowercasing (using the provided tokenization). Canoe was run without any special settings, although weights for distortion, word penalty, language model, and translation model were optimized using a grid search, as described above. Rescoring was also done, and usually resulted in at least an extra BLEU point. Our final results are shown in Table 2 . Ranks at the shared WPT05 Finnish-, German-, and Spanish-English tasks were assigned as second, third and fourth, with differences of 1.06, 1.87 ter s of BLEU sc Conclusion We have reported on our participation in the shared task of the ACL 2005 Workshop on Building and Using Parallel Texts, conducting evaluations of Portage, our statistical machine translation system, on all four language pairs. Our best BLEU scores for the French-, Finnish-, German-, and Spanish-English at this stage were 29.5, 20.95, 23.21 and 29.08 , respectively. In total, eleven teams took part at the shared task and most of them submitted results for all pairs of languages. Our results distinguished the NRC team at the third, second, third and fourth ranks with slight differences with the first ranked participants. A major goal of this work was to evaluate Portage at its first stage of implementation on different pairs of languages. This evaluation has served to identify some problems with our system in the areas of weight optimization and number and date rules. It has also indicated the limits of using out-ofdomain corpora, and the difficulty of morphologically complex languages like Finnish. Current and planned future work includes the exploitation of comparable corpora for statistica machine transl knowledge, and better features for nbest rescoring.",1289925,dd91cae2486eb393f524c3cc32006991568e8ba5,47,https://aclanthology.org/W05-0822,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Sadat, Fatiha  and
Johnson, Howard  and
Agbago, Akakpo  and
Foster, George  and
Kuhn, Roland  and
Martin, Joel  and
Tikuisis, Aaron",{PORTAGE}: A Phrase-Based Machine Translation System,129--132,,,,,,,inproceedings,sadat-etal-2005-portage,,,
20,L02-1317,,,29668420,92d0a81bac409f940d416a440ee240d53ba95194,0,http://www.lrec-conf.org/proceedings/lrec2002/pdf/317.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Jung, Daniel",Humans as Corpus - Language Learning Strategies in Virtually Mediated Authentic Environments,,,,,,,,inproceedings,jung-2002-humans,,,
21,R13-1049,"The Unit Graphs (UGs) framework is a graph-based knowledge representation (KR) formalism that is designed to allow for the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory Combinatorial Dictionary of the Meaning-Text Theory (MTT). This paper introduces the UGs framework, and overviews current published outcomes. It first introduces rationale of this new formalism: neither semantic web formalisms nor Conceptual Graphs can represent linguistic predicates. It then overviews the foundational concepts of this framework: the UGs are defined over a UG-support that contains: i) a hierarchy of unit types which is strongly driven by the actantial structure of unit types, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. On these foundational concepts and on the definition of UGs, this paper finally overviews current outcomes of the UGs framework: the definition of a deep-semantic representation level for the MTT, representation of lexicographic definitions of lexical units in the form of semantic graphs, and two formal semantics: one based on UGs closure and homomorphism, and one based on model semantics.","The Unit Graphs (UGs) framework is a graph-based knowledge representation (KR) formalism that is designed to allow for the representation, manipulation, query, and reasoning over linguistic knowledge of the Explanatory Combinatorial Dictionary of the Meaning-Text Theory (MTT). This paper introduces the UGs framework, and overviews current published outcomes. It first introduces rationale of this new formalism: neither semantic web formalisms nor Conceptual Graphs can represent linguistic predicates. It then overviews the foundational concepts of this framework: the UGs are defined over a UG-support that contains: i) a hierarchy of unit types which is strongly driven by the actantial structure of unit types, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. On these foundational concepts and on the definition of UGs, this paper finally overviews current outcomes of the UGs framework: the definition of a deep-semantic representation level for the MTT, representation of lexicographic definitions of lexical units in the form of semantic graphs, and two formal semantics: one based on UGs closure and homomorphism, and one based on model semantics. Introduction The Meaning-Text Theory (MTT) is a theoretical dependency linguistics framework for the construction of models of natural language. As such, its goal is to write systems of explicit rules that express the correspondence between meanings and texts (or sounds) in various languages (Kahane, 2003) . From semantic representations to surface phonologic representations, seven different levels of linguistic representation are supposed for each set of synonymous utterances. Thus, two times six modules containing transformation rules are used to transcribe representations of a level into representations of an adjacent level. The main constituent of the MTT is the dictionary model where lexical units are described, which is called the Explanatory Combinatorial Dictionary (ECD) (Mel'čuk, 2006) . As for any community of interest, linguists and lexicographers of the MTT framework produce knowledge. Knowledge Representation (KR) is an area of artificial intelligence that deals with recurrent needs that emerge with such knowledge production. The aim of this paper is to introduce the Unit Graphs KR formalism that is designed to allow for the representation, manipulation, query, and reasoning over dependency structures, rules and lexicographic knowledge of the ECD. The rest of this paper is organized as follows. We will first introduce rationale of this new KR formalism ( §2), then the fundamental concepts of the UGs framework ( §3), implications for the MTT, lexicographic definitions and application to a specific MTT lexicographic edition project ( §4), and finally two approaches to assign UGs with logical semantics, so as to enable reasoning in the UGs framework ( §5). Rationale: Representation of Valency-based Predicates Most past or current projects that aimed at implementing the ECD did so in a lexicographic perspective. One important example is the RE-LIEF project (Lux-Pogodalla and Polguère, 2011) , which aims at representing a lexical system graph named RLF (Polguère, 2009) , where lexical units are interlinked by paradigmatic and syntagmatic links of lexical functions (Mel'čuk, 1996) . In the RELIEF project, the description of Lexical Functions is based on a formalization proposed by Kahane and Polguère (2001) . Moreover, lexicographic definitions start to be partially formalized in the RELIEF project using the markup type that has been developed in the Definiens project (Barque and Polguère, 2008; Barque et al., 2010) . One exception is the proprietary linguistic processor ETAP-3 that implements a variety of ECD for Natural Language Processing (Apresian et al., 2003; Boguslavsky et al., 2004) . Linguistic knowledge are asserted, and transformation rules are directly formalized in first order logic. Adding to these formalization works, our goal is to propose a formalization from a knowledge engineering perspective, compatible with standard KR formalisms. The term formalization here means not only make non-ambiguous, but also make operational, i.e., such that it supports logical operations (e.g., knowledge manipulation, query, reasoning). We thus adopt a knowledge engineering approach applied to the domain of the MTT. At first sight, two existing KR formalisms seem interesting for this job: semantic web formalisms (e.g., RDF 1 , RDFS 2 , OWL 3 , SPARQL 4 ), and Conceptual Graphs (CGs) (Sowa, 1984; Chein and Mugnier, 2008) . Both of them are based on directed labelled graph structures, and some research has been done towards using them to represent dependency structures and knowledge of the ECD (OWL in (Lefranc ¸ois and Gandon, 2011; Boguslavsky, 2011) , CGs at the conceptual level in (Bohnet and Wanner, 2010) ). Yet Lefranc ¸ois (2013) showed that neither of these KR formalisms can represent valency-based predicates, therefore lexicographic definitions. One crucial issue is the following: in RDFS, OWL and the CGs, there is a strong distinction between concept types and relations. Yet, a linguistic predicate may be considered both as a concept type as it is instantiated in dependency structures, and as a relation as its instances may link other instances. The simple semantic representation illustrated on figure 1 thus cannot be represented with these formalisms unless we use reification of n-ary rela-tions. But then these formalisms lack logical semantics to reason with such relations. ( Peter ) ( try ) ( push ) ( cat ) 1 2 1 2 Figure 1 : Semantic representation of sentence Peter tries to push the cat. As the CGs formalism is the closest to the semantic networks, the following choice has been made to overcome these issues: Modify the CGs formalism basis, and define transformations to syntaxes of Semantic Web formalisms for sharing and querying knowledge. As we are to represent linguistic units of different nature (e.g., semantic units, lexical units, grammatical units, words), term unit has been chosen to be used in a generic manner, and the result of this adaptation is thus the Unit Graphs (UGs) framework. Fundamental Concepts of the UGs Framework First, for a specific Lexical Unit L, Mel'čuk (2004, p.5 ) distinguishes considering L in language (i.e., in the lexicon), or in speech (i.e., in an utterance). KR formalisms and the UGs formalism also do this distinction using types. In this paper and in the UGs formalism, there is thus a clear distinction between units (e.g., semantic unit, lexical unit), which will be represented in the UGs, and their types (e.g., semantic unit type, lexical unit type), which are roughly classes of units that share specific features. It is those types that will specify through their so-called actancial structure (Mel'čuk, 2004) how their instances (i.e., units) are to be linked to other units in a UG. Hierarchy of Unit Types The core of the UGs framework is a structure called hierarchy of unit types and noted T , where unit types and their actantial structure are described. This structure is thoroughly described in (Lefranc ¸ois, 2013; Lefranc ¸ois and Gandon, 2013b) and studied in (Lefranc ¸ois and Gandon, 2013d). Whether they are semantic, lexical or grammatical, unit types are assigned a set of Actant Slots (ASlots), and every ASlot has a so-called Actant Symbol (ASymbol) which is chosen in a set denoted S T . S T contains numbers for the semantic unit types, and other ""classical"" symbols for the other levels under consideration (e.g, roman numerals I to VI for the Deep Syntactic actants). The set of ASlots of a unit type t is represented by the set α α α(t) of ASymbols these ASlots have. Moreover, • some ASlots are obligatory, they form the set α α α 1 (t) of Obligatory Actant Slots (OblASlots); • other are prohibited, they form the set α α α 0 (t) of Prohibited Actant Slots (ProASlots); • the ASlots that are neither obligatory nor prohibited are said to be optional, they form the set α α α ? (t) of Optional Actant Slots (Op-tASlots). Finally, every unit type t has a signature function ς ς ς t that assigns to every ASlot of t a unit type, which characterises units that fill such a slot. The set of unit types is then pre-ordered 5 by a specialization relation , and for mathematical reasons as one goes down the hierarchy of unit types the actantial structure of unit types may only become more and more specific: (i) some ASlot may appear, be optional a moment, and at some points become obligatory or prohibited; (ii) the signatures may only become more specific. Hierarchy of Circumstantial Symbols UGs include actantial relations, which are considered of type predicate-argument and are described in the hierarchy of unit types. Now UGs also include circumstantial relations which are considered of type instance-instance. Example of such relations are the deep syntactic representation relations ATTR, COORD, APPEND of the MTT, but we may also use such relations to represent the link between a lexical unit and its associated surface semantic unit for instance. Circumstantial relations are labelled by symbols chosen in a set of so-called Circumstantial Symbols (CSymbols), denoted S C , and their categories and usage are described in a hierarchy denoted C, that has been formally defined in (Lefranc ¸ois and Gandon, 2013a). Unit Graphs UGs are defined over a so-called support, S def = (T , C, M) where T is a hierarchy of unit types, C 5 A pre-order is a reflexive and transitive binary relation. is a hierarchy of CSymbols, and M is a set of unit identifiers. A UG G defined over a support S is a tuple denoted G def = (U, l, A, C, Eq), where U is the set of unit nodes, l is a labelling mapping over U that associate every unit node with a unit type and one or more unit identifiers, A and C are respectively actantial and circumstantial triples, and Eq is a set of asserted unit node equivalences. Unit nodes are illustrated by rectangles with their label written inside, actantial triples are illustrated by double arrows, circumstantial triples are illustrated by simple arrows, and asserted unit node equivalences are illustrated by dashed arrows. For instance, figure 1 is a semantic representation of sentence Peter tries to push the cat. in which units are typed by singletons and ASymbols are numbers, in accordance with the MTT. Figure 2 is a simplified deep syntactic representation of Peter is gently pushing the cat. In this figure unit nodes u 2 and u 4 are typed by singletons, and only unit node u 2 is not generic and has a marker: {P eter}. P is composed of (u 1 , I, u 2 ) and (u 1 , II, u 3 ), where I and II are ASymbols. C is composed of (u 1 , ATTR, u 4 ) where ATTR is a CSymbol. In the relation Eq there is (u 1 , u 1 ), (u 2 , u 2 ), and (u 3 , u 3 ). {PUSH,present, progressive} UGs so defined are the core dependency structures of the UGs mathematical framework. MAN:P eter {CAT,def } GENTLY u 1 u 2 u 3 u 4 I II ATTR Unit Graphs and the Meaning-Text Theory 4.1 A Deep-Semantic Representation Level As the unit types hierarchy T is driven by the actantial structure of unit types, and as semantic ASymbols are numbers, the pre-order over unit types at the semantic level represents a specialization of the actantial structure, and not of meanings. For instance, the french lexical unit INSTRUMENT (en: instrument) has a Semantic ASlot 1 that corresponds to the activity for which the instrument is designed. Now PEIGNE (en: comb) has a stricter meaning than INSTRUMENT, and also two Semantic ASlots: 1 correspond to the person that uses the comb, and 2 is a split variable 6 that corresponds either to the hair or to the person that is to be combed. Then semantic unit type ( peigne ) cannot be more specific than ( instrument ) in the hierarchy of unit types because the signature of its ASlot 1 is not more specific than the signature of the ASlot 1 of ( instrument ) , i.e., ς ς ς ( peigne ) (1) = ( person ) ( activity ) = ς ς ς ( instrument ) (1). In fact, the meaning of ASlot 1 is not the same for ( instrument ) and ( peigne ) . Lefranc ¸ois and Gandon (2013b) therefore introduced a deeper level of representation to describe meanings: the deep semantic level, and defined the deep and surface semantic unit types and their actantial structure. The Deep Semantic Unit Type (DSemUT) associated with a Lexical Unit Type (LexUT) L is denoted / L \ . So that the ASlots of deep semantic unit types convey meanings, the set of ASsymbols that is used to symbolize ASlots at this level is a set of lexicalized semantic roles (e.g., agent, combedhair, combedperson). For instance the DSemUT / instrument \ associated with the LexUT INSTRUMENT may have an ASlot arbitrarily symbolized activity, which would be inherited by the DSemUT / peigne \ . Then / peigne \ also introduces three new ASlots: one arbitrarily symbolized possessor that corresponds to the ASlot 1 of ( peigne ) , and two arbitrarily symbolized combedhair, and combedperson that correspond to the ASlot 2 of ( peigne ) . Actually, one may need to introduce a new ASymbol every time a Semantic ASlot that conveys a new meaning is introduced. The set of semantic roles thus cannot be bound to a small set of universal semantic roles. Lexicographic Definitions It is at the deep semantic representation level that one may represent the actual meaning of a LexUT L. The lexicographic definition of L corresponds to the definition of its associated DSe-mUT / L \ , which is roughly an equivalence between two deep semantic UGs. Unit type definitions have been formally defined in (Lefranc ¸ois and Gandon, 2013a), and the definition of / L \ is a triple D / L \ def = (D − / L \ , D + / L \ , κ) , where (roughly): • D − / L \ represents only a central unit node typed with / L \ , and some other unit nodes that fill some of the ASlots of / L \ ; • D + / L \ is a UG called the expansion of / L \ , • there is no circumstantial triple in these two UGs because circumstantials must not be part of the lexicographic definition of a LexUT. • κ is a mapping from the unit nodes of D − / L \ to some unit nodes of D + / L \ . Figure 3 is an example of lexicographic definition of PEIGNE: an instrument that a person X uses to untangle the hair Y 1 of a person Y 2 . Intuitively, a definition corresponds to two reciprocal rules. If there is the defined PUT in a UG then one may infer its definition, and vice versa. A set of unit type definitions D may thus be added to the unit types hierarchy. / peigne \ / person \ / hair \ / person \ possessor combedperson combedhair / instrument \ / person \ / untangle \ / person \ / hair \ Lefranc ¸ois et al. ( 2013 ) illustrated how the UGs framework may be used to edit lexicographic definitions in the RELIEF lexicographic edition project (Lux-Pogodalla and Polguère, 2011) . Lexical Units are assigned a semantic label that may be considered as a deep semantic unit type and to which one may assign an actantial structure. A lexicographer may then manipulate nodes in a graphical user interface so as to little by little construct a deep semantic UG that represents the decomposition of the DSemUT associated with the defined LexUT. A prototype web application has been developed, and a demonstration is available online: http://wimmics.inria.fr/doc/ video/UnitGraphs/editor1.html. We currently lead an ergonomic study in partnership with actors of the RELIEF project in order to enhance the workflow of this prototype. Reasoning in the Unit Graphs Framework The prime decision problem of the UGs framework is the following: Considering two UGs G and H defined over the same support S, does the knowledge of G entails the knowledge of H ? Reasoning with UGs-Homomorphisms Lefranc ¸ois and Gandon (2013a) proposed to use the notion of UGs homomorphism to define this entailment problem. There is a homomorphism from a UG H to a UG G if and only if there is a homomorphism from the underlying oriented labelled graphs of H to that of G. Now one need to define the notion of knowledge of a UG. In fact, the UGs framework makes the open-world assumption, which means that a UG along with the support on which it is defined represents explicit knowledge, and that additional knowledge may be inferred. Consider the UG G = (U, l, A, C, Eq) defined over the support S illustrated in figure 4a . Some knowledge in G is implicit: 1. two unit nodes u 1 and u 2 share a common unit marker M ary, so one may infer that they represent the same unit. (u 1 , u 2 ) may be added to Eq. 2. every unit type is a subtype of the prime universal unit type , so one could add to all the types of unit nodes in G. 3. there are two unit nodes v 1 and v 2 that fill the same ASlot activity of the unit node typed / instrument \ . So one may infer that v 1 and v 2 represent the same unit. Said otherwise, (v 1 , v 2 ) may be added to Eq. 4. one may recognize the expansion of / peigne \ as defined in figure 3 , so this type may be made explicit in the unit node typed / instrument \ . Each of the rules behind these cases explicit knowledge in G. More generally, Lefranc ¸ois and Gandon (2013a) listed a set of rules which defines the axiomatization of the UGs semantics. The process of applying this set of rules on a UG G until none of them has any effect is called closing G. Figure 4b illustrates the closure of G, where all of the inferable knowledge has been made explicit. The notion of entailment may hence be defined as follows: G entails H, noted G h H, if and only if there is a homomorphism from H to the closure of G. Lefranc ¸ois and Gandon (2013a) illustrated problematic cases where the closure is infinite for finite UGs. If that occurs it makes the closure undecidable, along with the entailment problem. We are currently working of the definition of restrictions of the unit types hierarchy and the set of definitions in order to ensure that any UG has a finite closure. Model Semantics for the UGs framework Another approach to defining the entailment problem has been presented in (Lefranc ¸ois and Gandon, 2013c), using model semantics based on relational algebra. The model of a support S = (T , C, M) is a couple M = (D, δ), where D is a set called the domain of M , and δ is denoted the interpretation function. In order to deal with the problem of prohibited and optional ASlots, D contains a special element denoted • that represents nothing, plus at least one other element, and must be such that: • M is a model of T ; • M is a model of C; • for all unit identifier m ∈ M, the interpretation of m is an object of the domain D except for the special nothing element; Lefranc ¸ois and Gandon (2013c) listed the different equations that the interpretation function must satisfy so that a model is a model of a unit types hierarchy and of a CSymbols hierarchy. A model of a UG G is a model of the support on which it is defined, augmented with an assignment function β, which is a mapping from the set of unit nodes of G to the domain D. Such a model needs to satisfy a list of equations so that it may be said to satisfy the unit graph G. Then the notion of entailment is defined as classically done with model semantics: Let H and G be two UGs defined over a support S. G entails H, or H is a semantic consequence of G, noted G m H, if and only if for any model (D, δ) of S and for any assignment β G such that (D, δ, β G ) satisfies G, then there exists an assignment β H of the unit nodes in H such that (D, δ, β H ) satisfies H. There are multiple directions of research for the reasoning problem. • the definition of the model semantics of the UGs shall be completed so as to take lexicographic definitions into account. • one need to define algorithms to construct a model that satisfy a UG, and to check the entailment of a UG by another. • such algorithms may lead to an infinite domain. A condition over the unit types hierarchy and the lexicographic definitions must be found so as to ensure that the model is decidable for a finite UG. • are the two entailment relations h and m equivalent ? / instrument \ / person \ :Mary / do \ / untangle \ / person \ :Mary / hair \ activity activity agent object partof u 1 u 2 v 2 v 1 (a) Incomplete deep semantic representation G { / peigne \ , / instrument \ , } { / person \ , }:Mary { / untangle \ , / do \ , } { / untangle \ , / do \ , } { / person \ , }:Mary { / hair \ , } activity Conclusion We thus introduced rationale of the new Unit Graphs Knowledge Representation formalism that is designed to formalize, in a knowledge engineering perspective, the dependency structures, the valency-based predicates, and lexicographic definitions in the ECD. The strong coherence in the unit types hierarchy justifies the introduction of a deep semantic representation level that is deeper than the MTT semantic level, and in which one may represent the lexicographic definitions. Finally, two different logical semantics have been provided for UGs and the prime entailment decision problem has been defined in two ways. More research is needed to determine if these two decision problems are equivalent, and what their complexity is. There are other longer-term directions of research for the Unit Graphs framework: We are working on a syntax based on semantic web standards for the different objects of the framework. Like WordNet today, the linguistic knowledge written with that syntax could be shared and queried on the web of linked data 7 . This would support their use as a highly structured lexical resource by consumers of the linked data cloud. Rules have already been defined in the UGs framework. Let G DSem be a deep semantic UG, we need algorithms to select and apply correspondence rules to transcribe G DSem to a surface semantic UG G SSem for instance. We are working on defining generic rules to formally represent semantic derivations. This is a first step towards representing Lexical Functions that play a very important role in the MTT. Finally, the design of the Unit Graphs framework is a first step towards Natural Language Processing applications. Future work include (semiautomatically) populating this model with linguistic data, and formulating classical NLP tasks in terms of UGs, such as machine translation, question answering, text summarization, and so on.",14330089,6a32b0e9b257dca89a95b8e18444fdbd329da232,4,https://aclanthology.org/R13-1049,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Lefran{\c{c}}ois, Maxime  and
Gandon, Fabien","Rationale, Concepts, and Current Outcome of the Unit Graphs Framework",382--388,,,,,,,inproceedings,lefrancois-gandon-2013-rationale,,,"Semantics: Sentence-level Semantics, Textual Inference and Other areas"
22,W09-1312,"In this paper we present an extractive system that automatically generates gene summaries from the biomedical literature. The proposed text summarization system selects and ranks sentences from multiple MEDLINE abstracts by exploiting gene-specific information and similarity relationships between sentences. We evaluate our system on a large dataset of 7,294 human genes and 187,628 MEDLINE abstracts using Recall-Oriented Understudy for Gisting Evaluation (ROUGE), a widely used automatic evaluation metric in the text summarization community. Two baseline methods are used for comparison. Experimental results show that our system significantly outperforms the other two methods with regard to all ROUGE metrics. A demo website of our system is freely accessible at http://60.195.250.72/onbires/summary.jsp.","In this paper we present an extractive system that automatically generates gene summaries from the biomedical literature. The proposed text summarization system selects and ranks sentences from multiple MEDLINE abstracts by exploiting gene-specific information and similarity relationships between sentences. We evaluate our system on a large dataset of 7,294 human genes and 187,628 MEDLINE abstracts using Recall-Oriented Understudy for Gisting Evaluation (ROUGE), a widely used automatic evaluation metric in the text summarization community. Two baseline methods are used for comparison. Experimental results show that our system significantly outperforms the other two methods with regard to all ROUGE metrics. A demo website of our system is freely accessible at http://60.195.250.72/onbires/summary.jsp. Introduction Entrez Gene is a database for gene-centric information maintained at the National Center for Biotechnology Information (NCBI). It includes genes from completely sequenced genomes (e.g. Homo sapiens). An important part of a gene record is the summary field (shown in Table 1 ), which is a small piece of text that provides a quick synopsis of what is known about the gene, the function of its encoded protein or RNA products, disease associations, genetic interactions, etc. The summary field, when available, can help biologists to understand the target gene quickly by compressing a huge amount of knowledge from many papers to a small piece of text. At present, gene summaries are generated manually by the National Library of Medicine (NLM) curators, a time-and labor-intensive process. A previous study has concluded that manual curation is not sufficient for annotation of genomic databases (Baumgartner et al., 2007) . Indeed, of the 5 million genes currently in Entrez Gene, only about 20,000 genes have a corresponding summary. Even in humans, arguably the most important species, the coverage is modest: only 26% of human genes are curated in this regard. The goal of this work is to develop and evaluate computational techniques towards automatic generation of gene summaries. To this end, we developed a text summarization system that takes as input MEDLINE documents related to a given target gene and outputs a small set of genic information rich sentences. Specifically, it first preprocesses and filters sentences that do The protein encoded by this gene is a receptor for interleukin 20 (IL20), a cytokine that may be involved in epidermal function. The receptor of IL20 is a heterodimeric receptor complex consisting of this protein and interleukin 20 receptor beta (IL20B). This gene and IL20B are highly expressed in skin. The expression of both genes is found to be upregulated in Psoriasis. Table1 . Two examples of human-written gene summaries not include enough informative words for gene summaries. Next, the remaining sentences are ranked by the sum of two individual scores: a) an authority score from a lexical PageRank algorithm (Erkan and Radev, 2004 ) and b) a similarity score between the sentence and the Gene Ontology (GO) terms with which the gene is annotated (To date, over 190,000 genes have two or more associated GO terms). Finally, redundant sentences are removed and top ranked sentences are nominated for the target gene. In order to evaluate our system, we assembled a gold standard dataset consisting of handwritten summaries for 7,294 human genes and conducted an intrinsic evaluation by measuring the amount of overlap between the machine-selected sentences and human-written summaries. Our metric for the evaluation was ROUGE 1 , a widely used intrinsic summarization evaluation metric. Related Work Summarization systems aim to extract salient text fragments, especially sentences, from the original documents to form a summary. A number of methods for sentence scoring and ranking have been developed. Approaches based on sentence position (Edmundson, 1969) , cue phrase (McKeown and Radev, 1995) , word frequency (Teufel and Moens, 1997) , and discourse segmentation (Boguraev and Kennedy, 1997) have been reported. Radev et al. (Radev et al., 2004 ) developed an extractive multidocument summarizer, MEAD, which extracts a summary from multiple documents based on the document cluster centroid, position and firstsentence overlap. Recently, graph-based ranking methods, such as LexPageRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004 ), 1 http://haydn.isi.edu/ROUGE/ have been proposed for multi-document summarization. Similar to the original PageRank algorithm, these methods make use of similarity relationships between sentences and then rank sentences according to the ""votes"" or ""recommendations"" from their neighboring sentences. Lin and Hovy (2000) first introduced topic signatures which are topic relevant terms for summarization. Afterwards, this technique was successfully used in a number of summarization systems (Hickl et al., 2007, Gupta and Nenkova et al., 2007) . In order to improve sentence selection, we adopted the idea in a similar way to identify terms that tend to appear frequently in gene summaries and subsequently filter sentences that include none or few such terms. Compared with newswire document summarization, much less attention has been paid to summarizing MEDLINE documents for genic information. Ling et al. (Ling et al., 2006 and 2007) presented an automatic gene summary generation system that constructs a summary based on six aspects of a gene, such as gene products, mutant phenotype, etc. In their system, sentences were ranked according to a) the relevance to each category (namely the aspect), b) the relevance to the document where they are from; and c) the position where sentences are located. Although the system performed well on a small group of genes (10~20 genes) from Flybase, their method relied heavily on high-quality training data that is often hard to obtain in practice. Yang et al. reported a system (Yang et al., 2007 and 2009 ) that produces gene summaries by focusing on gene sets from microarray experiments. Their system first clustered gene set into functional related groups based on free text, Medical Subject Headings (MeSH ® ) and Gene Ontology (GO) features. Then, an extractive summary was generated for each gene following the Edmundson paradigm (Edmundson, 1969) (Hersh and Bhupatiraju, 2003) . Many teams approached the task as a sentence classification problem using GeneRIFs in the Entrez database as training data (Bhalotia et al., 2003; Jelier et al., 2003) . This task has also been approached as a single document summarization problem (Lu et al., 2006) . The gene summarization work presented here differs from the TREC task in that it deals with multiple documents. In contrast to the previously described systems for gene summarization, our approach has three novel features. First, we are able to summarize all aspects of gene-specific information as opposed to a limited number of predetermined aspects. Second, we exploit a lexical PageRank algorithm to establish similarity relationships between sentences. The importance of a sentence is based not only on the sentence itself, but also on its neighbors in a graph representation. Finally, we conducted an intrinsic evaluation on a large publicly available dataset. The gold standard assembled in this work makes it possible for comparisons between different gene summarization systems without human judgments. Method To determine if a sentence is extract worthy, we consider three different aspects: (1) the number of salient or informative words that are frequently used by human curators for writing gene summaries; (2) the relative importance of a sentence to be included in a gene summary; (3) the gene-specific information that is unique between different genes. Specifically, we look for signature terms in handwritten summaries for the first aspect. Ideally, computer generated summaries should resemble handwritten summaries. Thus the terms used by human curators should also occur frequently in automatically generated summaries. In this regard, we use a method similar to Lin and Hovy (2000) to identify signature terms and subsequently use them 2 http://ir.ohsu.edu/genomics/ to discard sentences that contain none or few such terms. For the second aspect, we adopt a lexical PageRank method to compute the sentence importance with a graph representation. For the last aspect, we treat each gene as having its own properties that distinguish it from others. To reflect such individual differences in the machinegenerated summaries, we exploit a gene's GO annotations as a surrogate for its unique properties and look for their occurrence in abstract sentences. Our gene summarization system consists of three components: a preprocessing module, a sentence ranking module, and a redundancy removal and summary generation module. Given a target gene, the preprocessing module retrieves corresponding MEDLINE abstracts and GO terms according to the gene2pubmed and gene2go data provided by Entrez Gene. Then the abstracts are split into sentences by the MEDLINE sentence splitter in the LingPipe 3 toolkit. The sentence ranking module takes these as input and first filters out some non-informative sentences. The remaining sentences are then scored according to a linear combination of the PageRank score and GO relevance score. Finally, a gene summary is generated after redundant sentences are removed. The system is illustrated in Figure 1 and is described in more detail in the following sections. Signature Terms Extraction There are signature terms for different topic texts (Lin and Hovy, 2000) . For example, terms such as eat, menu and fork that occur frequently in a corpus may signify that the corpus is likely to be We use the Pearson's chi-square test (Manning and Schütze, 1999) to extract topic signature terms from a set of handwritten summaries by comparing the occurrence of terms in the handwritten summaries with that of randomly selected MEDLINE abstracts. Let R denote the set of handwritten summaries and R denote the set of randomly selected abstracts from MEDLINE. The null hypothesis and alternative hypothesis are as follows: 0 H : ( | ) ( | ) i i P t R p P t R = = 1 1 2 H : ( | ) ( | ) i i P t R p p P t R = ≠ = The null hypothesis says that the term i t appears in R and in R with an equal probability and i t is independent from R . In contrast, the alternative hypothesis says that the term i t is correlated with R . We construct the following 2-by-2 contingency ij ij i j ij O E X E = − = ∑ where ij O is the observed frequency and ij E is the expected frequency. In our experiments, the significance level is set to 0.001, thus the corresponding chi-square value is 10.83. Lexical PageRank Scoring The lexical PageRank algorithm makes use of the similarity between sentences and ranks them by how similar a sentence is to all other sentences. It originates from the original PageRank algorithm (Page et al., 1998) that is based on the following two hypotheses: (1) A web page is important if it is linked by many other pages. (2) A web page is important if it is linked by important pages. The algorithm views the entire internet as a large graph in which a web page is a vertex and a directed edge is connected according to the linkage. The salience of a vertex can be computed by a random walk on the graph. Such graph-based methods have been widely adapted to such Natural Language Processing (NLP) problems as text summarization and word sense disambiguation. The advantage of such graph-based methods is obvious: the importance of a vertex is not only decided by itself, but also by its neighbors in a graph representation. The random walk on a graph can imply more global dependence than other methods. Our PageRank scoring method consists of two steps: constructing the sentence graph and computing the salience score for each vertex of the graph. Let { |1 } i S s i N = ≤ ≤ be the sentence collection containing all the sentences to be summarized. According to the vector space model (Salton et al., 1975) Taking each sentence as a vertex, and the similarity score as the weight of the edge between two sentences, a sentence graph is constructed. The graph is fully connected and undirected because the similarity score is symmetric. The sentence graph can be modeled by an adjacency matrix M , in which each element corresponds to the weight of an edge in the graph. Thus [ ] ij N N M × = M is defined as: , || || || || 0, i j i j ij s s if i j s s M otherwise ⋅ ⎧ ≠ ⎪ ⋅ = ⎨ ⎪ ⎩ We normalize the row sum of matrix M in order to assure it is a stochastic matrix such that the PageRank iteration algorithm is applicable. The normalized matrix is: 1 1 , 0 0, N N ij ij ij j j ij M M if M M otherwise = = ⎧ ≠ ⎪ = ⎨ ⎪ ⎩ ∑ ∑ . Using the normalized adjacency matrix, the salience score of a sentence i s is computed in an iterative manner: 1 (1 ) ( ) ( ) N i j j i j d score s d score s M N = − = ⋅ ⋅ + ∑ where d is a damping factor that is typically between 0.8 and 0.9 (Page et al., 1998) . If we use a column vector p to denote the salience scores of all the sentences in S , the above equation can be written in a matrix form as follows: [ ( 1 ) ] T p d d p = ⋅ + − ⋅ ⋅ M U where U is a square matrix with all elements being equal to 1/ N . The component (1 ) d − ⋅U can be considered as a smoothing term which adds a small probability for a random walker to jump from the current vertex to any vertex in the graph. This guarantees that the stochastic transition matrix for iteration is irreducible and aperiodic. Therefore the iteration can converge to a stable state. In our implementation, the damping factor d is set to 0.85 as in the PageRank algorithm (Page et al., 1998) . The column vector p is initialized with random values between 0 and 1. After the algorithm converges, each component in the column vector p corresponds to the salience score of the corresponding sentence. This score is combined with the GO relevance score to rank sentences. GO Relevance Scoring Up to this point, our system considers only geneindependent features, in both sentence filtering and PageRank-based sentence scoring. These features are universal across different genes. However, each gene is unique because of its own functional and structural properties. Thus we seek to include gene-specific features in this next step. The GO annotations provide one kind of genespecific information and have been shown to be useful for selecting GeneRIF candidates (Lu et al., 2006) . A gene's GO annotations include descriptions in three aspects: molecular function; biological process; and cellular component. For example, the human gene AANAT (gene ID 15 in Entrez Gene) is annotated with the GO terms in Table 4 . GO ID GO term GO:0004059 aralkylamine N-acetyltransferase activity GO:0007623 circadian rhythm GO:0008152 metabolic process GO:0008415 acyltransferase activity GO:0016740 transferase activity Table 4 . GO terms for gene AANAT The GO relevance score is computed as follows: first, the GO terms and the sentences are both stemmed and stopwords are removed. For example, the GO terms in Table 4 are processed into a set of stemmed words: aralkylamin, N, acetyltransferas, activ, circadian, rhythm, metabol, process, acyltransferas and transferas. Second, the total number of occurrence of the GO terms appearing in a sentence is counted. Finally, the GO relevance score is computed as the ratio of the total occurrence to the sentence length. The entire process can be illustrated by the following pseudo codes: Redundancy Removal A good summary contains as much diverse information as possible for a gene, while with as little redundancy as possible. For many well-studied genes, there are thousands of relevant papers and much information is redundant. Hence it is necessary to remove redundant sentences before producing a final summary. We adopt the diversity penalty method (Zhang et al., 2005; Wan and Xiao, 2007) for redundancy removal. The idea is to penalize the candidate sentences according to their similarity to the ones already selected. The process is as follows: (1) Initialize two sets, A φ = , { | 1, 2,..., } i B s i K = = containing all the extracted sentences; (2) Sort the sentences in B by their scores in descending order; (3) Suppose i s is the top ranked sentence in B , move it from B to A . Then we penalize the remaining sentences in B as follows: For each sentence j s in B , j i ≠ sim s s is the similarity between i s and j s . (4) Repeat steps 2 and 3 until enough sentences have been selected. Results and Discussion Evaluation Metrics Unlike the newswire summarization, there are no gold-standard test collections available for evaluating gene summarization systems. The two previous studies mentioned in Section 2 both conducted extrinsic evaluations by asking human experts to rate system outputs. Although it is important to collect direct feedback from the users, involving human experts makes it difficult to compare different summarization systems and to conduct large-scale evaluations (both studies evaluated nothing but a small number of genes). In contrast, we evaluated our system intrinsically on a much larger dataset consisting of 7,294 human genes, each with a preexisting handwritten summary downloaded from the NCBI's FTP site 5 . The handwritten summaries were used as reference summaries (i.e. a gold standard) to compare with the automatically generated summaries. Although the length of reference summaries varies, the majority of these summaries contain 80 to 120 words. To produce a summary of similar length, we decided to select five sentences consisting of about 100 words. For the intrinsic evaluation of a large number of summaries, we made use of the ROUGE metrics that has been widely used in automatic evaluation of summarization systems (Lin and Hovy, 2003; Hickl et al., 2007) . It provides a set of evaluation metrics to measure the quality of a summary by counting overlapping units such as n-grams or word sequences between the generated summary and its reference summary. We computed three ROUGE measures for each summary, namely ROUGE-1 (unigram based), ROUGE-2 (bigram based) and ROUGE-SU4 (skip-bigram and unigram) (Lin and Hovy, 2003) . Among them, ROUGE-1 has been shown to agree most with human judgments (Lin and Hovy, 2003) . However, as biomedical concepts usually contain more than one word (e.g. transcription factor), ROUGE-2 and ROUGE-SU4 scores are also important for assessing gene summaries. Determining parameters for best performance The two important parameters in our system -the linear coefficient α for the combination of Page-Rank and GO scores and the diversity penalty degree factor ω in redundancy removal -are investigated in detail on a collection of 100 randomly selected genes. First, by setting α to values from 0 to 1 with an increment of 0.1 while holding ω steady at 0.7, we observed the highest ROUGE-1score when α was 0.8 (Figure 2 ). This suggests that the two scores (i.e. PageRank and GO score) complement to each other and that the PageRank score plays a more dominating role in the summed score. Next, we variedω gradually from 0 to 5 with an increment of 0.25 while holding α steady at 0.75.The highest ROUGE-1 score was achieved when ω was 1.3 (Figure 3 ). For ROURE-2, the best performance was obtained when α was 0.7 and ω was 0.5. In order to balance ROUGE-1 and ROUGE-2 scores, we set α to 0.75 and ω to 0.7 for the remaining experiments. Comparison with other methods Because there are no publicly available gene summarization systems, we compared our system with two baseline methods. The first is a well known publicly available summarizer -MEAD (Radev et al., 2004) . We adopted the latest version of MEAD 3.11 and used the default setting in MEAD that extracts sentences according to three features: centroid, position and length. As shown in Table 5 , our system significantly outperformed the two baseline systems in all three ROUGE measures. Furthermore, larger performance gains are observed in ROUGE-2 and ROUGE-SU4 than in ROUGE-1. This is because many background words (e.g. gene, protein and enzyme) also appeared frequently as unigrams in randomly selected summaries. In Figure 4 , we show that the majority of the summaries have a ROUGE-1 score greater than 0.4. Our further analysis revealed that almost half summaries with a low score (smaller than 0.3) either lacked sufficient relevant abstracts, or the reference summary was too short or too long. In either case, only few overlapping words can be found when comparing the generated gene summary with the reference. The statistics for low ROUGE-1 score are listed in Table 6 . We also note that almost half of the summaries that have low ROUGE-1 scores were due to other causes: mostly, machine generated summaries differ from human summaries in that they describe different functional aspects of the same gene product. Take the gene TOP2A (ID: 7153) for example. While both summaries (handwritten and machine generated) focus on its encoded protein DNA topoisomerase, the handwritten summary describes the chromosome location of the gene whereas our algorithm selects statements about its gene expression when treated with a chemotherapy agent. We plan to investigate such differences further in our future work. Results on various summary length Figure 5 shows the variations of ROUGE scores as the summary length increases. At all lengths and for both ROUGE-1 and ROUGE-2 measures, our proposed method performed better than the two baseline methods. By investigating the scores of different summary lengths, it can be seen that the advantage of our method is greater when the summary is short. This is of great importance for a summarization system as ordinary users typically prefer short content for summaries. In this paper we have presented a system for generating gene summaries by automatically finding extract-worthy sentences from the biomedical literature. By using the state-of-the-art summarization techniques and incorporating gene specific annotations, our system is able to generate gene summaries more accurately than the baseline methods. Note that we only evaluated our system for human genes in this work. More summaries are available for human genes than other organisms, but our method is organism-independent and can be applied to any other species. This research has implications for real-world applications such as assisting manual database curation or updating existing gene records. The ROUGE scores in our evaluation show comparable performance to those in the newswire summarization (Hickl et al., 2007) . Nonetheless, there are further steps necessary before making our system output readily usable by human curators. For instance, human curators are generally in favor of sentences presented in a coherent order. Thus, information-ordering algorithms in multi-document summarization need to be investigated. We also plan to study the guidelines and scope of the curation process, which may provide additional important heuristics to further refine our system output. ",16527070,01ad43b5665e8f56dae779d445ed5c67ea34790e,17,https://aclanthology.org/W09-1312,Association for Computational Linguistics,"Boulder, Colorado",2009,June,Proceedings of the {B}io{NLP} 2009 Workshop,"Jin, Feng  and
Huang, Minlie  and
Lu, Zhiyong  and
Zhu, Xiaoyan",Towards Automatic Generation of Gene Summary,97--105,,,,,,,inproceedings,jin-etal-2009-towards,,,
23,Y18-1058,"In this paper we proposed the use of effective features and transfer leaning to improve the accuracies of neural-network-based models for accurate semantic role labeling (SRL) of Japanese, which is an aggregated language. We first reveal that the final morphemes in each argument, which have not been discussed in previous work on English SRL are effective features in determining semantic role labels in Japanese. We then discuss the possibility of using large-scale training corpora annotated with different semantic labels from the target semantic labels by transfer learning on CNN, 3-LNN, and GRU models. The experimental results of Japanese SRL on the proposed models indicate that all of the neural-network-based models performed better with transfer learning as well as using the feature vectors of final moprhemes in each argument. 1 In CoNLL 2009 (Hajič et al., 2009), annotated corpora with semantic roles in multiple languages including Japanese were used; however, most semantic tags in the Japanese corpus were case-marker-based relations, which are different from semantic roles annotated in PropBank. 2","In this paper we proposed the use of effective features and transfer leaning to improve the accuracies of neural-network-based models for accurate semantic role labeling (SRL) of Japanese, which is an aggregated language. We first reveal that the final morphemes in each argument, which have not been discussed in previous work on English SRL are effective features in determining semantic role labels in Japanese. We then discuss the possibility of using large-scale training corpora annotated with different semantic labels from the target semantic labels by transfer learning on CNN, 3-LNN, and GRU models. The experimental results of Japanese SRL on the proposed models indicate that all of the neural-network-based models performed better with transfer learning as well as using the feature vectors of final moprhemes in each argument. 1 In CoNLL 2009 (Hajič et al., 2009), annotated corpora with semantic roles in multiple languages including Japanese were used; however, most semantic tags in the Japanese corpus were case-marker-based relations, which are different from semantic roles annotated in PropBank. 2 Introduction Several studies on semantic role labeling (SRL) have been conducted, mainly for English texts with a wide coverage from revealing syntactic and grammatical features that impact SRL decisions (Gildea and Jurafsky, 2002) to end-to-end models without syntactic inputs. This is because current rich language resources are related to annotated corpora with semantic roles such as PropBank (Kingsbury et al., 2002) , FrameNet (Baker et al., 1998) , and shared tasks of CoNLL 2005 (Carreras and Màrquez, 2005) , 2009 (Hajič et al., 2009 ), and 2012 (Pradhan et al., 2012) . Instead of English SRL, most studies (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011; Hayashibe et al., 2011; Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) on SRL for Japanese texts, the grammar of which is quite different from English, have been focused on detecting three types of case-marker-based labels, i.e., nominative, accusative, and dative. This is because large-scale corpora annotated with these three labels have been developed (Kyoto Corpus (Kawahara et al., 2002) and NAIST Text Corpus (Iida et al., 2007) ) and widely used 1 . There are, however, annotated corpora with semantic tags corresponding to semantic role labels (EDR corpus 2 , BCCWJ-PT (Takeuchi et al., 2015) and Japanese FrameNet (Ohara et al., 2011) ) or semantic categories (GDA corpus 3 ) that contain semantic relations as cause and location. Thus, by constructing a Japanese SRL system using these resources, we can discuss the effective approaches and models for a situation without standard semantic-role-labeled corpora as well as effective features for Japanese SRL. We address the following two issues: what is an effective grammatical feature for Japanese SRL, and what is a model when there are no standard labeled corpora for Japanese SRL. We first argue that the last two morphemes of arguments have an effect on determining SRL. This is because Japanese grammar allows scrambling (Saito, 1989) ; thus, case markers and functional expressions located at the last part of arguments contribute to the selection of semantic relations. We then propose three neural-network-based models, i.e., a convolutional network (CNN), 3-layer neural-network (3-LNN) and GRU, by applying transfer learning for using different semantic-role-labeled corpora. With transfer learning, using annotated corpora with different tag sets from the target semantic role labels is expected. We conducted experiments on the Japanese SRL accuracy of the proposed models that involved using the BCCWJ Predicate-Thesaurus (BCCWJ-PT) corpus, which is annotated with dependencies for target verbs in BCCWJ (Maekawa et al., 2014) . The reasons of using BCCWJ-PT are 1) most of the semantic role labels correspond to the well-known categories of Agent, Theme, Goal, and Recipient, which are also used in other resources (e.g., Verb-Net and FrameNet); and 2) the semantic role labels and their verb senses (i.e., semantic frames) are defined with example sentences in the Predicate Thesaurus (PT), which are freely available on the Internet 4 . The experimental results indicate that the last two morphemes of arguments sufficiently boosted the SRL performance of our neural-network-based models as well as a baseline, i.e., an SVM-based model. We also reveal that transfer learning improved the accuracy of recognizing labels annotated in BCCWJ-PT using different semantic tags annotated in GDA. Characteristics of Japanese SRL There are two main characteristics of Japanese SRL, i.e., Japanese grammatical features and language resources for SRL. Japanese Grammatical Features for SRL In Japanese syntax, the case markers located in the final part of each phrase, play important roles in determining the semantic relations (i.e., semantic roles) to the predicate. Not only the case markers but also certain functional multi-morphemes, attached to the final part of each phrase have an effect on determining the semantic relations to the predicate. The example sentence in Figure 1 shows !""#$%&""'#(&)(*'$$% +&'($!,%(&)-.%/ 0#1!)$'(2! ""*!&3%1#(3!1%($! !""#$%&""'(&)*#+&$',$ -./0('*123$ 4""54&.*67889:;*678< <(&$-./0('$%""=$4""54&.&#$#+&$',$!""#$%&""'(&) Fig. 1 : Example of functional multi-morphemes that the different functional morphemes ni-yotte, no-tame-ni, and de 5 can designate the same type of semantic relation, i.e., ""Cause"" to the verb kyanseru (cancel). Also, certain functional morphemes as well as case markers can provide different semantic relations depending on their contexts. Unfortunately, there is no standard dictionary of functional morphemes 6 nor morphological analyzers that can detect these functional morphemes 7 . Thus, we do not currently have language resources that can help determine the possible semantic relations that functional morphemes provide 8 . With this background, we take the last two morphemes in each argument as a simple grammatical feature to take into account the functional morphemes and case markers of arguments in Japanese, as in a previous study (Ishihara and Takeuchi, 2016) . Language Resources of Japanese SRL Several language resources have been constructed on internal semantic relations in predicates and their arguments for Japanese. The language resource EDR provides English and Japanese parsed corpora containing about 400,000 examples, which are annotated with semantic tags and original sense tags defined in the EDR dictionary. The semantic tags not only contain semantic role labels, such as Agent and Object, but also semantic relations 5 Most of the functional suffixes are written in Hiragana． 6 There was a study on the collection of Japanese suffixes (Matsuyoshi et al., 2007) ; however, there are still no registered functional morphemes such as to-issyoni (with). 7 For example, the Japanese morphological analyzer MeCab (http://taku910.github.io/mecab/) with the IPA dictionary does not extract the functional suffix no-tame-ni. 8 The functional morphemes are annotated in CoNLL2009 not as features but as semantic role tags (i.e., target of disambiguations). Thus, the semantic role tags in the Japanese corpus of CoNLL 2009 might be different from those for English SRL. between main and subordinate clauses, e.g., Cooccurrence and Sequence; however, these similar semantic role tags are not connected to predicate concepts, such as frames, as defined in PropBank or FrameNet. Thus, we do not use EDR as the target SRL corpus. Unlike the EDR corpus, the JFN (Ohara et al., 2003) was constructed in the same structure of English FrameNet, however, JFN is a closed corpus, and several essential information such as total amount of the annotated sentences and number of lexicons are not revealed. Thus, it does not seem to be easy to use the JFN corpus. The GDA corpus has 37,000 sentences that are annotated with 100 semantic tags that contain not only semantic role tags but discourse-related, syntactic, and grammatical tags. This can be regarded as a sufficient amount; however, the semantic role tags are not related to frames for predicates nor are lexical frames provided. Instead of the above semantic role-related corpora in Japanese, BCCWJ-PT contains 64 semantic role labels that are based on the analysis of semantic relations between predicates and arguments for about 5,000 sentences. The semantic role labels are also connected to frames defined in the PT, which is freely published on the Web. This framework of tags and frames is the same as PropBank and FrameNet. We therefore use BCCWJ-PT as the target corpus of Japanese SRL. Task Definition To focus on the impact of how features and learning models can recognize semantic roles in Japanese texts, we apply the simple SRL task, i.e., SRL systems identify the semantic role labels for a pair of an argument and head verb that are correctly extracted in the preprocessing step. Figure 2 shows an example of a partial sentence annotated with semantic roles in BCCWJ-PT. The BCCWJ-PT corpus has not only semantic role labels but also annotated tags of morphemes, the target predicate, and its arguments; thus, we convert the tagged texts into the target data, which are separated into target semantic role labels and their features. Figure 3 shows an example of the target data converted from the annotated data in Figure 2 . In the example features in Figure 3 , basic forms as well as surface forms of the verbs are stored for normalization as the predicate features. The basic forms and/or the surface forms are selected as the predicate features depending on the models of neural networks. Proposed Models and Approaches Previous work has shown that neural-networkbased approaches models are powerful for English SRL (He et al., 2017) as well as Japanese case-marker disambiguation and anaphora detection (Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) ; however, neural-network-based models have various tuning parameters such as network structure, hyper parameters, and several methods of avoiding over fitting. Therefore, it is not easy to determine the cause-and-effect relations between models and the final results. Thus, we used simple neuralnetwork architectures to focus on clarifying what input features are effective, which learning models are powerful, and how different labeled corpora can be effective for the target label set. As described above, several corpora annotated with different sets of semantic role labels for Japanese have been constructed, but the amount of labeled examples for each annotated corpus is limited. Thus, we applied transfer learning, which uses corpora annotated with different semantic labels from the target labels. Because of the flexibility of the neural-network structures, transfer learning can be easily implemented by changing the network structure. In the following sections, we give details of the three proposed models, their input features, and the methods of transfer learning. 3-LNN Our 3-LNN model is applied to determine semantic role labels by varying the following different input feature vectors to determine the effectiveness of the input feature vector. (1) Bag-of-Words (BOW) for morphemes in an argument and its head verb (Asahara, 2018) that are made from 1 billion corpus of Japanese using fasttext 9 . These vectors are expected to determine the tendency between semantic role tags and the combination of a content morpheme and verb. (3) Feature vector (2) with a BOW of the last two morphemes in an argument. This added feature is expected to capture functional suffixes in arguments, as described in Section 2.1. For example, feature vector (1) for the first data in Figure 3 is a BOW vector of granturisumo, 4, o, and kau. In feature vector (2), the skip-gram vectors of the content morpheme-head, i.e., 4 and its verb kau, are added to feature vector (1). In feature vector (3), a BOW vector of the two morphemes 4 and o is added to feature vector (2). Regarding the network structure and tuning methods, ReLU (Nair and Hinton, 2010) is applied to the non-linear function of the intermediate units, and softmax is used as the function of the units at the final layer. To obtain better accuracy for test data, we applied a dropout method for the intermediate units with 50%; and used Adam as an optimization method with the recommended setting 10 . CNN Instead of using BOW, we take certain sequential characteristics in verb-argument features with a simple CNN. Our CNN is a simple structure that consists of an input layer, convolution layer, pooling layer, and output layer (Figure 4 ). The input of the CNN is a sequence of feature morphemes in verb-argument order. The nwjc2vec was used to convert the input morphemes to the skip-gram vectors with 200 dimensions. We define the three types of filters that take into account three, four, and five successive morphemes. The number of filters for each type is 128.  After the convolution layer, a pooling layer with max pooling is applied, then a fully connected layer is applied to the connection to the final output layer. In addition to the simple network, we also use a variation model in which the input features defined in the 3-LNN model are added into the previous final layer, as mentioned in Section 6. !""#$%%&'((!""#)&*""+ !!! ""#'*&"")((!""#$,$+-( !!! !""#'$%,$((!""#$""$-* !!! ""#'***+((!""#--*&' !!! ""#)-+-$((!""#$&,&% !!! ""#$%%'&((!""#)%)' !!! ""#''$**((!""#)+-+$ !!! ./ GRU To focus on the sequential feature of arguments and verbs, we use our GRU model, which is a type of recurrent neural network. Figure 5 shows the structure of how we apply our GRU model for SRL. Input feature morphemes are converted to dense vectors with nwjc2vec, the final GRU state is applied to a dense layer, then the final output is obtained. !""#$%&""'(&)* ! ! ! !""# !""# !""# ! ! ! $%& To observe how the order and inflection of input morphemes can contribute to the accuracy of identifying semantic role labels, four types of input features are defined in Table 1 . The slash in the examples denotes a delimiter of morphemes. The GRU model takes a verb with the final position in v1, but a verb or verbs at the first position in the other features. Feature vectors v3 and v4 have a surface form of the verb. Transfer Learning As described in Section 2.2, the amount of the target corpus, BCCWJ-PT, is limited; thus, we apply a method of using other annotated corpora whose semantic labels are even different from that of the target corpus, which is called transfer learning. The basic procedure of transfer learning is as follows: 1) a neural network is trained using an annotated corpus of different semantic tags from the target corpus; 2) the units at the final layer of the neural network are replaced with new units for the target semantic tags; and 3) all the weights in the neural work are trained using the target corpus 11 . Transfer learning is applied to the three proposed neural-network-based models. Baseline Model We apply an SVM-based model that has a linear kernel to the SRL task as a baseline model. For multi-class categorization, we use the one-versusrest method. The input feature is the BOW, which is the same as case (1) used in the 3-LNN model. Experiments on Japanese SRL Accuracy Experimental Setup The BCCWJ-PT corpus was the target corpus containing 5069 sentences, 64 semantic role labels for 548 verb types 12 . We extracted 10,390 instances of the target data whose format is shown in Figure 3 , i.e., combinations of an argument and its verb. The target data were divided into the three parts: training (65%), development (5%), and test (30%). The GDA corpus containing 100 semantic tags, which are different from those in BCCWJ-PT, was selected as the corpus for transfer learning. We extracted 82,892 instances whose format is the same as that in Figure 3 . The instances of GDA were divided into development (85%), training (5%), and test (10%) data. Table 2 shows the details of the data sets used in the experiments. The top five most frequent semantic role labels contained in the BCCWJ-PT data are listed in Table 3 . The semantic role labels of Theme and Agent were most frequent, which indicates the same tendency shown in PropBank (Palmer et al., 2005) , which is an English semantic-role-labeled corpus. We applied the proposed models to the training data for training parameters and applied them to the test data for evaluating their SRL accuracy. The SRL accuracy of the SVM model was evaluated with 5-fold cross validation of all target data. This indicates that the SVM model is advantageous compared to our neural-network-based models, because the amount of training data is larger than that of our neural-network-based models. The SRL accuracies of our models and the SVM model were evaluated based on the instances of the test data using the following accuracy formula: accuracy = #instances correctly estimated labels #total instances (1) In the SVM model, evaluation on test data in one hold was conducted with the above accuracy formula. The final accuracy of the SVM model was then an average evaluated based on all the test data in 5-fold cross validation. The mini-batch size of all three proposed neuralnetwork-based models was set to 100, and the number of training iterations, i.e., epochs, was determined using the accuracy of the development data. To avoid overfitting to the training data, we used 20 epochs on the BCCWJ-PT data because almost all the proposed models converged in 20 epochs. Experimental Results Table 4 lists that experimental results of the SRL accuracies of the proposed and baseline models using only the BCCWJ-PT data. All three neural-network-based models outperformed the SVM model regarding SLR accuracy. When we look at the effectiveness of the features in the SVM and 3-LNN models, the skip-gram vectors significantly improved the accuracies of the both models. Adding a BOW of the last two morphemes in each argument further improved their accuracies. For the GRU results, v2 showed the best performance among the other feature sets. This indicates that the verb should come first in the input sequence by comparing to v1; and the base form of the verbs must be more effective than the inflected form compared to the results of v3 and v4. The SRL accuracy of the GRU model, however, was inferior to that of the 3-LNN model. This indicates that the GRU model currently does not seem to fully use contextual information. The best SRL accuracy of all the models was that of our CNN model. Compared to the 3-LNN model, the convolution and pooling structure contributes to improve 0.015 points. The CNN model with only using the base feature vector conv did not perform as well as the 3-LNN model with the BOW + skip + two feature vector. This indicates that the manually designed feature vectors, i.e., skip + two with BOW, work well to obtain the characteristics of the semantic role labels. Table 5 shows the results of incorporating transfer learning, i.e., using the GDA data for training the initial values of the weights. All three proposed neural-network-based models improved their accuracies with transfer learning, but the increase in the accuracies differed depending on the model. The 3-LNN model had the most improvement with transfer learning and showed the best accuracy among all the models. The CNN model improved in accuracy, but was not as effective as the 3-LNN model. The accuracy of the GRU model also increased 0.011 points within the maximum score, but the best accuracy of the GRU model was lower than those of the other models. According to the effects of the training epochs in the GDA data, too many training epochs for the GDA data will decrease the SLR accuracy with BCCWJ-PT for all three models. This indicates that neural-network-based models would have caused overfitting to the GDA data if the models were trained with too many iterations. Thus, we need to stop the training in GDA data with a small number of iterations. Table 5 shows that the best training epoch for the 3-LNN and CNN models is only ten iterations, which would be the best for obtaining the initial weights towards learning the final BCCWJ-PT data. Discussions The results of transfer learning in Table 5 indicate that transfer learning contributed to the improvement in the SRL accuracies of our neural-networkbased models, but the best accuracy score of 0.67 is not so different from 0.665 with the CNN model without transfer learning even though the GDA data are about ten times larger than the BCCWJ-PT data. The role of transfer learning is to obtain better initial weights in neural-network-based models than randomized initial weights. In transfer learning, all the units in the final layer for GDA tags are discarded; however, some of the tags are almost the same as the semantic role tags defined in BCCWJ-PT, such as Agent, Theme, and Goal. Therefore, we must consider how we can use the similar semantic tags in transfer learning. As described in Section 2.1, the last two morphemes in each argument are defined to capture functional suffixes that have an effect on determining its semantic role of the argument. The experimental results listed in Table 4 reveal that the multi-word functional suffix i.e., two feature vector, improves the accuracy of the GRU model as well as those of the SVM and 3-LNN models. The three feature vectors v2, v3, v4 outperformed v1 in terms of SRL accuracy. Since v1 is only the case in which a verb comes at the end, the other case markers come last. In the GRU model, the final layer of the GRU loated at the final positions of a time sequence determines the label. Therefore, the last two morphemes located at the final positions of a time sequence are naturally taken into account in v2, v3, and v4. The accuracies of all the models show that the last morphemes have a positive effect on determining the semantic role labels in Japanese. Related Work Several SRL studies have been conducted mainly in English because of existing high-quality language resources such as FrameNet and PropBank as well as shared tasks of semantic role labels such as in CoNLL-2005 (Carreras and Màrquez, 2005) , 2009 (Hajič et al., 2009 ) and 2012 (Pradhan et al., 2012) . In the early stage of SRL investigation, statistical modeling and effective features for SRL have been studied. Gildea and Jurafsky (2002) revealed that several syntactic features, such as parse tree path, phrase type, and voice, can improve the accuracy of a statistical learning model. More detailed features were studied by Surdeanu et al. (2003) and Xue and Palmer (2004) . Toutanova et al. (2008) showed effective combinations of statistical joint models with rich features. Syntactic features are powerful; however, parse errors will decrease the accuracies of SRL systems. Thus Zhou and Xu (2015) proposed an end-toend SRL model using bi-directional long shortterm memory (LSTM) without any syntactic features. Roth and Lapata (2016) used dependency information on LSTM. The dependency path is convenient, but He et al. ( 2017 ) revealed that higher accuracies on neural-network-based SRL models could be obtained if correct parsed information is available. Most studies on Japanese SRL (Taira et al., 2008; Imamura et al., 2009; Sasano and Kurohashi, 2011; Hayashibe et al., 2011; Ouchi et al., 2015; Shibata et al., 2016; Ouchi et al., 2017; Matsubayashi and Inui, 2018) have been focused on recognizing three types of case-marker-based semantic roles with anoaphra resolutions. Taira et al. (2008) have shown that the detailed noun categories of nominals in arguments improve the accuracy of statistical models for recognizing the three case-markers. Imamura et al. (2009) proposed effective grammatical features such as dependency path, phrase positions, and several detailed characteristics. Sassano and Kurohashi 2011 and Hnagyo et al. 2013 proposed models to use large-scale case frames to provide selectional preference between a head noun in an argument and its predicate. Ouchi et al. (2015) , Shibata et al. (2016) , Ouchi et al. (2017) and Matsubayashi and Inui (2018) proposed neural-network-based models. These studies are focused on anaphora resolution, i.e., detecting arguments for a predicate without dependency relations and recognizing their semantic roles. Thus, these studies discussed how to incorporate the effectiveness of multiple predicates. For SRL in Japanese, Ishihara and Takeuchi (2015) revealed that the last morphemes in an argument are effective on a linear-chain CRFs for determining 64 semantic roles for BCCWJ-PT. Thus, the effective grammatical features as well as approaches on neural-network-based models for Japanese SRL are required to be studied. Conclusion We propose three neural-network-based models and described the effective features and methods for Japanese SRL. We revealed that the last two morphemes in an argument, the dense morpheme vector concatenated with a head noun morpheme and its predicate, and bag-of-morphemes in an argument, are effective for our 3-LNN and CNN models. We conducted experiments on Japanese SRL with BCCWJ-PT containing 64 semantic roles, which was different from most previous studies, which focused on 3 semantic roles. We applied transfer learning using GDA, which has different semantic role tags from BCCWJ-PT. After pre-training the weights using the GDA data, the neural network models were trained on the BCCWJ-PT data. The experimental results indicate that transfer learning improved the accuracies of all three proposed neural network models compared to the cases without transfer learning. We are planning more detailed analyses of the combinations of features and neural network models on BCCWJ-PT.",201642107,f0b536df92c287b556a482f1480e52764bf1202c,1,https://aclanthology.org/Y18-1058,Association for Computational Linguistics,Hong Kong,2018,1{--}3 December,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation","Okamura, Takuya  and
Takeuchi, Koichi  and
Ishihara, Yasuhiro  and
Taguchi, Masahiro  and
Inada, Yoshihiko  and
Iizuka, Masaya  and
Abo, Tatsuhiko  and
Ueda, Hitoshi",Improving {J}apanese semantic-role-labeling performance with transfer learning as case for limited resources of tagged corpora on aggregated language,,,,,,,,inproceedings,okamura-etal-2018-improving,,,
24,R13-1050,"We are interested in a graph-based Knowledge Representation formalism that would allow for the representation, manipulation, query, and reasoning over dependency structures, and linguistic knowledge of the Explanatory and Combinatorial Dictionary in the Meaning-Text Theory framework. Neither the semantic web formalisms nor the conceptual graphs appear to be suitable for this task, and this led to the introduction of the new Unit Graphs framework. This paper first introduces the foundational concepts of this framework: Unit Graphs are defined over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. Then, this paper provides all of these objects with a model semantics that enables to define the notion of semantic consequence between Unit Graphs.","We are interested in a graph-based Knowledge Representation formalism that would allow for the representation, manipulation, query, and reasoning over dependency structures, and linguistic knowledge of the Explanatory and Combinatorial Dictionary in the Meaning-Text Theory framework. Neither the semantic web formalisms nor the conceptual graphs appear to be suitable for this task, and this led to the introduction of the new Unit Graphs framework. This paper first introduces the foundational concepts of this framework: Unit Graphs are defined over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial symbols, and iii) a set of unit identifiers. Then, this paper provides all of these objects with a model semantics that enables to define the notion of semantic consequence between Unit Graphs. Introduction We are interested in the ability to reason over dependency structures and linguistic knowledge of the Explanatory and Combinatorial Dictionary (ECD), which is the lexicon at the core of the Meaning-Text Theory (MTT) (Mel'čuk, 2006) . Some formalisation works have been led on the ECD. For instance Kahane and Polguère, 2001) proposed a formalization of Lexical Functions, and the Definiens project (Barque and Polguère, 2008; Barque et al., 2010) aims at formalizing lexicographic definitions with genus and specific differences for the TLFi 1 . Adding to these formalization works, the goal of the Unit Graphs formalism is to propose a formalization from a knowledge engineering perspective, compatible with standard Knowledge Representation (KR) formalisms. The term formalization here means not only make nonambiguous, but also make operational, i.e., such that it supports logical operations (e.g., knowledge manipulation, query, reasoning). We thus adopt a knowledge engineering approach applied to the domain of the MTT. At first sight, two existing KR formalisms seemed interesting for representing dependency structures: semantic web formalisms (RDF/S, OWL, SPARQL), and Conceptual Graphs (CGs) (Sowa, 1984; Chein and Mugnier, 2008) . Both formalisms are based on directed labelled graph structures, and some research has been done towards using them to represent dependency structures and knowledge of the lexicon (OWL in (Lefranc ¸ois and Gandon, 2011; Boguslavsky, 2011) , CGs at the conceptual level in (Bohnet and Wanner, 2010) ). Yet Lefranc ¸ois, 2013) showed that neither of these KR formalisms can represent linguistic predicates. As the CG formalism is the closest to the semantic networks, the following choice has been made (Lefranc ¸ois, 2013): Modify the CGs formalism basis, and define transformations to the RDF syntax for sharing, and querying knowledge. As we are to represents linguistic units of different nature (e.g., semantic units, lexical units, grammatical units, words), term unit has been chosen to be used in a generic manner, and the result of this adaptation is thus the Unit Graphs (UGs) framework. The valency-based predicates are represented by unit types, and are described in a structure called the unit types hierarchy. Unit types specify through actant slots and signatures how their instances (i.e., units) may be linked to other units in a UG. Unit Graphs are then defined over a support that contains: i) a hierarchy of unit types which is strongly driven by their actantial structure, ii) a hierarchy of circumstantial sym-bols, and iii) a set of unit identifiers. Apart from giving an overview foundational concepts of the UGs framework, the main goal of this paper is to answer the following research question: What semantics can be attributed to UGs, and how can we define the entailment problem for UGs ? The rest of this paper is organized as follows. Section 2 overviews the UGs framework: the hierarchy of unit types ( §2.1), the hierarchy of circumstantial symbols ( §2.2), and the Unit Graphs ( §2.3). Then, section 3 provides all of these mathematical objects with a model, and finally the notion of semantic consequence between UGs is introduced ( §3.4). 2 Background: overview of the Unit Graphs Framework For a specific Lexical Unit L, (Mel'čuk, 2004, p.5) distinguishes considering L in language (i.e., in the lexicon), or in speech (i.e., in an utterance). KR formalisms and the UGs formalism also make this distinction using types. In this paper and in the UGs formalism, there is thus a clear distinction between units (e.g., semantic unit, lexical unit), which will be represented in the UGs, and their types (e.g., semantic unit type, lexical unit type), which are roughly classes of units for which specific features are shared. It is those types that specify through actant slots and signatures how their instances (i.e., units) are to be linked to other units in a UG. Hierarchy of Unit Types Unit types and their actantial structure are described in a structure called hierarchy, that specifies how units may, must, or must not be interlinked in a UG. Definition 2.1. A hierarchy of unit types is denoted T and is defined by a tuple: T def = (T D , S T , γ γ γ, γ γ γ 1 , γ γ γ 0 , C A , ⊥ A , {ς ς ς t } t∈T ) This structure has been thoroughly described in (Lefranc ¸ois and Gandon, 2013a; Lefranc ¸ois, 2013) . Let us overview its components. T D is a set of declared Primitive Unit Types (PUTs). This set is partitioned into linguistic PUTs of different nature (e.g., deep semantic, semantic, lexical). S T is a set of Actant Symbols (ASymbols). γ γ γ (resp1. γ γ γ 1 , resp2. γ γ γ 0 ) assigns to every s ∈ S T its radix 2 (resp1. obligat 3 , resp2. prohibet 4 ) unit type γ γ γ(s) (resp1. γ γ γ 1 (s), resp2. γ γ γ 0 (s)) that introduces (resp1. makes obligatory, resp2. makes prohibited) an Actant Slot (ASlot) of symbol s. The set of PUTs is denoted T and defined as the disjoint union of T D , the ranges of γ γ γ, γ γ γ 1 and γ γ γ 0 , plus the prime universal PUT and the prime absurd PUT ⊥ (eq. 1). T def = T D • ∪ γ γ γ(S T ) • ∪ γ γ γ 1 (S T ) • ∪ γ γ γ 0 (S T ) • ∪{⊥, } (1) T is then pre-ordered by a relation which is computed from the set C A ⊆ T 2 of asserted PUTs comparisons. t 1 t 2 models the fact that the PUT t 1 is more specific than the PUT t 2 . Then a unit type has a set (that may be empty) of ASlots, whose symbols are chosen in the set S T . Moreover, ASlots may be obligatory, prohibited, or optional. The set of ASlots (resp1. obligatory ASlots, resp2. prohibited ASlots, resp3. optional ASlots) of a PUT is thus defined as the set of their symbols α α α(t) ⊆ S T (resp1. α α α 1 (t), resp2. α α α 0 (t), resp3. α α α ? (t)). The set of ASlots (resp1. obligatory ASlots, resp2. prohibited ASlots) of a PUT t ∈ T is defined as the set of ASymbol whose radix (resp1. obligat, resp2. prohibet) is more general or equivalent to t, and the set of optional ASlots of a PUT t is the set of ASlots that are neither obligatory nor prohibited. The number of ASlots of a PUT is denoted its valency. {ς ς ς t } t∈T , the set of signatures of PUTs, is a set of functions. For all PUT t, ς ς ς t is a function that associates to every ASlot s of t a set of PUT ς ς ς t (s) that characterises the type of the unit that fills this slot. Signatures participate in the specialization of the actantial structure of PUTs, which means that if t 1 t 2 and s is a common ASlot of t 1 and t 2 , the signature of t 1 for s must be more specific or equivalent than that of t 2 . Hence t 1 t 2 implies that the actancial structure of t 1 is more specific than the actantial structure of t 2 . Now a unit type may consist of several conjoint PUTs. We introduce the set T ∩ of possible Conjunctive Unit Types (CUTs) over T as the power-set 5 of T. The set ⊥ A is the set of declared absurd CUTs that can not be instantiated. The definition of the actancial structure of PUTs is naturally extended to CUTs as follows: α α α ∩ (t ∩ ) def = t∈t ∩α α α(t) (2) α α α ∩ 1 (t ∩ ) def = t∈t ∩α α α 1 (t) (3) α α α ∩ 0 (t ∩ ) def = t∈t ∩α α α 0 (t) (4) α α α ∩ ? (t ∩ ) def = α α α ∩ (t ∩ ) − α α α ∩ 1 (t ∩ ) − α α α ∩ 0 (t ∩ ) (5) ς ς ς ∩ t ∩ (s) def = t∈t ∩ |s∈α α α(t) ς ς ς t (s) (6) Finally the pre-order over T is extended to a pre-order ∩ over T ∩ as defined by Lefranc ¸ois and Gandon, 2013a). Lefranc ¸ois and Gandon, 2013b) proved that in the hierarchy of unit types, if t ∩ 1 ∩ t ∩ 2 then the actantial structure of t ∩ 1 is more specific than that of t ∩ 2 , except for some degenerated cases. Thus as one goes down the hierarchy of unit types, an ASlot with symbol s is introduced by the radix {γ γ γ(s)} and first defines an optional ASlot for any unit type t ∩ more specific than {γ γ γ(s)}, as long as t ∩ is not more specific than the obligat {γ γ γ 1 (s)} (resp. the prohibet {γ γ γ 0 (s)}) of s. If that happens, the ASlot becomes obligatory (resp. prohibited). Moreover, the signature of an ASlot may only become more specific. Hierarchy of Circumstantial Symbols Unit types specify how unit nodes are linked to other unit nodes in the UGs. As for any slot in a predicate, one ASlot of a unit may be filled by only one unit at a time. Now, one may also encounter dependencies of another type in some dependency structures: circumstantial dependencies (Mel 'čuk, 2004) . Circumstantial relations are considered of type instance-instance contrary to actantial relations. Example of such relations are the deep syntactic representation relations ATTR, COORD, AP-PEND of the MTT, but we may also define other such relations to represent the link between a lexical unit and its sense for instance. We thus introduce a finite set of so-called Circumstantial Symbols (CSymbols) S C which is a set of binary relation symbols. In order to classify S C in sets and subsets, we introduce a partial order C over S C . C is the reflexo-transitive closure of a set of asserted comparisons C S C ⊆ T 2 . 5 The powerset of X is the set of all subsets of X: 2 X Finally, to each CSymbol is assigned a signature that specifies the type of units that are linked through a relation having this symbol. The set of signatures of CSymbol {σ σ σ s } s∈S C is a set of couples of CUTs: {(domain(s), range(s))} s∈S C . As one goes down the hierarchy of PUTs, we impose that the signature of a CSymbol may only become more specific (eq. 7). s 1 s 2 ⇒ σ σ σ(s 1 ) ∩ σ σ σ(s 2 ) (7) We may hence introduce the hierarchy of CSymbols: Definition 2.2. The hierarchy of CSymbols, denoted C def = (S C , C S C , T , {σ σ σ s } s∈S C ), Definition of Unit Graphs (UGs) The UGs represent different types of dependency structures. Parallel with the Conceptual Graphs, UGs are defined over a so-called support. Definition 2.3. A UGs support is denoted S def = (T , C, M) and is composed of a hierarchy of unit types T , a hierarchy of circumstantial symbols C, and a set of unit identifiers M. Every element of M identifies a specific unit, but multiple elements of M may identify the same unit. In a UG, unit nodes that are typed and marked are interlinked by dependency relations that are either actantial or circumstantial. Definition 2.4. A UG G defined over a UGsupport S is a tuple denoted G def = (U, l, A, C, Eq) where U is the set of unit nodes, l is a labelling mapping over U , A and C are respectively actantial and circumstantial triples, and Eq is a set of asserted unit node equivalences. Let us detail the components of G. U is the set of unit nodes. Every unit node represents a specific unit, but multiple unit nodes may represent the same unit. Unit nodes are typed and marked so as to respectively specify what CUT they have and what unit they represent. The marker of a unit node is a set of unit identifiers for mathematical reasons. The set of unit node markers is denoted M ∩ and is the powerset 5 of M. If a unit node is marked by ∅, it is said to be generic, and the represented unit is unknown. On the other hand, if a unit node is marked {m 1 , m 2 }, then the unit identifiers m 1 and m 2 actually identify the same unit. l is thus a labelling mapping over U that assigns to each unit node u ∈ U a couple l(u) = (t ∩ , m ∩ ) ∈ T ∩ × M ∩ of a CUT and a unit node marker. We denote t ∩ = type(u) and m ∩ = marker(u). A is the set of actantial triples (u, s, v) ∈ U × S T × U . For all a = (u, s, v) ∈ A, the unit represented by v fills the ASlot s of the unit represented by u. We denote u = governor(a), s = symbol(a) and v = actant(a). We also denote arc(a) = (u, v). C is the set of circumstantial triples (u, s, v) ∈ U × S C × U . For all c = (u, s, v) ∈ C, the unit represented by u governs the unit represented by v with respect to s. We denote u = governor(c), s = symbol(c) and v = circumstantial(c). We also denote arc(c) = (u, v). Eq ⊆ U 2 is the set of so-called asserted unit node equivalences. For all (u 1 , u 2 ) ∈ U 2 , (u 1 , u 2 ) ∈ Eq means that u 1 and u 2 represent the same unit. The Eq relation is not an equivalence relation over unit nodes 6 . We thus distinguish explicit and implicit knowledge. UGs so defined are the core dependency structures of the UGs mathematical framework. On top of these basic structures, one may define for instance rules and lexicographic definitions. Due to space limitation we will not introduce such advanced aspects of the UGs formalism, and we will provide a model to UGs defined over a support that does not contain definitions of PUTs. 3 Model Semantic for UGs Model of a Support In this section we will provide the UGs framework with a model semantic based on a relational algebra. Let us first introduce the definition of the model of a support. Definition 3.1 (Model of a support). Let S = (T , C, M) be a support. A model of S is a couple M = (D, δ). D is a set called the domain of M that contains a special element denoted • that represents nothing, plus at least one other element. δ is denoted the interpretation function and must be such that: • M is a model of T ; • M is a model of C; 6 An equivalent relation is a reflexive, symmetric, and transitive relation. • ∀m ∈ M, δ(m) ∈ D \ •; This definition requires the notion of model of a unit types hierarchy, and model of a CSymbols hierarchy. We will sequentially introduce these notions in the following sections. Model of a Hierarchy of Unit Types The interpretation function δ associates with any PUT t ∈ T a relation δ({t}) of arity 1 + valency(t) with the following set of attributes (eq. 8): • a primary attribute denoted 0 (0 / ∈ S T ) that provides {t} with the semantics of a class; • an attribute for each of its ASlot in α α α(t) that provides {t} with the dual semantics of a relation. ∀t ∈ T, δ({t}) ⊆ D 1+valency(t) with attributes {0} ∪ α α α(t) (8) Every tuple r of δ({t}) can be identified to a mapping, still denoted r, from the attribute set {0} ∪ α α α(t) to the universe D. r describes how a unit of type {t} is linked to its actants. r(0) is the unit itself, and for all s ∈ α α α(t), r(s) is the unit that fills ASlot s of r(0). If r(s) = •, then there is no unit that fills ASlot s of r(0). A given unit may be described at most once in δ({t}), so 0 is a unique key in the interpretation of every PUT: ∀t ∈ T, ∀r 1 , r 2 ∈ δ({t}), r 1 (0) = r 2 (0) ⇒ r 1 = r 2 (9) must be the type of every unit, except for the special nothing element •, and ⊥ must be the type of no unit. As the projection π 0 δ({t}) on the main attribute 0 represents the set of units having type {t}, equations 10 and 11 model these restrictions. π 0 δ({ }) = D \ •; (10) δ({⊥}) = ∅ (11) The ASlot s of the obligat γ γ γ 1 (s) must be filled by some unit, but no unit may fill ASlot s of the prohibet γ γ γ 0 (s). As for every s ∈ α α α(t), the projection π s δ({t}) represents the set of units that fill the ASlot s of some unit that has type t, equations 12 and 13 model these restrictions. ∀s ∈ S T , • / ∈ π s δ({γ γ γ 1 (s)}); (12) ∀s ∈ S T , π s δ({γ γ γ 0 (s)}) = {•}; (13) Now if a unit i ∈ D is of type {t 1 } and t 1 is more specific than t 2 , then the unit is also of type {t 2 }, and the description of i in δ({t 2 }) must correspond to the description of i in δ({t 1 }). Equivalently, the projection of δ({t 1 }) on the attributes of δ({t 2 }) must be a sub-relation of δ({t 2 }): ∀t 1 t 2 , π {0}∪α α α(t 2 ) δ({t 1 }) ⊆ δ({t 2 }) (14) The interpretation of a CUT is the join of the interpretation of its constituting PUTs, except for ∅ which has the same interpretation as { }, and asserted absurd CUTs t ∩ ∈ ⊥ A that contain no unit. ∀t ∩ ∈ T ∩ \ ∅ − ⊥ A , δ(t ∩ ) = t∈t ∩ δ({t}) (15) δ(∅) = δ({ }) (16) ∀t ∩ ∈ ⊥ A , δ(t ∩ ) = ∅ (17) Finally, for every unit of type {t} and for every ASlot of t, the unit that fills ASlot s must be either nothing, or a unit of type ς ς ς t (s): ∀t ∈ T, ∀s ∈ α α α(t), π s δ({t}) \ • ⊆ π 0 δ(ς ς ς t (s)) (18) We may now define the model of a unit type hierarchy. Definition 3.2. Let be a unit types hierarchy D, δ ) such that the interpretation function δ satisfies equations 8 to 18. T = (T D , S T , γ γ γ, γ γ γ 1 , γ γ γ 0 , C A , ⊥ A , {ς ς ς t } t∈T ). A model of T is a couple M = ( Model of a Hierarchy of Circumstantial Symbols So as to be also a model of a CSymbols hierarchy, the interpretation function δ must be extended and further restricted as follows. The interpretation function δ associates with every CSymbol s ∈ S C a binary relation δ(s) with two attributes : gov which stands for governor, and circ which stands for circumstantial. Parallel with binary relations in the semantic model of the CGs formalism, if a CSymbol s 1 is more specific than another CSymbol s 2 , then the interpretation of s 1 must be included in the interpretation of s 2 . ∀s 1 , s 2 ∈ S C , s 1 C s 2 ⇒ δ(s 1 ) ⊆ δ(s 2 ) (20) Finally, the type of the units that are linked through a CSymbol s must correspond to the signature of s. Model Satisfying a UG and Semantic Consequence Now that the model of a support is fully defined, we may define the model of a UG. A model of a UG is a model of the support on which it is defined, augmented with an assignment mapping over unit nodes that assigns to every unit node an element of D. Definition 3.4 (Model of a UG). Let G = (U, l, A, C, Eq) be a UG defined over a support S. A model of G is a triple (D, δ, β) where: • (D, δ) is a model of S; • β, called an assignment, is a mapping from U to D. So as to satisfy the UG, the assignment β must satisfy a set of requirements. First, if a unit node u ∈ U has a marker m ∈ marker(u), then the assignment of u must correspond to the interpretation of m. ∀u ∈ U, ∀m ∈ marker(u), β(u) = δ(m) (23) Then, the assignment of any unit node u must belong to the set of units that have type type(u). ∀u ∈ U, β(u) ∈ π 0 δ(type(u)) (24) For every actantial triple (u, s, v) ∈ A, and as {γ γ γ(s)} is the CUT that introduces a ASlot s, the interpretation δ({γ γ γ(s)}) must reflect the fact that the unit represented by v fills the actant slot s of the unit represented by u. ∀(u, s, v) ∈ A, π 0,s δ({γ γ γ(s)}) = {(β(u), β(v))} (25) Similarly, for every circumstantial triple (u, s, v) ∈ C, the interpretation of s must reveal the fact that the unit represented by v depends on the unit represented by u with respect to s. ∀(u, s, v) ∈ C, (β(u), β(v)) ∈ δ(s) (26) Finally, if two unit nodes are asserted to be equivalent, then the unit they represent are the same and their assignment must be the same. ∀(u 1 , u 2 ) ∈ Eq, β(u 1 ) = β(u 2 ) (27) We may now define the notion of satisfaction of a UG by a model. Conclusion We thus studied how to formalize, in a knowledge engineering perspective, the dependency structures and the valency-based predicates. We gave an overview of the foundational concepts of the new graph-based Unit Graphs KR formalism. The valency-based predicates are represented by unit types, and are described in a unit types hierarchy. Circumstantial relations are another kind of dependency relation that are described in a hierarchy, and along with a set of unit identifiers these two structures form a UGs support on which UGs may be defined. We then provided these foundational structures with a model, in the logical sense, using a relational algebra. We dealt with the problem of prohibited and optional actant slots by adding a special nothing element • in the domain of the model, and listed the different equations that the interpretation function must satisfy so that a model satisfies a UG. We finally introduced the notion of semantic consequence, which is a first step towards reasoning with dependency structure in the UGs framework. We identify three future directions of research. • We did not introduce the definition of PUTs that are to model lexicographic definitions in the ECD and shall be included to the support. The definition of the model semantics of the UGs shall be completed so as to take these into account. • A UG represents explicit knowledge that only partially define the interpretations of unit types, CSymbols, and unit identifiers. One need to define algorithms to complete the model, so as to check the entailment of a UG by another. • We know from ongoing works that such an algorithm may lead to an infinite domain. A condition over the unit types hierarchy must be found so as to ensure that the model is decidable for a finite UG.",10247031,c167d01bc826e954295b153102c23c0afd054ff6,3,https://aclanthology.org/R13-1050,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Lefran{\c{c}}ois, Maxime  and
Gandon, Fabien",The Unit Graphs Framework: Foundational Concepts and Semantic Consequence,389--395,,,,,,,inproceedings,lefrancois-gandon-2013-unit,,,"Semantics: Sentence-level Semantics, Textual Inference and Other areas"
25,W05-0825,"In this paper, we present a phrase extraction algorithm using a translation lexicon, a fertility model, and a simple distortion model. Except these models, we do not need explicit word alignments for phrase extraction. For each phrase pair (a block), a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs; a fertility score is computed to estimate how good the lengths are matched between phrase pairs; a center distortion score is computed to estimate the relative position divergence between the phrase pairs. We presented the results and our experience in the shared tasks on French-English.","In this paper, we present a phrase extraction algorithm using a translation lexicon, a fertility model, and a simple distortion model. Except these models, we do not need explicit word alignments for phrase extraction. For each phrase pair (a block), a bilingual lexicon based score is computed to estimate the translation quality between the source and target phrase pairs; a fertility score is computed to estimate how good the lengths are matched between phrase pairs; a center distortion score is computed to estimate the relative position divergence between the phrase pairs. We presented the results and our experience in the shared tasks on French-English. Introduction Phrase extraction becomes a key component in today's state-of-the-art statistical machine translation systems. With a longer context than unigram, phrase translation models have flexibilities of modelling local word-reordering, and are less sensitive to the errors made from preprocessing steps including word segmentations and tokenization. However, most of the phrase extraction algorithms rely on good word alignments. A widely practiced approach explained in details in (Koehn, 2004) , (Och and Ney, 2003) and (Tillmann, 2003) is to get word alignments from two directions: source to target and target to source; the intersection or union operation is applied to get refined word alignment with pre-designed heuristics fixing the unaligned words. With this refined word alignment, the phrase extraction for a given source phrase is essentially to extract the target candidate phrases in the target sentence by searching the left and right projected boundaries. In (Vogel et al., 2004) , they treat phrase alignment as a sentence splitting problem: given a source phrase, find the boundaries of the target phrase such that the overall sentence alignment lexicon probability is optimal. We generalize it in various ways, esp. by using a fertility model to get a better estimation of phrase lengths, and a phrase level distortion model. In our proposed algorithm, we do not need explicit word alignment for phrase extraction. Thereby it avoids the burden of testing and comparing different heuristics especially for some language specific ones. On the other hand, the algorithm has such flexibilities that one can incorporate word alignment and heuristics in several possible stages within this proposed framework to further improve the quality of phrase pairs. In this way, our proposed algorithm is more generalized than the usual word alignment based phrase extraction algorithms. The paper is structured as follows: in section 2, The concept of blocks is explained; in section 3, a dynamic programming approach is model the width of the block; in section 4, a simple center distortion of the block; in section 5, the lexicon model; the complete algorithm is in section 6; in section 7, our experience and results using the proposed approach. Blocks We consider each phrase pair as a block within a given parallel sentence pair, as shown in Figure 1 . The y-axis is the source sentence, indexed word by word from bottom to top; the x-axis is the target sentence, indexed word by word from left to right. The block is defined by the source phrase and its projection. The source phrase is bounded by the start and the end positions in the source sentence. The projection of the source phrase is defined as the left and right boundaries in the target sentence. Usually, the boundaries can be inferred according to word alignment as the left most and right most aligned positions from the words in the source phrase. In this paper, we provide another view of the block, which is defined by the centers of source and target phrases, and the width of the target phrase. Phrase extraction algorithms in general search for the left and right projected boundaries of each source phrase according to some score metric computed for the given parallel sentence pairs. We present here three models: a phrase level fertility model score for phrase pairs' length mismatch, a simple center-based distortion model score for the divergence of phrase pairs' relative positions, and a phrase level translation score to approximate the phrase pairs' translational equivalence. Given a source phrase, we can search for the best possible block with the highest combined scores from the three models. Length Model: Dynamic Programming Given the word fertility definitions in IBM Models (Brown et al., 1993) , we can compute a probability to predict phrase length: given the candidate target phrase (English) e I 1 , and a source phrase (French) of length J, the model gives the estimation of P (J|e I 1 ) via a dynamic programming algorithm using the source word fertilities. Figure 2 shows an example fertility trellis of an English trigram. Each edge between two nodes represents one English word e i . The arc between two nodes represents one candidate non-zero fertility for e i . The fertility of zero (i.e. generating a NULL word) corresponds to the direct edge between two nodes, and in this way, the NULL word is naturally incorporated into this model's representation. Each arc is A path φ I 1 through the trellis represents the number of French words φ i generated by each English word e i . Thus, the probability of generating J words from the English phrase along the Viterbi path is: P (J|e I 1 ) = max {φ I 1 ,J= I i=1 φ i } I i=1 P (φ i |e i ) (1) The Viterbi path is inferred via dynamic programming in the trellis of the lower panel in Figure 2 : φ[j, i] = max        φ[j, i − 1] + log P N U LL (0|e i ) φ[j − 1, i − 1] + log P φ (1|e i ) φ[j − 2, i − 1] + log P φ (2|e i ) φ[j − 3, i − 1] + log P φ (3|e i ) where P N U LL (0|e i ) is the probability of generating a NULL word from e i ; P φ (k = 1|e i ) is the usual word fertility model of generating one French word from the word e i ; φ[j, i] is the cost so far for generating j words from i English words e i 1 : e 1 , • • • , e i . After computing the cost of φ[J, I], we can trace back the Viterbi path, along which the probability P (J|e I 1 ) of generating J French words from the English phrase e I 1 as shown in Eqn. 1. With this phrase length model, for every candidate block, we can compute a phrase level fertility score to estimate to how good the phrase pairs are match in their lengthes. Distortion of Centers The centers of source and target phrases are both illustrated in Figure 1 . We compute a simple distortion score to estimate how far away the two centers are in a parallel sentence pair in a sense the block is close to the diagonal. In our algorithm, the source center f j+l j of the phrase f j+l j with length l + 1 is simply a normalized relative position defined as follows: f j+l j = 1 |F | j =j+l j =j j l + 1 (2) where |F | is the French sentence length. For the center of English phrase e i+k i in the target sentence, we first define the expected corresponding relative center for every French word f j using the lexicalized position score as follows: e i+k i (f j ) = 1 |E| • (i+k) i =i i • P (f j |e i ) (i+k) i =i P (f j |e i ) (3) where |E| is the English sentence length. P (f j |e i ) is the word translation lexicon estimated in IBM Models. i is the position index, which is weighted by the word level translation probabilities; the term of I i=1 P (f j |e i ) provides a normalization so that the expected center is within the range of target sentence length. The expected center for e i+k i is simply a average of e i+k i (f j ): e i+k i = 1 l + 1 j+l j =j e i+k i (f j ) (4) This is a general framework, and one can certainly plug in other kinds of score schemes or even word alignments to get better estimations. Given the estimated centers of Lexicon Model Similar to (Vogel et al., 2004) , we compute for each candidate block a score within a given sentence pair using a word level lexicon P (f |e) as follows: P (f j+l j |e i+k i ) = j ∈[j,j+l] i ∈[i,i+k] P (f j |e i ) k + 1 • j / ∈[j,j+l] i / ∈[i,i+k] P (f j |e i ) |E| − k − 1 6 Algorithm Our phrase extraction is described in Algorithm 1. The input parameters are essentially from IBM Model-4: the word level lexicon P (f |e), the English word level fertility P φ (φ e = k|e), and the center based distortion P ( e i+k i | f j+l j ). Overall, for each source phrase f j+l j , the algorithm first estimates its normalized relative center in the source sentence, its projected relative center in the target sentence. The scores of the phrase length, center-based distortion, and a lexicon based score are computed for each candidate block A local greedy search is carried out for the best scored phrase pair (f j+l j , e i+k i ). In our submitted system, we computed the following seven base scores for phrase pairs: P ef (f j+l j |e i+k i ), P f e (e i+k i |f j+l j ), sharing similar function form in Eqn. 5. P ef (f j+l j |e i+k i ) = j i P (f j |e i )P (e i |e i+k i ) = j i P (f j |e i ) k + 1 (5) We compute phrase level relative frequency in both directions: P rf (f j+l j |e i+k i ) and P rf (e i+k i |f j+l j ). We compute two other lexicon scores which were also used in (Vogel et al., 2004) : S 1 (f j+l j |e i+k i ) and S 2 (e i+k i |f j+l j ) using the similar function in Eqn. 6: S(f j+l j |e i+k i ) = j i P (f j |e i ) (6) In addition, we put the phrase level fertility score computed in section 3 via dynamic programming to be as one additional score for decoding. Algorithm 1 A Generalized Alignment-free Phrase Extraction score the phrase pair (f j+l j , e i+k i ), where score = P ( e | f )P (l|e i+k i )P (f j+l j |e i+k i ) 14: add top-n {(f j+l j , e i+k i )} into PhraseSet. Experimental Results Our system is based on the IBM Model-4 parameters. We train IBM Model 4 with a scheme of 1 7 2 0 h 7 3 0 4 3 using GIZA++ (Och and Ney, 2003) . The maximum fertility for an English word is 3. All the data is used as given, i.e. we do not have any preprocessing of the English-French data. The word alignment provided in the workshop is not used in our evaluations. The language model is provided by the workshop, and we do not use other language models. The French phrases up to 8-gram in the development and test sets are extracted with top-3 candidate English phrases. There are in total 2.6 million phrase pairs 1 extracted for both development set and the unseen test set. We did minimal tuning of the parameters in the pharaoh decoder (Koehn, 2004) settings, simply to balance the length penalty for Bleu score. Most of the weights are left as they are given: [ttable-limit]=20, [ttable-threshold]=0.01, 1 Our phrase table is to be released to public in this workshop 1 , setting s 1 was our submission without using the inverse relative frequency of P rf (e i+k i |f j+l j ). s 2 is using all the seven scores. Discussions In this paper, we propose a generalized phrase extraction algorithm towards word alignment-free utilizing the fertility model to predict the width of the block, a distortion model to predict how close the centers of source and target phrases are, and a lexicon model for translational equivalence. The algorithm is a general framework, in which one could plug in other scores and word alignment to get better results.",13293973,bb746564a712ee90541b6b5649aaa06fffa03dce,25,https://aclanthology.org/W05-0825,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Zhao, Bing  and
Vogel, Stephan",A Generalized Alignment-Free Phrase Extraction,141--144,,,,,,,inproceedings,zhao-vogel-2005-generalized,,,
26,L02-1318,,,35441707,193fd13989c20b2bb57ad670e617819bbd8f88de,13,http://www.lrec-conf.org/proceedings/lrec2002/pdf/318.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Koizumi, Atsuko  and
Sagawa, Hirohiko  and
Takeuchi, Masaru",An Annotated {J}apanese {S}ign {L}anguage Corpus,,,,,,,,inproceedings,koizumi-etal-2002-annotated,,,
27,W09-1315,"We introduce a controlled natural language for biomedical queries, called BIOQUERYCNL, and present an algorithm to convert a biomedical query in this language into a program in answer set programming (ASP)-a formal framework to automate reasoning about knowledge. BIOQUERYCNL allows users to express complex queries (possibly containing nested relative clauses and cardinality constraints) over biomedical ontologies; and such a transformation of BIOQUERYCNL queries into ASP programs is useful for automating reasoning about biomedical ontologies by means of ASP solvers. We precisely describe the grammar of BIOQUERYCNL, implement our transformation algorithm, and illustrate its applicability to biomedical queries by some examples.","We introduce a controlled natural language for biomedical queries, called BIOQUERYCNL, and present an algorithm to convert a biomedical query in this language into a program in answer set programming (ASP)-a formal framework to automate reasoning about knowledge. BIOQUERYCNL allows users to express complex queries (possibly containing nested relative clauses and cardinality constraints) over biomedical ontologies; and such a transformation of BIOQUERYCNL queries into ASP programs is useful for automating reasoning about biomedical ontologies by means of ASP solvers. We precisely describe the grammar of BIOQUERYCNL, implement our transformation algorithm, and illustrate its applicability to biomedical queries by some examples. Introduction The rapid increase in the popularity and usage of Web leads researchers to store data and make it publicly available in many ways. In particular, to facilitate access to its desired parts, it is stored in a structured form, like ontologies. These ontologies can be queried with an SQL-like formal query language. However, since these ontologies have been developed for and widely used by people that lacks the necessary knowledge in a formal query language, a simpler and more commonly known language is needed to represent queries. A natural language is the perfect answer, but ambiguities in its grammar and vocabulary make it difficult to automate reasoning about queries in natural language. Therefore, to represent queries, we consider a middle ground between these two options: a Controlled Natural Language (CNL). A CNL is a subset of a natural language, with a restricted grammar and vocabulary, that overcomes the ambiguity of natural languages. Since we consider queries in a specific domain, namely biomedicine, and over specific sources of information, namely biomedical ontologies, a CNL designed and developed for reasoning about biomedical ontologies is sufficient to represent biomedical queries. Essentially, a CNL is a formal language but with a look of a natural language. Therefore, compared to a natural language, a CNL can be easily converted to some other formalisms. This allows us to use automated reasoners, specifically developed for such formalisms, to find answers to queries expressed in a CNL. One such formalism is Answer Set Programming (ASP) (Baral, 2003) . ASP is a new knowledge representation and reasoning paradigm which supports representation of defaults, constraints, preferences, aggregates, etc., and provides technologies that allow us to automate reasoning with incomplete information, and to integrate other technologies, like description logics reasoners and Semantic Web technologies. For instance, in (Bodenreider et al., 2008) , the authors illustrate the applicability and effectiveness of using ASP to represent a rule layer that integrates relevant parts of some biomedical ontologies in RDF(S)/OWL, and to compute answers to some complex biomedical queries over these ontologies. Although CNLs are appropriate for expressing biomedical queries, and methods and technologies of ASP are appropriate for automated reasoning about biomedical ontologies, there is no algorithm to convert a CNL biomedical query into a program. In (Bodenreider et al., 2008) , biomedical queries are represented as programs in ASP; however, these programs are constructed manually. However, manually constructing ASP programs to represent biomedical queries is not only time consuming but also requires expertise in ASP. This prevents automating the whole process of computing an answer to a query, once it is given in a CNL. In this paper, we design and develop a CNL (called BIOQUERYCNL) for expressing biomedical queries over some ontologies, and introduce an algorithm to convert a biomedical query expressed in this CNL into a program in ASP. The idea is to automatically compute an answer to the query using methods of (Bodenreider et al., 2008) , once the user types the query. This idea is illustrated in Figure 1 . Similar approaches of using a CNL for querying ontologies have been investigated in various studies. For instance, (Bernstein et al., 2005) considers queries in the controlled natural language, Attempto Controlled English (ACE) (Attempto, 2008) , and transforms them into queries in PQL (Klein and Bernstein, 2004) to be evaluated by a query engine. (Bernstein et al., 2006) presents a system that guides the user to write a query in ACE, and translates the query into SPARQL to be evaluated by the reasoner of JENA (Jena, 2008) . On the other hand, (Kaufmann et al., 2006) transforms a given natural language query to a SPARQL query (using the Stan-ford Parser and WORDNET) to be evaluated by a reasoner like that of JENA. Our work is different from these studies in two ways: we consider queries over biomedical ontologies (thus different forms of queries, and vocabulary), and we transform a query into an ASP program to automate reasoning over a rule layer presented in ASP. Transformations of natural language sentences into ASP has been studied in (Baral et al., 2008) and (Baral et al., 2007) . In (Baral et al., 2008) , the authors introduce methods to transform some simple forms of sentences into ASP using Lambda Calculus. In (Baral et al., 2007) , the authors use C&C tools (CC, 2009) to parse the some forms of natural language input, and perform a semantic analysis over the parser output using BOXER (Boxer, 2009) , to do reasoning in ASP. Our work is different in that we consider a CNL to express queries, and introduce a different method for converting CNL to a program in ASP, via Discourse Representation Structures (DRS) (Kamp, 1981) . In the rest of the paper, first we briefly discuss ASP with some examples (Section 2). Then we define the grammatical structure of BIOQUERYCNL and give some examples (Section 3). Next, we introduce our algorithm for transforming a BIO-QUERYCNL query into an ASP program and explain it by an example (Section 4). We conclude with a discussion of challenges related to the implementation of our algorithm (Section 5) and other related problems that we are working on (Section 6). Answer Set Programming Answer Set Programming (ASP) (Lifschitz, 1999; Marek and Truszczyński, 1999; Niemelä, 1999; Baral, 2003) is a new knowledge representation and reasoning paradigm which supports representation of defaults, constraints, preferences, aggregates, etc., and provides technologies that allow us to automate reasoning with incomplete information, and to integrate other technologies, like description logics reasoners and Semantic Web technologies. In ASP, knowledge is represented as a ""program"" (a finite set of ""rules"") whose meaning is captured by its models (called ""answer sets"" (Gelfond and Lifschitz, 1988) ). Answer sets for a program can be computed by ""answer set solvers"" such as DLV (DLV, 2009) . Consider for instance the program: gene_gene(''ADRB1'',''CHRM5''). gene_gene(''CHRM1'',''CHRM5''). chain(X,Y) :-gene_gene(X,Y). chain(X,Y) :-gene_gene(Y,X). chain(X,Y) :-gene_gene(X,Z), chain(Z,Y). The first rule expresses that the gene ADRB1 interacts with the gene CHRM5. The second rule expresses that the gene CHRM1 interacts with the gene CHRM5. The third, the fourth, and the fifth rules express a chain of such interactions. In a rule containing :-, the left-hand-side of :-is called the head of the rule, the right-hand-side is called the body of the rule. Such a rule p :-q, r. is read as ""p if q and r"". Here the head atom is p, and the body atoms are q and r. The answer set for this program describes that there is a chain of interactions between CHRM1 and CHRM5, ADRB1 and CHRM5, and ADRB1 and CHRM1. As mentioned above, the language of ASP is expressive enough to represent defaults, constraints, preferences, aggregates, etc.. For instance, the rule treats_2diseases(R) :- #count{D:treats(R,D)}>=2, drug(R). describes drugs R that treat at least 2 diseases. A Controlled Natural Language for Biomedical Queries We introduce a controlled natural language, called BIOQUERYCNL, to express biomedical queries, whose grammar is shown in Table 1 . This grammar should be considered in connection with the given biomedical ontologies. The italic words in the grammar, for instance, represent the information extracted from the related ontologies. We call these italic words ontology functions; the detailed description of these functions are given in Table 2 . With BIOQUERYCNL, the users can ask simple queries, queries with nested relative clauses (with any number of conjunctions and disjunctions), and queries with cardinalities. Some sample queries are given below. (Q1) Which symptoms are alleviated by the drug Epinephrine? (Q2) What are the side-effects of the drugs that treat the disease Asthma? (Q3) What are the genes that are related to the disease Asthma and are targeted by the drug Epinephrine? (Q4) What are the symptoms of the diseases that are related to the gene ADRB1 or that are treated by the drug Epinephrine? (Q5) Which genes are targeted by at least 2 drugs and are related to at most 3 diseases? BIOQUERYCNL is a subset of Attempto Controlled English (ACE) (Attempto, 2008) , which can represent a wide range of queries (Fuchs et al., 2008) , specialized for biomedical ontologies. Converting Controlled Natural Language Queries to Programs We have implemented an algorithm, QUERY, presented in Algorithm 1, that obtains an ASP rule Head ← Body from a query Q expressed in BIO-QUERYCNL, via transforming Q into a DRS. We will explain the main steps of the QUERY algorithm by an example, considering query (Q4). (Kamp, 1981) -a variant of the first-order logic that is used for the dynamic interpretation of natural language and systematic translation of natural language into logical form -without any ambiguity, using tools like Attempto Parsing Engine (APE). APE converts ACE text to DRS by an approach similar to (Blackburn and Bos, 2005) , as explained in (Fuchs et al., 2008) . For instance, APE transforms query (Q4) into the following DRS: QUERY → YESNOQUERY | WHQUERY QUESTIONMARK YESNOQUERY → DODOESQUERY | ISAREQUERY WHQUERY → WHATQUERY | WHICHQUERY DODOESQUERY → [ Do | Does ] T ype() Instance(T ) PREDICATERELATION ISAREQUERY → [ Is | Are ] T ype() Instance(T ) V erb(T ) WHATQUERY → What BE T ype() that PREDICATERELATION WHATQUERY → What BE OFRELATION that PREDICATERELATION WHATQUERY → What BE OFRELATIONINSTANCE that PREDICATERELATION WHICHQUERY → Which T ype() PREDICATERELATION OFRELATION → N oun(T ) of T ype() OFRELATIONINSTANCE → N oun(T ) of T ype() Instance(T ) PREDICATERELATION → ACTIVERELATION (CONNECTOR (that)? PREDICATERELATION)* PREDICATERELATION → PASSIVERELATION (CONNECTOR (that)? PREDICATERELATION)* ACTIVERELATION → V erb(T, T ) T ype() Instance(T ) ACTIVERELATION → V erb(T, T ) GENERALISEDQUANTOR PositiveNumber T ype() PASSIVERELATION → BE V erb(T , T ) by T ype() Instance(T ) PASSIVERELATION → BE V erb(T , T ) by GENERALISEDQUANTOR PositiveNumber T ype() BE → is | are CONNECTOR → and | or GENERALISEDQUANTOR → at least | at most | more than | less than | exactly QUESTIONMARK → ? Table 2 : The Ontology Functions T ype() returns the type information the ontologies keep, ex. gene, disease, drug Instance(T ) returns instances of the type T , ex. Asthma for type disease V erb(T ) returns the verbs related to the type T , ex. approve for type drug V erb(T, T ) returns the verbs where type T is the subject and type T is the object, ex. drug treat disease N oun(T ) returns the nouns that are related to the type T , ex. symptom for type disease Note that the DRS consists of two kinds of expressions. The lines with a list of uppercase letters, like [E,F,G], describe the domain of the DRS; each uppercase letter is a referent. The rest of the DRS describe the conditions about the domain. The DRS above contains some predefined predicates, such as object, property, predicate, query, etc.. All the nouns, adjectives, verbs, modifiers, etc. are represented with one of them. For instance, • object describes objects and the relevant forms of nouns denoting them (like ""diseases"") • predicate describes relations that are pro-duced by different forms of verbs (like ""treated""), • relation describes relations that are produced by of-constructions (like ""symptoms of disease""), • query describes the form of the query and the objects that the query is referring to. Ontologies represent relations between concepts. A rule layer over ontologies introduce further concepts integrating them. ASP takes into account relevant concepts and relations to answer a given query about these ontologies. In the biomedical queries we consider, the concepts and instances are represented with object and the relations between these concepts are represented with predicate and relation. The query is also important in terms of the type of information the user asks for. Constructing the Head and the Body Atoms Once the corresponding DRS is obtained from a given BIOQUERYCNL query, the head and the body atoms are constructed by analyzing the conditions in the DRS, as described in Algorithms 2 and 3. The HEAD algorithm is about the query predicate, which refers to objects or relations that are asked for in the given query. By following the referents, starting from the one mentioned in query, the algorithm finds out the type of the information that is asked for in the given query. Consider, for instance, query (Q4). The referent mentioned in query(A,what) is A. It is mentioned in predicate(B,be,A,C)-1, and here it denotes an object with referent C. Now let's find where C is mentioned: in object(C,symptoms,countable,na,eq,1)-1 to denote symptoms. Therefore, the query asks for symptoms. Based on this information, Algorithm 2 returns the head of the ASP rules as follows: what_be_symptoms(SYM1) The BODY algorithm analyzes the predicate and the relation predicates. These two predicates describe relations between objects described by the object predicates. The algorithm starts from the predicate and the relation predicates, and then, by following the referents, it returns the body atoms of the ASP rule. For instance, Algorithm 3 returns the following body atoms for query (Q4): symptoms_of_diseases(symptom_SYM1, disease_DIS1) diseases_be_related_to_gene(disease_DIS1, gene_''ADRB1'') drug_treated_diseases(drug_''Epinephrine'', disease_DIS1) These body atoms are given to POSTPROCESSING step, to produce bodies of the ASP rules. Constructing the ASP Rules POSTPROCESSING is the last step of the QUERY algorithm. At this step, first the number of rules is determined, and then the body atoms are placed in the bodies of these rules. In ASP, a conjunctive query can be represented by a rule. However, disjunctive queries are represented by several rules with same head but different bodies. For instance, query (Q4) is a disjunctive query (a disjunction of two queries), so there will be two rules representing this query: what_be_symptoms(SYM1) :- symptoms_of_diseases(symptom_SYM1, disease_DIS1), diseases_be_related_to_gene(disease_DIS1, gene_''ADRB1''). what_be_symptoms(SYM1) :-drug_treated_diseases(drug_''Epinephrine'', disease_DIS1), symptoms_of_diseases(symptom_SYM1, disease_DIS1). Next, the predicate names in the bodies of these rules are matched with the names of the already defined predicates in ontologies or in the rule layer over these ontologies. After matching the predicate names, the parameters of the predicates may have to be reordered. The matching of the predicates very much depends on the user interface (UI). If UI enforces users to use a specific grammar and lexicon while forming the query, then the matching can be done with an easy table look-up method. If the UI allows more flexibility of constructing a query, then the matching algorithm should use some basic Natural Language Processing methods and similarity metrics to find the most probable matching. After matching the predicates, the ordering of the parameters can be done easily. The Object := REFERSTO(Ref ) // e.g., A refers to ""genes"" GEN E1 7: Head := CONCAT(QuestionWord, Predicate, Object, Ref ) // e.g., what be genes(GEN E1) 8: end if 9: return Head returns the body predicates with the parameters. In these parameters, the type and the instance names are kept together. Thus, ordering of those parameters are done just by using the type information. After the ordering is done, the type information part is removed from the parameters. For instance, after matching the predicates, we get the following ASP rule for query (Q4). With an ASP rule layer over ontologies, and this ASP program, an ASP solver, like DLVHEX (DLVHEX, 2009) , returns an answer to query (Q4). For instance, consider the ASP rule layer, and the gene, disease, drug ontologies of (Bodenreider et al., 2008) . The ontologies of (Bodenreider et al., 2008) are obtained from the ontologies PHAR-MGKB (PharmGKB, 2008) , UNIPROT (UniProt, 2008) , GENE ONTOLOGY (GO) (GeneOntology, 2008) , GENENETWORK database (GeneNetwork, 2008) , DRUGBANK (DrugBank, 2008) , and the Medical Symptoms and Signs of Disease web page (MedicalSymptomsSignsDisease, 2008) . With this rule layer and the ontologies, and the ASP program above, the following is a part of the answer DLVHEX finds to the query above: noisy breathing faster breathing shortness of breath coughing chest tightness wheezing Another Example The algorithm discussed above returns the following ASP program for query (Q5): which_genes(GN1) :-2<=#count{DRG1:drug_gene(DRG1,GN1)}, #count{DIS1:disease_gene(DIS1,GN1)}<=3. Since query (Q5) contains cardinality constraints, the ASP program uses the aggregate #count. More examples of biomedical queries, and the ASP programs generated by our program can be seen at http://people.sabanciuniv.edu/ esraerdem/bioquery-asp/bionlp09/ . Implementational Issues We have implemented the algorithms explained above in PERL. We have used Attempto Parsing Engine APE to convert a given BIOQUERYCNL query into a DRS. Since BIOQUERYCNL is about biomedical ontologies, we provided APE some information about biomedical concepts, such as gene, drug, and words that represent relations between these concepts such as treat, target etc.. However, providing such information is not sufficient to convert all BIOQUERYCNL biomedical queries into programs, mainly due to specific instances of these concepts (consider, for instance, various drug names that appear in ontologies). One way to deal with this problem is to extract from the ontologies all instances of each concept and provide them to APE as an additional lexicon. This may not be the perfect solution since this process has to be repeated when an instance is added to the ontology. An alternative way can be enforcing the user to enter Subject := REFERSTO(SubRef ) // e.g., A refers to ""genes"" GEN E1 // e.g., genes targeted by drugs(GEN E1, DRG1) 13: end for 14: for each relation R do // e.g., symptoms of diseases(SY M 1, DIS1) 20: end for 21: return Body the concept name just before the instance (like ""the drug Epinephrine"") in the query. This is how we deal with instance names, in the current version of our implementations. However, such BIOQUERYCNL queries are not in the language of APE; so, with some preprocessing, we rewrite these queries in the correct syntax for APE. Conclusion We have designed and developed a Controlled Natural Language (CNL), called BIOQUERYCNL, to represent biomedical queries over some ontologies, and provided a precise description of its grammatical structure. We have introduced an algorithm to convert queries in BIOQUERYCNL to a program in Answer Set Programming (ASP). The idea is to compute answers to these queries automatically, by means of automated reasoners in ASP, over biomedical ontologies in RDF(S)/OWL and a rule layer in ASP integrating these ontologies. Our algorithm can handle various forms of simple/complex disjunc-tive/conjunctive queries that may contain (nested) relative clauses and cardinality constraints. We have implemented this algorithm in PERL, and tried it with the ASP rule layer, and the ontologies of (Bodenreider et al., 2008) . One essential part of the overall system is an intelligent user interface that allows a user to enter biomedical queries in BIOQUERYCNL. Design and implementation of such a user-interface is a part of our ongoing work. Acknowledgments Thanks to Tobias Kuhn for his help with ACE. This work is supported by the Scientific and Technological Research Council of Turkey (TUBITAK) grant 108E229.",14985007,8c3a4fc75d23259d5bd413d0a632713a39ed39c5,24,https://aclanthology.org/W09-1315,Association for Computational Linguistics,"Boulder, Colorado",2009,June,Proceedings of the {B}io{NLP} 2009 Workshop,"Erdem, Esra  and
Yeniterzi, Reyyan",Transforming Controlled Natural Language Biomedical Queries into Answer Set Programs,117--124,,,,,,,inproceedings,erdem-yeniterzi-2009-transforming,,,
28,L02-1308,,,12788925,fc69fee57012a1d01da5db12534b7d9380c9f88b,2,http://www.lrec-conf.org/proceedings/lrec2002/pdf/308.pdf,European Language Resources Association (ELRA),"Las Palmas, Canary Islands - Spain",2002,May,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),"Di Eugenio, Barbara  and
Glass, Michael  and
Scott, Michael J.","The binomial cumulative distribution function, or, is my system better than yours?",,,,,,,,inproceedings,di-eugenio-etal-2002-binomial,,,
29,R13-1051,"Information extraction systems automatically extract structured information from machine-readable documents, such as newswire, web, and multimedia. Despite significant improvement, the performance is far from perfect. Hence, it is useful to accurately estimate confidence in the correctness of the extracted information. Using the Knowledge Base Population Slot Filling task as a case study, we propose a confidence estimation model based on the Maximum Entropy framework, obtaining an average precision of 83.5%, Pearson coefficient of 54.2%, and 2.3% absolute improvement in F-measure score through a weighted voting strategy.","Information extraction systems automatically extract structured information from machine-readable documents, such as newswire, web, and multimedia. Despite significant improvement, the performance is far from perfect. Hence, it is useful to accurately estimate confidence in the correctness of the extracted information. Using the Knowledge Base Population Slot Filling task as a case study, we propose a confidence estimation model based on the Maximum Entropy framework, obtaining an average precision of 83.5%, Pearson coefficient of 54.2%, and 2.3% absolute improvement in F-measure score through a weighted voting strategy. Introduction Despite significant progress in recent years, Information Extraction (IE) technologies are still far from completely reliable. Errors result from the fact that language itself is ambiguous as well as methodological and technical limitations (Gandrabur et al., 2006) . Therefore, evaluating the probability that the extracted information is correct can contribute to improve IE system performance. Confidence Estimation (CE) is a generic machine learning rescoring approach for measuring the probability of correctness of the outputs, and usually adds a layer on top of the baseline system to analyze the outputs using additional information or models (Gandrabur et al., 2006) . There is previous work in IE using probabilistic and heuristic methods to estimate confidence for extracting fields using a sequential model, but to the best of our knowledge, this work is the first probabilistic CE model for the multi-stage systems employed for the Knowledge Base Population (KBP) Slot Filling task (Section 2). The goal of Slot Filling (SF) is to collect information from a corpus of news and web documents to determine a set of predefined attributes (""slots"") for given person and organization entities (Ji et al., 2011a ) (Section 3). Many existing methodologies have been used to address the SF task, such as Distant Supervision (Min et al., 2012) and Question Answering (Chen et al., 2010) , and each method has its own strengths and weaknesses. Many current KBP SF systems actually consist of several independent SF pipelines. The system combines intermediate responses generated from different pipelines into final slot fills. Since these intermediate outputs may be highly redundant, if confidence values can be associated, it will definitely help re-ranking and aggregation. For this purpose, we require comparable confidence values from disparate machine learning models or different slot filling strategies. Robust probabilistic machine learning models are capable of accurate confidence estimation because of their intelligent handling of uncertainty information. In this paper, we use the Maximum Entropy (MaxEnt) framework (Berger et al., 1996) to automatically predict the correctness of KBP SF intermediate responses (Section 4). Results achieve an average precision of 83.5%, Pearson's r of 54.2%, and 2.3% absolute improvement in final F-measure score through a weighted voting system (Section 5). Related Work Confidence estimation is a generic machine learning approach for measuring confidence of a given output, and many different CE methods have been used extensively in various Natural Language Processing (NLP) fields (Gandrabur et al., 2006) . Gandrabur and Foster (2003) and Nguyen et al. (2011) investigated the use of machine learning approaches for confidence estimation in machine translation. Agichtein (2006) showed Expectation-Maximization algorithms to estimate the confidence for partially supervised relation extraction. White et al. (2007) described how a maximum entropy model can be used to generate confidence scores for a speech recognition engine. Louis and Nenkova (2009) presented a study of predicting the confidence of automatic summarization outputs. Many approaches for confidence estimation have also been explored and implemented in other NLP research areas. There are also many previous confidence estimation studies in IE, and most of these have been in the Active Learning literature. Thompson et al. (1999) proposed a rule-based extraction method to compute confidence. Scheffer et al. (2001) utilized hidden Markov models to measure the confidence in an IE system, but they only estimated the confidence of singleton tokens. Culotta and McCallum (2004) 's work is the most relevant to our work, since they also utilized a machine learning model to estimate the confidence values for IE outputs. They estimated the confidence of both extracted fields and entire multi-field records mainly through a linear-chain Conditional Random Field (CRF) model, but their case studies are not as complicated and challenging as slot filling, since SF systems need to handle difficult crossdocument coreference resolution, sophisticated inference, and also other challenges (Min and Grishman, 2012) . Furthermore, to the best of our knowledge, there is no previous work in confidence estimation for the KBP slot filling task. KBP Slot Filling Task Definition The Knowledge Base Population (KBP) track, organized by U.S. National Institute of Standards and Technology (NIST)'s Text Analysis Conference (TAC), aims to promote research in discovering information about entities and augmenting a Knowledge Base (KB) with this information (Ji et al., 2010) . KBP mainly consists of two tasks: Entity Linking, linking names in a provided document to entities in the KB or NIL; and Slot Filling (SF), extracting information about an entity in the KB to automatically populate a new or existing KB. As a new but influential IE evaluation, Slot Filling is a challenging and practical task (Min and Grishman, 2012) . The Slot Filling task at KBP2012 provides a large collection of 3.7 million newswire articles and web texts as the source corpus, and an initial KB derived from the Wikipedia infoboxes. In such a large corpus, some information can be highly redundant. Given a list of person (PER) and organization (ORG) entity names (""queries""), SF systems retrieve the documents about these entities in the corpus and then fill the required slots with correct, non-redundant values. Each query consists of the name of the entity, its type (PER or ORG), a document (from the corpus) in which the name appears, its node ID if the entity appears in the provided KB, and the slots which need not be filled. Along with each slot fill, the system should also provide the ID of the document that justifies this fill. If the system does not extract any information for a given slot, the system just outputs ""NIL"" without any document ID. The task defines a total of 42 slots, 26 for person entities and 16 for organization entities. Some slots are single-valued, like ""per:date of birth"", which can only accept at most a single value, while the other slots, for example ""org:subsidiaries"", are list-valued, which can take a list of values. Since the overall goal is to augment an existing KB, the redundancy in list-valued slots must be detected and avoided, requiring a system to identify different but equivalent strings. Such as, both ""United States"" and ""U.S."" refer to the same country. More information can be found in the task definition (Ji et al., 2010) . Baseline System Description We use a slot filling system that has achieved highly competitive results (ranked top 2) at the KBP2012 evaluation as our baseline. Like most SF systems, our system has three basic components: Document Retrieval, Answer Extraction, and Response Combination. Our SF system starts by retrieving relevant documents based on a match to the query name or the results of query expansion. where the best answer is selected for each singlevalued slot and non-redundant fills are generated for list-valued slots. More details about our KBP Slot Filling system can be found in the system description paper (Min et al., 2012) . Confidence Estimation Model Experiments We have collected and merged the previous three years' KBP SF evaluation data, which consists of a total of 280 queries, and Table 1 lists the number of person and organization queries as well as the number of intermediate responses from each year. There are in total 31878 intermediate responses generated by 6 different pipelines from our SF system. We trained our CE model and measured the confidence values through a 10-fold crossvalidation, so that each fold randomly contains 14 person queries and 14 organization queries with their associated intermediate responses. Then for each iteration, the CE model is trained on 9 folds and approximates the confidence values in the remaining fold, and it assigns the probability of each intermediate response being correct as confidence. Voting Systems To evaluate the reliability of confidence values generated by this model, we used the weighted voting method to investigate the relationship between the confidence values and the performance. Baseline Voting System Our baseline SF system applies a basic plurality voting to combine all intermediate responses to generate the final response submission. This voting system simply counts the frequencies of each response entity, which is a unique response tuple in the form <Query ID, Slot Name, Response Fill>. For a single-valued slot of a query, the response with the highest count is returned as the final response fill. For the list-valued slots, all non-redundant responses are returned as the final response fills. In this basic voting system, each intermediate response contributes equally. Weighted Voting System Weighted voting is based on the idea that not all the voters contribute equally. Instead, voters have different weights concerning the outcome of an election. In our experiment, voters are all of intermediate responses generated by all pipelines, and the voters' weights are their confidence values. We set a threshold τ in this weighted voting system, where those intermediate responses with Category Feature Description Response Features slot name The slot name slot response length The conjunction of the length of R and the slot name name response slot The slot requires a name as the response Pipeline Features pipeline name The name of pipeline which generates R pipeline precision The Precision of the pipeline which generates R pipeline recall The Recall of the pipeline which generates R pipeline fmeasure The F-measure of the pipeline which generates R Local Features sent contain QR S contains both original Q and R sent contain ExQR S contains both co-referred Q or expanded Q and R dpath length The length of shortest dependency path between Q and R in S shortest dpath The shortest dependency path between Q and R in S NE boolean R is a person or organization name in S NE margin The difference between the log probabilities of this name R and the second most likely name n-gram Tri-gram context window associated with part-of-speech tags containing Q or R genre The supporting document is a newswire or web document Global Features query doc num The number of documents retrieved by Q response doc num The number of documents retrieved by R co-occur doc num The number of documents retrieved by the co-occurrences of Q and R cond prob givenQ The conditional probability of R given Q cond prob givenR The conditional probability of Q given R mutual info The Point-wise Mutual Information (PMI) of Q and R Table 2 : Features of Confidence Estimation Model confidences that are lower than τ would be eliminated. For each response entity, this weighted voting system simply sums all the weights of the intermediate responses that support this response entity as its weight. Then for a single-valued slot of a query, it returns the response with the highest weight as the final slot fill, while it returns all nonredundant responses as the final slot fills for the list-valued slots. The maximum confidence ψ of supporting intermediate responses is used as the final confidence for that slot fill. We also set a threshold η (optimized on a validation data set), where the final slot fills with confidence ψ lower than η would not be submitted finally. Results Table 3 compares the results of this weighted voting system (with τ = 0, η = 0.17) and the baseline voting system, where the responses were judged based only on the answer string, ignoring the document ID. As we can see, the weighted voting system achieves 2.3% absolute improvement in F-measure over the baseline, at a 99.8% confi- Figure 1 summarizes the results of this weighted voting system with different threshold τ settings. When τ is raised, Precision continuously increases to around 1, while Recall gradually decreases to 0. In addition to improving overall performance, the confidence estimates can be used to convey to the user of slot filling output our confidence in individual slot fills. After the intermediate responses are combined by the above weighted voting system (setting τ and η as 0), we divide the range of confidence values (0 to 1) into 10 equal intervals (0 to 0.1, 0.1 to 0.2, and so on) and categorize these Table 4 shows the Pearson's r and average precision results for all intermediate responses, where RANKED ranks the responses based on their confidence values; RANDOM assigns confidence values uniformly at random between 0 and 1; WORSTCASE ranks all incorrect responses above all correct ones. Applying the features separately, we find that slot response length and response doc num are the best predictors of correctness. dpath length (the length of the shortest dependency path between query and response) is also a significant contributor. Among the features, only NE margin seeks to directly estimate the confidence of a pipeline component, and it makes only a minimal contribution to the result. Overall this shows that confidence can be predicted quite well from features of the query and response, their appearance in the corpus, and prior IE system performance, without modeling the confidence of individual pipeline components. Conclusion We have presented our Maximum Entropy based confidence estimation model for information extraction systems. The effectiveness of this model has been demonstrated in the challenging Knowledge Base Population Slot Filling task, where a weighted voting system achieves 2.3% absolute improvement in F-measure score based on the confidence estimates. A strong correlation between the confidence estimates in KBP slot fills and the correctness has also been proved by obtaining an average precision of 83.5% and Pearson's r of 54.2%. In the future, further experiments are planned to investigate more elaborate models, explore more interesting feature sets, and study the contribution of each feature through a more detailed and thorough analysis.",11868924,66b33c252aca322d12526c14c56d49af494be455,7,https://aclanthology.org/R13-1051,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Li, Xiang  and
Grishman, Ralph",Confidence Estimation for Knowledge Base Population,396--401,,,,,,,inproceedings,li-grishman-2013-confidence,,,Information Extraction
30,W05-0826,"We describe the Spanish-to-English LDV-COMBO system for the Shared Task 2: ""Exploiting Parallel Texts for Statistical Machine Translation"" of the ACL-2005 Workshop on ""Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond"". Our approach explores the possibility of working with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on Word-Net which is used for unknown words.","We describe the Spanish-to-English LDV-COMBO system for the Shared Task 2: ""Exploiting Parallel Texts for Statistical Machine Translation"" of the ACL-2005 Workshop on ""Building and Using Parallel Texts: Data-Driven Machine Translation and Beyond"". Our approach explores the possibility of working with alignments at different levels of abstraction, using different degrees of linguistic annotation. Several phrase-based translation models are built out from these alignments. Their combination significatively outperforms any of them in isolation. Moreover, we have built a wordbased translation model based on Word-Net which is used for unknown words. Introduction The main motivation behind our work is to introduce linguistic information, other than lexical units, to the process of building word and phrase alignments. Many other authors have tried to do so. See (Och and Ney, 2000) , (Yamada and Knight, 2001) , (Koehn and Knight, 2002) , (Koehn et al., 2003) , (Schafer and Yarowsky, 2003) and (Gildea, 2003) . Far from full syntactic complexity, we suggest to go back to the simpler alignment methods first described by (Brown et al., 1993) . Our approach exploits the possibility of working with alignments at two different levels of granularity, lexical (words) and shallow parsing (chunks). In order to avoid confusion so forth we will talk about tokens instead of words as the minimal alignment unit. Apart from redefining the scope of the alignment unit, we may use different degrees of linguistic annotation. We introduce the general concept of data view, which is defined as any possible representation of the information contained in a bitext. We enrich data view tokens with features further than lexical such as PoS, lemma, and chunk label. As an example of the applicability of data views, suppose the case of the word 'plays' being seen in the training data acting as a verb. Representing this information as 'plays V BZ ' would allow us to distinguish it from its homograph 'plays N N S ' for 'plays' as a noun. Ideally, one would wish to have still deeper information, moving through syntax onto semantics, such as word senses. Therefore, it would be possible to distinguish for instance between two realizations of 'plays' with different meanings: 'he P RP plays V BG guitar N N ' and 'he P RP plays V BG basketball N N '. Of course, there is a natural trade-off between the use of data views and data sparsity. Fortunately, we hava data enough so that statistical parameter estimation remains reliable. System Description The LDV-COMBO system follows the SMT architecture suggested by the workshop organizers. First, training data are linguistically annotated for the two languages involved (See subsection 2.1). 10 different data views have been built. Notice that it is not necessary that the two parallel counterparts of a bitext share the same data view, as long as they share the same granularity. However, in all our experiments we have annotated both sides with the same linguistic information. See token descriptions: (W) word, (WL) word and lemma, (WP) word and PoS, (WC) word and chunk label, (WPC) word, PoS and chunk label, (Cw) chunk of words (Cwl), chunk of words and lemmas, (Cwp) chunk of words and PoS (Cwc) chunk of words and chunk labels (Cwpc) chunk of words, PoS and chunk labels. By chunk label we refer to the IOB label associated to every word inside a chunk, e.g. 'I B−N P declare B−V P resumed I−V P the B−N P session I−N P of B−P P the B−N P European I−N P Parliament I−N P . O '). We build chunk tokens by explicitly connecting words in the same chunk, e.g. '(I) N P (declare resumed) V P (the session) N P (of) P P (the European Parliament) N P '. See examples of some of these data views in Table 1 . Then, running GIZA++, we obtain token alignments for each of the data views. Combined phrasebased translation models are built on top of the Viterbi alignments output by GIZA++. See details in subsection 2.2. Combo-models must be then postprocessed in order to remove the additional linguistic annotation and split chunks back into words, so they fit the format required by Pharaoh. Moreover, we have used the Multilingual Central Repository (MCR), a multilingual lexical-semantic database (Atserias et al., 2004) , to build a wordbased translation model. We back-off to this model in the case of unknown words, with the goal of improving system recall. See subsection 2.3. Data Representation In order to achieve robustness the same tools have been used to linguistically annotate both languages. The SVMTool 1 has been used for PoS-tagging (Giménez and Màrquez, 2004) . The Freeling 2 package (Carreras et al., 2004) has been used for lemmatizing. Finally, the Phreco software by (Carreras et al., 2005) has been used for shallow parsing. No additional tokenization or pre-processing steps other than case lowering have been performed. Special treatment of named entities, dates, numbers, 1 The SVMTool may be freely downloaded at http://www.lsi.upc.es/˜nlp/SVMTool/ . 2 Freeling Suite of Language Analyzers may be downloaded at http://www.lsi.upc.es/˜nlp/freeling/ currency, etc., should be considered so as to further enhance the system. Building Combined Translation Models Because data views capture different, possibly complementary, aspects of the translation process it seems reasonable to combine them. We consider two different ways of building such combo-models: LPHEX Local phrase extraction. To build a separate phrase-based translation model for each data view alignment, and then combine them. There are two ways of combining translation models: MRG Merging translation models. We work on a weighted linear interpolation of models. These weights may be tuned, although a uniform weight selection yields good results. Additionally, phrase-pairs may be filtered out by setting a score threshold. noMRG Passing translation models directly to the Pharaoh decoder. However, we encountered many problems with phrasepairs that were not seen in all single models. This obliged us to apply arbitrary smoothing values to score these pairs. GPHEX Global phrase extraction. To build a single phrased-based translation model from the union of alignments from several data views. In its turn, any MRG operation performed on a combo-model results again in a valid combo-model. In any case, phrase extraction 3 is performed as depicted by (Och, 2002) . Using the MCR Outer knowledge may be supplied to the Pharaoh decoder by annotating the input with alternative translation options via XML-markup. We enrich every unknown word by looking up every possible translation for all of its senses in the MCR. These are scored by relative frequency according to the number of senses that lexicalized in the same manner. Let w f , p f be the source word and PoS, and w e be the target word, we define a function  Scount(w f , p f , w e ) which counts the number of senses for (w f , p f ) which can lexicalize as w e . A translation pair is scored as: score(w f , p f |w e ) = Scount(w f , p f , w e ) (w f ,p f ) Scount(w f , p f , w e ) (1) Better results would be expected working with word sense disambiguated text. We are not at this point yet. A first approach could be to work with the most frequent sense heuristic. Experimental Results Data and Evaluation Metrics We have used the data sets and language model provided by the organization. No extra training or development data were used in our experiments. We evaluate results with 3 different metrics: GTM F 1 -measure (e = 1, 2), BLEU score (n = 4) as provided by organizers, and NIST score (n = 5). Experimenting with Data Views Table 2 presents MT results for the 10 elementary data views devised in Section 2. Default parameters are used for λ tm , λ lm , and λ w . No tuning has been performed. As expected, word-based views obtain significatively higher results than chunk-based. All data views at the same level of granularity obtain comparable results. In is performed. We refer to the W model as our baseline. In this view, only words are used. The 5W-MRG and 5W-GPHEX models use a combination of the 5 word-based data views, as in MRG and GPHEX, respectively. The 5C-MRG and 5C-GPHEX system use a combination of the 5 chunk based data views, as in MRG and GPHEX, respectively. The 10-MRG system uses all 10 data views combined as in MRG. The 10-GPHEX/MRG system uses the 5 word based views combined as in GPHEX, the 5 chunk based views combined as in GPHEX, and then a combination of these two combo-models as in MRG.  It can be seen that results improve by combining several data views. Furthermore, global phrase extraction (GPHEX) seems to work much finer than local phrase extraction (LPHEX). Table 4 shows MT results after optimizing λ tm , λ lm , λ w , and the weights for the MRG operation, by means of the Downhill Simplex Method in Multidimensions (William H. Press and Flannery, 2002) . Observe that tuning the system improves the performance considerably. The λ w parameter is particularly sensitive to tuning. Even though the performance of chunk-based models is poor, the best results are obtained by combinining the two levels of abstraction, thus proving that syntactically motivated phrases may help. 10-MRG and 10-GPHEX models achieve a similar performance. The 10-MRG-best W N system corresponds to the 10-MRG model using WordNet. The 10-MRGsub W N system is this same system at the time of submission. Results using WordNet, taking into account that the number of unknown 4 words in the development set was very small, are very promising. Conclusions We have showed that it is possible to obtain better phrase-based translation models by utilizing alignments built on top of different linguistic data views. These models can be robustly combined, significantly outperforming all of their components in isolation. We leave for further work the experimentation of new data views such as word senses and semantic roles, as well as their natural porting and evolution from the alignment step to phrase extraction and decoding. Acknowledgements This research has been funded by the Spanish Ministry of Science and Technology (ALIADO TIC2002-04447-C02). Authors are thankful to Patrik Lambert for providing us with the implementation of the Simplex Method used for tuning.",81127,c6a3e8627dd17293efa7c49d426befb52e0263fa,16,https://aclanthology.org/W05-0826,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Gim{\'e}nez, Jes{\'u}s  and
M{\`a}rquez, Llu{\'\i}s",Combining Linguistic Data Views for Phrase-based {SMT},145--148,,,,,,,inproceedings,gimenez-marquez-2005-combining,,,
31,2009.mtsummit-posters.22,"We have developed a web site called Minna no Hon'yaku (""Translation for Everyone by Everyone""), which hosts online volunteer translators. Its core features are (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor QRedit. Translators who use QRedit daily reported an up to 30 per cent reduction of the overall translation time. As of 3 July 2009, there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan.","We have developed a web site called Minna no Hon'yaku (""Translation for Everyone by Everyone""), which hosts online volunteer translators. Its core features are (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor QRedit. Translators who use QRedit daily reported an up to 30 per cent reduction of the overall translation time. As of 3 July 2009, there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. Introduction Online volunteer translators, who are involved in translating online electronic documents in their free time, translate a variety of documents every day, such as blogs, Wikipedia articles, open source software manuals, documents on nongovernmental organization (NGO) activities, and so on. These translations are read for pleasure, for practical purposes, for language learning, and many other reasons. Needless to say, volunteer translators contribute a great deal to the sharing and spreading of information around the world. Consequently, supporting their activities is a very important research issue. Volunteer translators translate a large number of documents everyday. However, they lack proper translation support tools (Abekawa and Kageura, 2007a) . Thus, providing a good supporting environment should be of great assistance in improving volunteer translators' efficiency and increasing the level of enjoyment they experience in translating. This is the motivation for our work in this paper. 2 Hosting volunteer translators Abekawa and Kageura (2007a) have developed a translation aid editor, QRedit, which has been experimentally provided to a limited number of volunteer translators. They report that QRedit is very effective for aiding their work, as described in Section 8. Based on the success of this translation aid editor, we have developed a web site called Minna no Hon'yaku (MNH, ""Translation for Everyone by Everyone"") where everyone uses QRedit and other translation support tools. In addition, documents translated on MNH are open to the public because these translations are assigned open licenses such as Creative Commons licenses. A screenshot of MNH is shown in Figure 1 . Currently, MNH hosts volunteer translators who translate Japanese (English) documents into English (Japanese), as QRedit currently only supports Japanese-English and English-Japanese translation directions. We plan to extend QRedit and MNH to other language pairs in our future work. We also plan to make QRedit and the software system of MNH open source. There are three reasons why we have developed MNH. First, we were inspired by the success of hosting services for open source software such as sourceforge.net. These services help engi- Our hope is that hosting services for volunteer translators could have a similar impact on the creation and distribution of translations. Second, hosting volunteer translators enables them to use translation support tools without installation efforts (or fees) because support tools such as QRedit are built-in functions of MNH (and everyone uses MNH for free). This situation sharply contrasts to the usual situation of volunteer translators in which they do not use translation aid systems for a variety of reasons, such as the fact that they are too expensive for personal use. (Abekawa and Kageura, 2007a) . Third, hosting volunteer translators means that their documents are saved and published on MNH. These translations are shared by translators and used to make a parallel corpus. Translators can search this parallel corpus for expressions translated by other translators. This is an important benefit of hosting volunteer translators, for if a large number of translators use MNH, a large parallel corpus will become available on the site. Consequently, translators will be able to share a rich parallel corpus for their translation activities. We opened MNH to the public on April 8 2009. We also asked several volunteer translation groups to use MNH as their translation platform in order to get feedback for improving MNH. We are now running MNH to see if MNH can attract and host many volunteer translators. 1 1 Currently, MNH only has a Japanese interface. We plan to In the following sections, we describe the details of MNH. MNH core features MNH has the following four core features: (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor QRedit. Blog-like look and feel We designed the look and feel of MNH to be similar to those of standard blogs, to make it as easy as possible for a wide variety of volunteer translators to use. For example, the top page of MNH (Figure 1 ) lists new translations, active translators, translations of Wikipedia articles, and so on. The three lines at the top part of the page list tags that are assigned by translators to their translations. By clicking on these tags, the corresponding translations are listed in the top page. The box in the upper right corner is the search box for documents, tags, translators, and so on. MNH also provides a place where translators can ask and answer questions on any topic. This mechanism is very useful for sharing translation experiences. When a translator, taro, logins to MNH, he is delivered to his personal page, as shown in Figure 2 , where he edits his translations. To translate a new document, he enters the URL of the document into the long box at the top of the page and clicks the button on the right side. This launches the translation aid editor QRedit, which he uses to translate that document. The details of QRedit are described in Section 7. When he finishes his translation, the translation is saved to his personal space. He may open his translation to the public after assigning a proper license to the document. Open translations are shared by translators (and others) as described in Section 5. Translators also share term lists which they enter into MNH. A screenshot of a term list is shown in Figure 3 . These terms are treated as public domain contents as described in the terms of service of MNH. Thus, anyone can use these term lists for any purpose. add an English interface by the end of this summer. Attribution This license lets others distribute, remix, tweak, and build upon the translators work, even commercially, as long as they credit the translator for the original creation. Attribution Share Alike This license lets others remix, tweak, and build upon the translator's work even for commercial reasons, as long as they credit the translator and license their new creations under the identical terms. Attribution Non-Commercial This license lets others remix, tweak, and build upon the translator's work non-commercially, and although their new works must also acknowledge the translator and be non-commercial, they don't have to license their derivative works on the same terms. Attribution Non-Commercial Share Alike This license lets others remix, tweak, and build upon the translator's work non-commercially, as long as they credit the translator and license their new creations under identical terms. Translators can also use other licenses that are similar to the Creative Commons licenses listed above. In this way, translators can legally share their translations on MNH. These shared translations are used to make a parallel corpus by using a sentence alignment method (Utiyama and Isahara, 2003) . Currently, MNH has a simple bilingual concordancer as shown in Figure 4 . We plan to extend this concordancer in our future work because a bilingual concordancer is a very important tool for translation (Macklovitch et al., 2009) . High quality, comprehensive language resources Dictionaries and the web are the two main language resources that online volunteer translators use during translation. MNH, in cooperation with Sanseido, provides the ""Grand Concise English Japanese Dictionary"" (Sanseido, 2006) to translators. It has about 360,000 entries and is socially accepted as a standard and comprehensive dictionary. Consequently, translators will not need to use other dictionaries in most cases. We also plan to incorporate a comprehensive Japanese-English dictionary of names, collected from the web and consisting of about 400,000 entries (Sato, 2009) , in order to handle person names that are not covered in usual bilingual dictionaries. MNH also provides seamless access to the web, because the web is a very important resource for checking factual information. 2 For example, MNH provides a dictionary that was made from the English Wikipedia. This enables translators to reference Wikipedia articles during the translation process as if they are looking up dictionaries. MNH also provides a seamless connection to web searches as described in the next section. 7 Translation aid editor: QRedit About QRedit QRedit is a translation aid system which is designed for volunteer translators working mainly online (Abekawa and Kageura, 2007a) . Volunteer translators involved in translating online documents have a variety of backgrounds. Some are professional translators, some translate documents about topics they are interested in, while others translate as a part of their NGO activities. They nevertheless share a few basic characteristics: (1) They are native-speakers of the target language (TL). (2) Most of them do not have a native-level command in the source language (SL). (3) They do not use a translation aid system or machine translation (MT) system. (4) They want to reduce the burden involved in the process of translation. (5) They spend a great deal of time looking up reference sources. (6) The smallest basic unit of translation is the paragraph and ""at a glance"" readability of the SL text is very important. The philosophy of QRedit reflects these characteristics and can be summarized as follows: (R1) To reduce the time it takes for translators to do what they are currently doing, rather than to add new functions; (R2) To provide sufficient information sources so that translators do not have to look elsewhere; (R3) To provide information to facilitate decisionmaking by translators; (R4) To provide information that triggers translators' creativity; (R5) To make the interface as simple as possible. These requirements stem from the requests made by volunteer translators (Abekawa and Kageura, 2007a) . QRedit was designed to meet these requirements. For example, it uses the high quality, comprehensive language resources described in Section 6, based on R2. It also provides a seamless connection to web searches, based on R4. Further, based on R1 and R3, QRedit does not provide an MT function, for the following reasons: (1) In terms of the quality of translation, MT results are far behind human translations for Japanese-English (or English-Japanese) translation. As a result, using MT results does not contribute to the reduction of the overall translation time.  (2) Given input texts, MT systems produce output translations without human intervention. This situation contradicts R3. Even if MT outputs are good translations, for now most translators are not ready to accept MT results as reliable, because they do not regard computers as communication partners. Thus, they want to decide on translations and control translation process by themselves. Finally, we made the interface of QRedit as simple as possible to meet R5. In the following sections, we introduce major features of QRedit. Automatic word lookup When a URL of an SL text is input into QRedit, it loads the corresponding text into the left-hand panel, as shown in Figure 5 . (Users can also copy-andpaste the SL text.) Then, QRedit automatically looks up all words in the SL text using the dictionaries described in Section 6. When a user clicks on an SL word, its translation candidates are displayed in a pop-up window. The user can paste a translation candidate into the right-hand panel, which is used for writing the translation, as described in Section 7.3. Idiom lookup In addition to the single word lookup method, QRedit has a flexible multi-word unit lookup function (Takeuchi et al., 2007) . For example, QRedit automatically looks up the dictionary entry ""with one's tongue in one's cheek"" for the expression ""He said that with his big fat tongue in his big fat cheek"" or ""head screwed on right"" for ""head screwed on wrong"". This function is a major advantage of QRedit compared to other MT systems or computer-aided translation (CAT) systems. Indeed, this function has not been realized in any English-Japanese MT system we have checked, and while some CAT systems realize similar functions through approximate matching, they do not specifically target the look-up of idioms with their variations. The lack of flexible idiom lookup in other systems does not mean that this function is not needed. In fact, there is a pressing need for it. The importance of this function derives from two factors: (a) many translators, even experienced ones, have relatively less knowledge of idioms than of words; and (b) some idioms may not be identified as such by translators, because they make sense without an idiomatic interpretation. This leads to translation mistakes. The flexible multi-word unit lookup function of QRedit helps translators identify idioms, and thus reduce translation mistakes. Stratified term emphasis Because idioms are often missed by translators, QRedit notifies translators of the existence of idioms or terms that have special meanings. To notify users of the existence of such terms, it is good to highlight the terms with text decoration. But too many highlighted terms reduces the readability of source language texts dramatically. This is a serious problem, because the richer the reference sources become, the greater the number of candidates for notification. To resolve this problem, QRedit adopts a stratified term emphasis method, which distinguishes three user awareness levels depending on the type and nature of the reference unit, or the candidate term for notification. These awareness levels are reflected in the way the reference units are displayed, such as a change of background color or underlining. These levels are decided according to a four criteria: ""composition,"" ""difficulty,"" ""specialty"" and ""resource type."" See Figure 5 for an example and refer to (Abekawa and Kageura, 2007b) for details of this method. Lookup that doesn't disturb translation When users click on a term in the left-hand panel, its translation candidates are displayed in a pop-up window, as shown in Figure 5 . This action does not affect keyboard operation. That is, users can still write their translations continuously without moving the mouse cursor back to the right-hand panel, as whatever they do with the mouse, the keyboard cursor always stays in this panel. This function helps translators concentrate on translation, as the interviews in Section 8 show. If translators had to reset the cursor every time they looked up the dictionaries, it would break the rhythm of their work. QRedit helps them maintain their rhythm and focus on their translation activities. Functions associated with lookup Translators can call up the following four functions for each term and each translated candidate in a popup window, as shown in Figure 5 . • Paste Paste a term or translation term to the righthand panel. This helps the translator avoid misspelling of long named entities or numerical representations. • Detailed lookup Display complete dictionary entries for the term: attributes (domain, nuance, etc.), relative information (derivative words, antonyms, etc.), and example sentences. These entries are displayed in another window because of the large amount of information. • Web search Display search results from a web search engine. This help translators confirm in which context the term is used or how to use the term. • Register term Register a term to user's term list. Note that users can look up registered terms in QRedit. Lookup by keyboard To display a pop-up window, translators click on terms with their mouse. But mouse operation can be an obstacle to efficient input of TL texts. Therefore, in QRedit, translators can look up terms by using only their keyboards. While inputting TL texts, translators can activate a keyboard incremental search method. Figure 6 shows an example of an incremental search when the user has input the first character 's'. Customization The environment for inputting TL texts can greatly impact the efficiency of translation work. In addition, an optimal environment differs from translator to translator. So, a flexible customization method that meets the needs of various translators is required. In QRedit, users can customize the following factors in their translation input environments. • Placement of the SL area [left, right, top, bottom] • Width or height of the SL area • Placement of translation candidate display [inside, left, right] of the SL area • Synchronized scroll [both directions, source→target, target→source, none] Figure 7 shows an example of a customized QRedit window. (See Figure 5 for a default window.) User response As of 3 July, 2009 -three months after we made MNH and QRedit publicly available -there are about 600 users and 4 groups registered to MNH, including such major NGOs as Amnesty International Japan and Democracy Now! Japan. As quantitatively evaluating the benefit of using translation aid systems is technically a difficult task (cf. (Macklovitch, 2006) ), and as we are dealing with volunteer translators who are not working on a ""time is money"" basis but rather wish to reduce the subjective burden of translation, we carried out qualitative evaluations through e-mail and phone interviews with three users. They are trial users who have been using QRedit since before it was made public, thus have enough experience to give informed judgment on the system. According to them, without QRedit, the division of time for the different elements of translation is roughly: 10 to 30 per cent for dictionary lookup, 10 to 40 per cent for searching the web to obtain information, 20 to 50 per cent for draft translation, and 0 to 10 per cent for revision and editing. All in all, around one fourth to half of the time is used for dictionary and web lookup. Translator A, a novice translator with two years' experience, said that she feels it takes her an average of 20 per cent and a maximum of 30 per cent less time to complete translations using QRedit, with the clear additional benefit that the quality of draft translations is improved because she can concentrate on context/paragraph reading rather than dictionary lookup in making draft translations. Translator B, a middle level translator with three years' experience, also reported an average 25 per cent to 30 per cent reduction in the overall translation time, with the same effect on quality as translator A. Translator C, who is an expert translator, reported that she prefers the environment she is familiar with for translating easier texts, but can reduce translation time by 20 per cent when dealing with other texts. She said that QRedit enables her to tackle a wider variety of texts than before. We have not yet been able to obtain responses from group users, because they have only started using QRedit recently and thus have not accumulated sufficient experience to give an informed judgment on the system. We also have not been able to evaluate the usability of MNH because it is still under development and we are now improving its usability based on comments and suggestions from users. Related work Related work can be roughly classified into three types, i.e. projects that aim at translation and publishing translated documents; work that is concerned with hosting translated documents, often multilingually; and work that is addressed at aiding translators and translation communities. There are too many joint or collaborative online translation projects to mention here. GlobalVoices Online 4 is perhaps one of the most well known, along with TUP 5 (Translators United for Peace). Most projects do not provide translation aid facilities or collaborative working environments. They are rather projects defined by interested groups of people, using existing facilities. An example of the second category is Yakushite.net (Shimohata et al., 2001) . It provides a collaborative translation environment in which users can use MT for translation, while contributing to collaborative terminology augmentation for the improvement of MT. Except for providing the MT engine, the translation aid functions are weak. Worldwide Lexicon (McConnell, 2007) is another example. Within the project a variety of mechanisms are provided that facilitate the sharing of translated documents world wide, with which one can (i) detect translated texts, if there are any; (ii) translate by oneself; (iii) subscribe to an RSS feed for translation; and (iv) use machine translation. The system, however, does not provide rich facilities to aid human translations (step (ii)). As such, the system is more a hosting service rather than a translation aid system. We are also witnessing the rapid growth of Wikibased platforms to facilitate collaborative translations, such as Traduwiki (related projects are listed in http://wiki-translation.com/ tiki-index.php) and BEYtrans (Bey et al., 2008) . They provide functions that support collaboration among translators, as well as providing a translation memory function, thus having the features of translation aid systems as well as translation hosting functions. Turning our eyes to fully-fledged translation aid systems, there are several commercial and noncommercial systems. SDL Trados 6 is one of the most well known and widely used. SDL also developed Idiom WorldServer system 7 , an online multilingual document management system with translation memory functions. There are many other systems such as TransType (Macklovitch, 2006) and the free translation memory and terminology management system Omega-T 8 . Though it is now a little dated, Gow (2003) evaluates different translation aid systems. The functions that MNH provides are closer to those provided by Idiom WorldServer or Wiki-based collaborative translation aid systems, but MNH provides a high-quality bilingual dictionary and functions for seamless Wikipedia and web searches within the integrated translation aid editor QRedit, thus providing connections to the existing reference information infrastructure that online translators use. This reflects the fact that the main target of MNH is online documents available under the Creative Commons licenses. On the other hand, collaborative, project-oriented document management functions are weak in MNH, because MNH basically assumes use by individual translators or smaller groups. Implementing this functionality will be a future task. Conclusion We have developed a web site called Minna no Hon'yaku (MNH, ""Translation for Everyone by Everyone""), which hosts online volunteer translators. Its core features are (1) a blog-like look and feel; (2) the legal sharing of translations; (3) high quality, comprehensive language resources; and (4) the translation aid editor; QRedit. Translators who use QRedit daily reported an up to 30 per cent reduction of their overall translation time. We are now running MNH to see if it can attract and host many volunteer translators. Please join MNH!",14565746,e12f5b1510741059062bce873e9083681ffa98eb,9,https://aclanthology.org/2009.mtsummit-posters.22,,"Ottawa, Canada",2009,August 26-30,Proceedings of Machine Translation Summit XII: Posters,"Utiyama, Masao  and
Abekawa, Takeshi  and
Sumita, Eiichiro  and
Kageura, Kyo",Hosting Volunteer Translators,,,,,,,,inproceedings,utiyama-etal-2009-hosting,,,
32,2020.lrec-1.386,"As part of constructing the NINJAL Parsed Corpus of Modern Japanese (NPCMJ), a web-accessible language resource, we are adding frame information for predicates, together with two types of semantic role labels that mark the contributions of arguments. One role type consists of numbered semantic roles, like in PropBank, to capture relations between arguments in different syntactic patterns. The other role type consists of semantic roles with conventional names. Both role types are compatible with hierarchical frames that belong to related predicates. Adding semantic role and frame information to the NPCMJ will support a web environment where language learners and linguists can search examples of Japanese for syntactic and semantic features. The annotation will also provide a language resource for NLP researchers making semantic parsing models (e.g., for AMR parsing) following machine learning approaches. In this paper, we describe how the two types of semantic role labels are defined under the frame based approach, i.e., both types can be consistently applied when linked to corresponding frames. Then we show special cases of syntactic patterns and the current status of the annotation work.","As part of constructing the NINJAL Parsed Corpus of Modern Japanese (NPCMJ), a web-accessible language resource, we are adding frame information for predicates, together with two types of semantic role labels that mark the contributions of arguments. One role type consists of numbered semantic roles, like in PropBank, to capture relations between arguments in different syntactic patterns. The other role type consists of semantic roles with conventional names. Both role types are compatible with hierarchical frames that belong to related predicates. Adding semantic role and frame information to the NPCMJ will support a web environment where language learners and linguists can search examples of Japanese for syntactic and semantic features. The annotation will also provide a language resource for NLP researchers making semantic parsing models (e.g., for AMR parsing) following machine learning approaches. In this paper, we describe how the two types of semantic role labels are defined under the frame based approach, i.e., both types can be consistently applied when linked to corresponding frames. Then we show special cases of syntactic patterns and the current status of the annotation work. Introduction Semantic role labeled data is important to capture predicate-argument sentence level meaning for NLP researchers, linguists, and language learners. For English, various kinds of annotated corpora with semantic roles and frames are provided (e.g., PropBank (Bonial et al., 2010) and FrameNet (Baker et al., 1998) ) and they are widely used. The numbered semantic roles (e.g., Arg0, Arg1, Arg2) proposed in PropBank are effective for adapting to various systems of semantic roles. Thus, the Abstract Meaning Representation (AMR) (Banarescu et al., 2013) approach, which is based on PropBank numbered semantic roles and frames, has attracted a lot of attention from NLP researchers as a model for describing sentence-level semantics. AMR parsing models are studied with deep neural network models, e.g., (Zhang et al., 2019) using AMR resources for English 1 . In terms of Japanese language resources, several corpora containing annotated tags related to predicateargument information have been proposed (Kyoto (Kawahara et al., 2002) , NAIST (Iida et al., 2007) , EDR (EDR, 1995) , GDA (Hashida, 2005) , Japanese FrameNet (Ohara et al., 2006) , BCCWJ-PT (Takeuchi et al., 2015) ). However, numbered semantic roles have not been annotated for Japanese texts. Also, none of the annotated corpora that are reported in the literature are web accessible because of licensing restrictions 2 . 1 LDC2017T10 and LDC2014T12. 2 Japanese FrameNet is constructed on the Balanced Corpus of Contemporary Written Japanese (BCCWJ) (Maekawa et al., 2014) , from which it inherits release issues. Thus, the annotated sentences are not available on the website: http://sato.fm.senshu-u.ac.jp/frameSQL/jfn23/notes/index2.html (accessed 2019/11/22). In this research project, we annotate two types of semantic role labels and frames of predicates from the NINJAL Parsed Corpus of Modern Japanese (NPCMJ), which is a freely available web-accessible treebank (NINJAL, 2016) 3 . The semantic roles and frames are annotated based on a freely available web-accessible thesaurus of predicateargument structure for Japanese (PT) 4 . Utilizing the tree structure of the NPCMJ, we can focus directly on the problem of labeling for the semantic roles and frames of target predicates. This is because the identification of target predicates and their arguments can be automatically derived from the treebank following (Horn et al., 2018) . The first type of annotation for semantic roles is PropBankstyle, that is, numbered semantic roles. The second type involves names for semantic roles (e.g., Agent, Theme, Goal), which has been used in PT. The reason why we employ the conventional named semantic roles is that the named semantic roles are intuitively more understandable for humans compared to the numbered roles. Thus, the named roles are expected to be used in queries when language learners or linguists want to extract example sentences that, e.g., contain ""Experiencer"" arguments in the emotional sense, or ""Source"" arguments in the moving sense. Contributions of this paper are as follows: (1) For NLP researchers, linguists, and Japanese language learners, we propose a dual semantic role annotation that is composed of: (i) numbered semantic roles, and (ii) semantic roles with conventional names. (2) The proposed numbered semantic roles are framebased so that roles can be assigned consistently with regards to their corresponding frame. (3) We reveal that the frame-based semantic roles are suitable for Japanese because of dividing transitive and intransitive verbs. (4) We show that the current annotated predicates are about 10,000, and reveal highly promising accuracy results from a semantic role annotation system with a neural network model. Frame-Based Semantic Roles 2.1. Frame-based Semantic Roles Are Needed in an Agglutinative Language Such as Japanese With semantic role annotation, the aim is to fix the semantic relations between arguments and predicates in a manner that is abstracted away from differences between case markers and syntactic alternations. One of the difficulties of annotating semantic roles with names involves settling on a system of names, such as Agent, Theme, and Patient. Several different systems of semantic roles are proposed in previous studies (Fillmore, 1968; Palmer et al., 2010; Loper et al., 2007; Baker et al., 1998) . By contrast, the numbered semantic roles proposed in PropBank can absorb these differences, because numbered arguments can be consistently assigned in example sentences for the target verb sense. In Japanese, some verbs, e.g., 'hirak-u' (open), behave as both intransitive and transitive verbs without any intransitivizing or transitivizing suffix 5 . (5) a. Intransitive and transitive pairs similar to 'ak-u' and 'ak-eru' above are registered as different lexical units in morphological analyzers (e.g., MeCab 6 ) as well as Japanese dictionaries even though they share the same stem. However, they share the same semantic role set. This is why we need to assign the same semantic role set and frame (that is, frame-based semantic roles) to each verb. Thus, we define a frame that indicates a shared concept of predicates that belong to the frame, and then the frame unites a consistent semantic role set. Each verb meaning of polysemous verbs is fixed by adding example sentences. Since a polysemous verb has several meanings, each verb meaning is connected to a distinct frame. For example, the verb 'hirak-u', shown in (5), has another meaning as in (7). The meaning of 'hirak-u' in ( 7 ) is assigned to the development frame to which verbs such as 'kaitaku-su-ru' (cultivate) and 'kaihatsu-su-ru' (develop) are also assigned. PT: Repository of Semantic Roles and Frames for Japanese Predicates As a base repository of semantic roles and frames for Japanese predicates, we use PT. PT is composed of hierarchical frames for predicates and each frame indicates a shared meaning of predicates whose sense is designated with semantic-role-annotated example sentences. In previous work (Takeuchi et al., 2010) , PT was constructed for about 11,000 Japanese predicates, with about 23,000 annotated example sentences. The semantic roles in PT have conventional role names (e.g., Agent, Theme, Goal) 7 . Thus, by adding PropBank-style semantic roles to the example sentences in PT, we utilize PT as a repository of frames containing frame based semantic roles that are both numbered and named for Japanese predicates. Figure 1 shows an example of how the verb 'hirak-u' is registered in PT. As a polysemous verb, 'hirak-u' has open, develop, and start meanings. Each word meaning has an example sentence that is annotated with both numbered semantic roles and named semantic roles. The semantic roles are Arg1 and Theme in all of the example sentences 8 in Figure 1 . !""#$%&'(# )*#+%&'*,%(#-. '/01,'/#""0 2%""'/0 2%""'/0 $3'43,,,2%""'/0 5""67 82#-# )93-#3:#,3;#:9,(2#,$33"". 6#:<'43,,2%""'/0 5""67 82#-# )93-#3:#,&'*(%='(#9 (2#,>#9(#""*':$. 2':'<'43,2%""'/0 5""67 82#-# )93-#3:#,9('""(9,(2#,?*3>#"" 923;. @%??#""#:(,-#':%:69 3?,;""#$%&'(# !""#$ %&'$(#)!*)+,',# 0; ,(3 ,A ,2 %# ""' ""& 2< 82#9'0""09,3?,?""'-#9 /'%('/090""0 /'%2'(9090""0 2%""'/0 -#.#/!""0#$, !12#%,)%&'$(# +,'3,)#$- +,'3, /%6<3090""0 /'%(#:90""0 2%""'/0 BCD7 BEAF BEGG Figure 1: Example of how a polysemous verb is registered in PT, where PT is the repository of semantic roles and frames. Then each example sentence is linked to a frame in the thesaurus. For example, the meaning of 'hanaya o hirak-u' (someone starts the flower shop) is assigned to start 9 whose frame ID is 366. The start frame also contains 'kigyoosu-ru' (start new venture) and 'kaiten-su-ru' (open a shop). The structure of frames is a thesaurus. There is no multiple inheritance in the thesaurus of frames. Such structure is similar to a synset in WordNet. Frame-based numbered semantic roles are convenient to capture the same type of arguments with different named roles for different verbs in the same frame. For example, both predicate verbs 'oros-u' (get down) and 'ochi-ru' (fall) in ( 8 ) and ( 9 ) belong to the moving from frame. In the moving from frame, the mover of the moving event is annotated as Arg0 and Agent. This raises concern for (9), where it is 'chichi' (father) who is moved. For such a 8 Subjects (i.e., Arg0) are often omitted in Japanese, even when the verb is transitive. 9 To be exact, this is the change of state/start end/start frame. For simplicity, we often omit to write the hierarchy of the frame. case, 'chichi' is annoated with the Experiencer named semantic role because 'chichi' is moved without intending to be moved. According to the annotation guidelines of Prop-Bank (Bonial et al., 2010) , Experiencer is typically Arg0 in numbered roles. However, the numbered role Arg1 is usually assigned to the Patient argument, i.e. ""the argument which undergoes the change of state or is being affected by the action"". Consequently, 'chichi' is annotataed as Arg1 in (9), so as to be consistent in the meaning of something which moves, i.e., 'hon' (book) and 'chichi' (father) in the sentences. On the other hand, named semantic roles are helpful for understanding the meanings of arguments. The named semantic roles are annotated across frames, and thus the same type of arguments can be extracted, while arguments numbered Arg2 or with a higher number depend on each frame 10 . In ( 10 ) and ( 11 ), even though the frames are different, 'kangoshi' (nurse) and 'asshoo' (big win) are a complement of the argument with the accusative case marker. For the arguments, the named semantic role is Complement (ACC) that indicates a complement of the argument with accusative case. Framework of Annotation Task As mentioned in the previous section, the repository of semantic roles and frames (i.e., PT) is web-accessible, and has search functionality so annotators can look up example sentences in PT. The annotation task is carried out by annotators under the guidance of a supervisor. Figure 2 gives an overview of the annotation task. The procedure of annotation of semantic roles and frames on the NPCMJ is as follows: 1. Target predicates and their arguments are extracted by a program (Treebank Semantics) (Butler, 2019) . This works by converting constituency tree annotations into logic based meaning representations from which 10 From the standpoint of Construction Grammar (Goldberg, 1995) , the named semantic roles and the numbered semantic roles can be regarded as 'argument roles' and 'patient roles', respectively. The named semantic roles are annotated according to syntactic expressions of arguments for verbs, while the numbered semantic roles are annotated based on frames. predicate-argument information can be extracted and remapped back into the tree structures of the NPCMJ for identifying target predicates and their arguments. 2. Annotators select the target sentence, look up a target predicate for an example, find the most appropriate frame for the example, and then assign the frame ID number to the target predicate. 3. Then, annotators assign semantic roles to the arguments according to the examples in PT. 4. If annotators find cases that are missing from PT or annotators are not confident, annotators write a report. 5. The supervisor adds new examples to PT or has discussions with annotators according to their reports 11 . In step 2, annotators can see the dependencies of the target sentence on the NPCMJ server (Figure 3 ). Since the NPCMJ is a web-accessible treebank, annotators can also look at the tree view of the target sentence from the same webpage (Figure 4 ). As can be seen in Figure 2 , annotators input frame IDs and semantic roles directly into the NPCMJ server, then the annotated results can be checked by all the other annotators and the supervisor. The frames of PT were constructed on the basis of the Lexeed database (Fujita et al., 2006) . This database is a sense 11 Currently there are three annotators working on the project. Discussions with the supervisor are used to settle all cases where annotators are not confident. repository for a rule-based machine translation system. It follows that the frames of PT have a wide coverage for frequent predicates. However, there remain words and word meanings that have not been registered. Adding new words or example sentences to PT is limited to the supervisor only so as to maintain the consistency of the dictionary. A problem with this setup is that the operation of extending PT could become a bottleneck for the annotation. !""""#$%#%&'()& &*+,-.&) /0123#4%&&#5+67 !66$4+4$%) ""&-&6""&689#""+4+ +66$4+4&""#""+4+ /0123#)&%'&% !""""#%$.&)#+6""#:%+,&) 0; <*+,-.&)#$:#)&,+64(8 %$.&)#+6""#:%+,&) =$$7#>- )>-&%'()$%# ?&-$%4# >6+66$4+4&"" 8+)&) 0%$5.&, %&-$%4) ""()8>))($6) ;+)7#$:#+66$4+4$%) ;+)7#$:#)>-&%'()$% Examples of Complicated Annotations With the annotation task, we found syntactic and grammatical variations because, as a corpus, the NPCMJ is composed of texts from very diverse genres, such as white papers, novels, speech dictations, news papers, and so on. Here we show some complicated cases of semantic role annotations. Causative form As described in Section 2.1., some transitive verbs have corresponding intransitive verbs. Both verbs can take the causative form. In PT numbered arguments and named roles are assigned on the basis of active voice. Note that causative forms of intransitive verbs exhibit structure similar to transitive verbs. Passive form Japanese has several special grammatical forms in the passive voice. The first one is the adversative passive (Wierzbicka, 1979) , that is, the passive form for an intransitive verb. Light verbs Japanese often uses the light verb construction. For this construction, there is an argument that works as the predicate, and so this argument that provides the predicate content is withdrawn from being assigned a semantic role. For example, consider (18). ( 18 In ( 18 ), the target verb 'shi-ma-su' (do) is a light verb. The content of the action is expressed by the deverbal noun 'oshaberi' (chatting). For this light verb case, the Arg PRX tag is assigned to the content argument 'oshaberi o'. This follows the treatment proposed in the PropBank guidelines (Bonial et al., 2010) . However, the composed meaning of the light verb construction, i.e., the chat frame and the verb 'oshaberi-sur-u' (chat-do-PRS) are assigned to the light verb 'shi-ma-su'. Thus the role sets of the chat frame are applied to the arguments and adjuncts except for 'oshaberi o' (ArgM PRX). Current Annotation Results In this section we discuss the current state of annotation. First, we show the statistics of annotated numbers of sentences, target predicates, types of predicates, and semantic roles. Second, we apply the annotated corpus to a machine learning system to estimate roughly the quality of annotated semantic roles with comparison to previous studies. Currently we are completing the first round of annotation that we plan to review. Statistics of Annotated Data Preliminary Experiments of Semantic Role Labeling Using Deep Learning Model Motivation Previous work has constructed a semantic role labeling system with neural network models for Japanese (Okamura et al., 2019) . The target data is BCCWJ-PT, where data is annotated with the semantic roles defined in PT. While the sentences in BCCWJ-PT are different from those of the NPCMJ, we can roughly estimate the quality of the named semantic roles in the NPCMJ by comparing the accuracy of a semantic role labeling system using the NPCMJ to the case of using BCCWJ-PT. Semantic role labeling system The input of the semantic role labeling system is the target predicate and morpheme sequence of its argument (or adjunct), and then the output is a semantic role label of the argument. All of the morphemes are converted to d dimension vectors with nwjc2vec, which gives Japanese word vectors 13 . Let X be a vector sequence of an input morpheme sequence, that is, X = x 1 , . . . , x t , . . . , x T (x t ∈ R d ). Let S be output of a semantic role label for input X, the estimated semantic role label is defined by Formula (1). Ŝ = argmax j∈Sem p(S j |X) (1) Where, Sem is a set of semantic role labels, and S j (j = 1, . . . , Sem) is the jth semantic role label. Let y j be an output of the jth unit at the final output layer of the neural network model. Since the softmax function is used as a non-linear function at the output layer, y j can be a probability. y j = p(S j |X). (2) Then Ŝ can be estimated by using a neural network. As a neural network model, we apply bi-directional GRU with the max-pooling model (hereafter referred to as the bi-GRU model) to the semantic role labeling because the bi-GRU model gave the best performance in the previous study of Okamura et al. (2019) . Figure 5 shows the 13 http://nwjc-data.ninjal.ac.jp/ !""#$%& !""'""#"" ("")))))))))))))) !""%"" !$%$ *+ @$A6)=%""&516) !2' ""B))))))))))))))) $C B%""# DDC@6E)@$C)=%""&516)C2(<6))DC@6E)F$C)$)!2'E -""%0@6#6C)2()=@6)$%3&#6(=G$'H&(<= 8$%36=) 0%6'2<$=6 Figure 5 : Bi-directional GRU with max-pooling model architecture of the bi-GRU model. An input vector sequence X is applied to the input of bi-directional GRU, and then the max-pooling is applied to outputs of the GRU with time sequence direction. The final output y = [y 1 , . . . , y j , . . . , y Sem ] is obtained after applying a fullyconnected layer to the results of max-pooling. Experimental setup The number of hidden units of GRU is 256. The settings of the optimizer are set the same as in Okamura et al. (2019) . The annotated data is divided into 65%, 5%, and 30% for training, development, and test data, respectively. The performance is evaluated by the accuracy of the test data. Accuracy = # Estimated semantic roles are correct # All instances (3) Experimental results Table 4 shows the total accuracies of numbered and named semantic role labels. The accuracy of the third column shows the results for BCCWJ-PT (Okamura et al., 2019) . According to the accuracies of named semantic roles in Ta-NPCMJ BCCWJ-PT Numbered semantic roles 0.716 N/A Named semantic roles 0.667 0.702 Table 4 : Total accuracy of semantic roles ble 4, the accuracy of the model using NPCMJ is near to that of BCCWJ-PT. BCCWJ-PT was annotated with two annotators for each semantic role as well as being checked by a third annotator, and so the quality of annotated semantic roles is expected to be high. The above results indicate that for the current annotated semantic roles of the NPCMJ the consistency of the annotated tags is promising for a resource that is under development. Comparing numbered and named semantic roles in their accuracy, numbered semantic roles are higher than named semantic roles. The result can be considered to show that the numbered semantic roles are annotated more consistently than the named semantic roles. Comparing the accuracy of numbered semantic roles to the results for the shared task in English semantic role labeling tasks, we have to improve our semantic role labeling models. Discussions One of the difficulties of annotating semantic roles is to discriminate between arguments and adjuncts, while keeping frame consistency in PT. Arguments are the essential factors for the defined frame, while adjuncts are not core elements for a frame. In principle, adjuncts can be attached to any frame. As described in Section 2.1. we added Arg0, Arg1, etc. tags to the previously defined named semantic roles in PT for the frame repository. However, because of the lack of variation in the example sentences, we have found arguments to be missing. This applies especially to arguments that are inserted as parts of constructions. Consider (19), and the need for a create frame that contains the meaning of the verb 'kak-u' (write). For (19), we need to define two essential semantic roles, namely: Arg0 (Agent, writer) and Arg1 (Theme, written thing). ( The recipient 'imooto-ni' (sister DAT) must be part of a construction (Goldberg, 1995) . Thus, the recipient can be considered as an argument because the recipient appears depending on the create frame. We can also confirm the semantic role of the above 'recipient' case by looking at the corresponding examples in English PropBank and FrameNet. In the frameset write.01 of PropBank, the corresponding roles are defined as A2 (benefactive), that is an essential argument. In FrameNet, the frames are more detailed than our frames in PT. The meaning of the above case, i.e., 'write' in English, is assigned to the Contacting frame. In the Contacting frame, the recipient role is defined as the Addressee role that indicates a core role (i.e., an essential argument). Thus both English language resources offer an analysis that is the same as our analysis for the Japanese data. Next, consider (21) 14 , which is another example that is not registered in PT. Frame: create '(They) write it as ""Shomyo"" in the old kanji style' (5 wikipedia KYOTO 11) We are currently investigating whether the two phrases ('kyuujitai de' and 'shoomyoo to') are arguments and/or adjuncts for the create frame of PT. According to FrameNet, the verb 'write' can belong to the Statement, Text creation, Contacting and Spelling and pronouncing frames. The create in PT can partially correspond to the Text creation frame. However, it is the Spelling and pronouncing frame that seems to correspond to (21). In the Spelling and pronouncing frame, 'kyuujitai de' might correspond to Manner and 'shoomyoo to' to Formal realization; and both roles are defined as core roles. This is suggestive evidence that FrameNet has analyzed examples that can be expected to cover Japanese examples too, even though Japanese has different syntactic and grammatical characteristics when compared to English. Thus we think it will be helpful to refer to existing language resources, especially PropBank and FrameNet, in revising framesets in PT. Such comparisons are only possible because there are web-accessible frame data sets, notably, PropBank and FrameNet. Thus, we believe that providing the NPCMJ and the frame repository PT as web-accessible resources is essential for being able to construct reliable semantic role labeled language resources for Japanese and beyond. Conclusion This paper has described ongoing research of constructing an annotated corpus of semantic roles and frames as an addition to the NPCMJ, a Japanese web-accessible parsed corpus. The annotation task is coupled with the expansion of PT, that is, the repository of semantic roles and frames. We annotate two types of semantic roles, i.e., numbered and named semantic roles for the arguments. Both the numbered semantic roles and the named semantic roles are defined consistently with respect to frames. Then numbered semantic roles are expected to be used in NLP, and the named roles are to be used as tags for searching example sentences by language learners and linguists. In the annotated texts, we have found various kinds of syntactical and grammatical variations: e.g., adversative passives, alternations of cases, and collocations. We also applied part of the annotated corpus into a semantic role annotation model based on neural networks to evaluate how the annotated corpus can contribute to the statistical learning model approach. The results show that the accuracies of semantic role annotation systems are almost the same as the current quality of named semantic roles, for results near to the achievements of previous work. Thus, we can estimate that the quality of the annotated semantic roles are highly promising. What is currently annotated is the first part of a planned annotation cycle, and so all is due for review as we also continue to annotate new texts from the NPCMJ. The NPCMJ is planned to increase in size to 60,000 sentences. Acknowledgements A part of the research reported in this paper is supported by the NINJAL project ""Development of and Research with a parsed corpus of Japanese"" through the NINJAL collaborative research project ""Annotating Semantic Role Labels and Frames to NPCMJ"", and by JSPS KAKENHI (Grant Number JP 15H03210 and 19K00552) .",218974487,df3027b9d07d2bbfdb0739a9f0592b3d80e23ce7,0,https://aclanthology.org/2020.lrec-1.386,European Language Resources Association,"Marseille, France",2020,May,Proceedings of the 12th Language Resources and Evaluation Conference,"Takeuchi, Koichi  and
Butler, Alastair  and
Nagasaki, Iku  and
Okamura, Takuya  and
Pardeshi, Prashant",Constructing Web-Accessible Semantic Role Labels and Frames for {J}apanese as Additions to the {NPCMJ} Parsed Corpus,3153--3161,,,,,,979-10-95546-34-4,inproceedings,takeuchi-etal-2020-constructing,English,,"Semantics: Sentence-level Semantics, Textual Inference and Other areas"
33,R13-1052,"We look into the problem of recognizing citation functions in scientific literature, trying to reveal authors' rationale for citing a particular article. We introduce an annotation scheme to annotate citation functions in scientific papers with coarse-to-fine-grained categories, where the coarse-grained annotation roughly corresponds to citation sentiment and the finegrained annotation reveals more about citation functions. We implement a Maximum Entropy-based system trained on annotated data under this scheme to automatically classify citation functions in scientific literature. Using combined lexical and syntactic features, our system achieves the F-measure of 67%.","We look into the problem of recognizing citation functions in scientific literature, trying to reveal authors' rationale for citing a particular article. We introduce an annotation scheme to annotate citation functions in scientific papers with coarse-to-fine-grained categories, where the coarse-grained annotation roughly corresponds to citation sentiment and the finegrained annotation reveals more about citation functions. We implement a Maximum Entropy-based system trained on annotated data under this scheme to automatically classify citation functions in scientific literature. Using combined lexical and syntactic features, our system achieves the F-measure of 67%. Introduction Citations in scientific papers serve different purposes, from comparing one work to another to acknowledging the inventor of certain concepts. Recognizing citation functions is important for understanding the structure of a single scientific document as well as mining citation graphs within a document collection. Therefore, this task has attracted researchers from the fields of discourse analysis, sociology of science, and information sciences for decades (Teufel et al., 2006a) . Most of the existing research in this area focused on the analysis of citation sentiment, which has achieved good accuracy (see, e.g., (Teufel et al., 2006a) ). Citation sentiment analysis systems are usually able to identify positive, neutral, or negative opinions, but if we want to better understand the exact function of a citation, we need to know not only whether the authors like the citation, but also how the citation is used in a given context (Section 2). In this paper, we try to reveal citation functions more accurately than simply classifying citation sentiment. We first create a two level coarse-to-fine grained annotation scheme (Section 3). The coarse-level annotation corresponds roughly to sentiment categories, including POSITIVE, NEGATIVE, and NEUTRAL. The fine-grained annotation scheme provides a more detailed description of citation functions, such as Significant, which asserts the importance of an article or a work, and Discover, which acknowledges the original discoverer/inventor of a method or material. Using data annotated under this scheme, we train classifiers to determine citation functions, and experiment with features from lexical to syntactic levels (Section 4). We predict the finegrained citation function at 67% in F-measure in our experiments, which is at the same level as the coarse-grained citation sentiment classification (Section 5). Related Work The background for our work is in citation analysis. Applications of citation analysis include evaluating the impact of a published literature through a measurable bibliometric (Garfield, 1972; Luukkonen, 1992; Borgman and Furner, 2002) , analyzing bibliometric networks (Radev et al., 2009) , summarizing scientific papers (Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011) , generating surveys of scientific paradigms (Mohammad et al., 2009) , among others. Correctly and accurately recognizing citation functions is a cornerstone for these tasks. et al. (2006b) is the most related to ours. They proposed an annotation scheme for citation functions based on why authors cite a particular paper, following Spiegel-Rüsing (1977) . This scheme provides clear definition for some of the basic citation functions, such as Contrast, but mainly concerns the citations that authors compare to or build upon, ignoring the relationship between two cited works. Sometimes the relationship between two cited works is also meaningful and important, from which we can know more about the functions and influences of one cited work on other works. For example, the cited work may be utilized or applied by another cited work, which would be captured by Practical in our annotation scheme but considered as neutral under their scheme. In addition, their annotation scheme does not explicitly recognize milestone or standard work in a particular research field, while our annotation scheme does through the Significant function. We continue to use these basic functions, but try to expand their scheme by incorporating more functions, such as acknowledgement and corroboration, which reflects the attitude of the research community towards a citation. Regarding the automatic recognition of citation functions or citation categories, Teufel et al. (2006a) presented a supervised learning framework to classify citation functions mainly utilizing features from cue phrases. Athar (2011) explored the effectiveness of sentence structurebased features to identify sentiment polarity of citations. Dong and Schäfer (2011) proposed a four-category definition of citation functions following Moravcsik and Murugesan (1975) and a self-training-based classification model. Different from previous work that mainly classified citations into sentiment categories or coarse-grained functions, our scheme, we believe, is more finegrained. It is also worth noting that Teufel et al. (2006a) , Athar (2011) , and Dong and Schäfer (2011) all worked on citations in computational linguistics papers, but we investigate citations in biomedical articles. Annotation Our annotation scheme contains three general citation function categories POSITIVE, NEUTRAL, and NEGATIVE: POSITIVE citations reflect agreement, usage, or compatibility with cited work; NEUTRAL citations refer to related knowledge or background in cited work; and NEGATIVE citations show weakness of cited work. These three general categories are often used as citation sentiments in previous citation sentiment analysis work. We extend these categories by sorting them into smaller subcategories that reflect the functions of citations. POSITIVE (see + in Table 1 Two annotators are trained to perform the annotation. The articles we work on are from the open access subset of PubMed, which consists of articles from the biomedical domain. We require the annotators to mark citation functions, and point to textual evidence for assigning a particular function. Recognizing Citation Functions We use the Maximum Entropy (MaxEnt) model to classify all citations into the above citation function categories. We experiment with both surface and syntactic features. When parsing the context sentence, we replace each citation content with a <CITATION> symbol, in order to remove the contextual bias. Surface Features We capture n-grams, signal words collected by system developers, pronouns, negation words, and words related to formulae, graphs, or tables in the context sentence as surface level features. • N-Gram Features use both uni-grams of the context sentence and the tri-gram context window that contains the citation. • Signal Word Features check whether the text signals for a citation function (151 words/phrases in total, collected by system developers from dictionaries) appear in the context sentence. • Pronoun Features look for third-person pronouns and their positions in the context sentence. (135 words in total) appear in the context sentence with its scope. NNP • FGT Features fire if words or structures like formula, graph, or table appear in the context sentence. Syntactic Features We capture more generalized or long-distance information by taking advantages of syntactic features. The Part-of-Speech Features use Part-of-Speech (POS) tags adds generalizability to surface level signals, e.g., ""VERB with"" covers signals like ""experiment with"" and ""solve with"", which might indicate a Practical function. We use a combination of POS tags and words in a two-word context window around the <CITATION> as features. In Figure 1 , ""VBD DT"", ""identified DT"", and ""VBD the"" would be extracted. The Dependency Features use the dependency structure of the context sentence to capture grammatical relationships between a citation and its signal words regardless of the distance between them. We extract both dependency triples and dependency labels as features. In Figure 1 , if we extract dependency relations and labels attached to a <CITATION>, we would obtain ""NSUBJ identified CITATION"", ""NSUBJ"", and ""NSUBJ showed CITATION"" as dependency features. ""NSUBJ showed CITATION"" captures the long-distance relation between <CITATION> and a signal word ""showed"", which other features miss. Experiments From 91 annotated articles with total 6, 355 citation instances, we train our model and test the performance through a 10-fold cross-validation procedure, so that each fold randomly contains 9 (or 10) articles with their associated citation instances. 3 : Overall Performance Using Different Features: n-gram features (baseline), FGT features (fgt), signal word features (sig), negation features (neg), pronoun features (pron), dependency structure features (dep), and Part-of-Speech features (pos). Features Table 3 shows the overall performance in Precision (P), Recall (R), and F-measure (F1) by incorporating different feature sets, at a 99.8% confidence level according to the Wilcoxon Matched-Pairs Signed-Ranks Significance Test. If we randomly assign one of the citation function classes to each citation instance, the performance is only 3.8% in F-measure. In addition, a simple majority classifier assigns each citation with whichever class that is in the majority in the training set, also only obtaining F-measure of 42.2%. Our results clearly show that our MaxEnt system easily outperforms these two simple baseline classifiers. We report macro-average numbers over all citation functions, except for NEUTRAL:Neutral, which simply reflects that a work is cited without any particular information. We observe that surface features do not work well enough alone, as they cannot generalize beyond the signal knowledge observed in a relatively small training set. Syntactic features, on the other hand, can utilize linguistic knowledge to solve the problem, and lead to better results. We compare F-measure of coarse-grained sentiment classification and fine-grained citation func- 5 . We see that coarsegrained classification performs only slightly better. We suspect that each citation function in the POSITIVE category needs different signal information to identify, so a more fine-grained annotation scheme could lead to a stronger correlation between a class label and its signals. This can explain the close performance between these two paradigms, although citation function prediction is more informative and harder. We report performance and distribution in annotated data for each citation function in Table 4. Note that the numbers in the ""Distribution"" column does not sum to 1, because we omit the NEUTRAL:Neutral category that does not carry information and some categories (e.g., Negative) that are too few (e.g., less than 5) in the corpus. We see that some of the functions (such as Discover) can perform much better than others. The major reason for the difference in performance is the imbalance distribution of citation functions in the annotated corpus, which, in turn, results in the difference in prediction ability of our classifier. In the extreme case, our system fails to find any positive instance for some of the categories because of the scarcity of training examples. In order to mitigate this problem, we plan to perform more function-specific annotation to obtain more data on current scarce functions. Conclusion In this paper, we introduced the task of citation sentiment analysis and citation function classification, which aims to analyze the fine-grained utility of citations in scientific documents. We described an annotation scheme to annotate citation functions in scientific papers into fine-grained categories. We presented our Maximum Entropybased system to automatically classify the citation functions, explored the advantages of different feature sets, and confirmed the necessity of using syntactic features in our task, obtaining 67% of final F-measure score. For future work, we plan to explore more features and perform more citation function-specific annotation for scarce functions in the current annotated corpus. Furthermore, we will also apply our annotation scheme and classification method in scientific literature from different domains, as well as investigate more elaborate machine learning models and techniques. Acknowledgement Supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior National Business Center contract number D11PC20154. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/NBC, or the U.S. Government. Approved for Public Release; Distribution Unlimited.",1329203,2351bf32623c4391c07d0cb3f532e32aa0aa2b63,30,https://aclanthology.org/R13-1052,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Li, Xiang  and
He, Yifan  and
Meyers, Adam  and
Grishman, Ralph",Towards Fine-grained Citation Function Classification,402--407,,,,,,,inproceedings,li-etal-2013-towards,,,"Sentiment Analysis, Stylistic Analysis, and Argument Mining"
34,W05-0824,"Thanks to the profusion of freely available tools, it recently became fairly easy to built a statistical machine translation (SMT) engine given a bitext. The expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another. We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT sharedtask.","Thanks to the profusion of freely available tools, it recently became fairly easy to built a statistical machine translation (SMT) engine given a bitext. The expectations we can have on the quality of such a system may however greatly vary from one pair of languages to another. We report on our experiments in building phrase-based translation engines for the four pairs of languages we had to consider for the SMT sharedtask. Introduction Machine translation is nowadays mature enough that it is possible without too much effort to devise automatically a statistical translation system from just a parallel corpus. This is possible thanks to the dissemination of valuable packages. The performance of such a system may however greatly vary from one pair of languages to another. Indeed, there is no free lunch for system developers, and if a black box approach can sometimes be good enough for some applications (we can surely accomplish translation gisting with the French-English and Spanish-English systems we developed during this exercice), making use of the output of such a system for, let's say, quality translation is another kettle of fish (especially in our case with the Finnish-English system we ended-up with). We devoted two weeks to the SMT shared task, the aim of which was precisely to see how well systems can do across different language families. We began with a core system which is described in the next section and from which we obtained baseline performances that we tried to improve upon. Since the French-and Spanish-English systems produced output that were comprehensible enough 1 , we focussed on the two languages whose translations were noticeably worse: German and Finnish. For German, we tried to move around words in order to mimic English word order; and we tried to split compound words. This is described in section 4. For the Finnish/English pair, we tried to decompose Finnish words into smaller substrings (see section 5). In parallel to that, we tried to smooth a phrasebased model (PBM) making use of WORDNET. We report on this experiment in section 3. We describe in section 6 the final setting of the systems we used for submitting translations and their official results as computed by the organizers. Finally, we conclude our two weeks of efforts in section 7. The core system We assembled up a phrase-based statistical engine by making use of freely available packages. The translation engine we used is the one suggested within the shared task: PHARAOH (Koehn, 2004) which control the decoder. For acquiring a PBM, we followed the approach described by Koehn et al. (2003) . In brief, we relied on a bi-directional word alignment of the training corpus to acquire the parameters of the model. We used the word alignment produced by Giza (Och and Ney, 2000) out of an IBM model 2. We did try to use the alignment produced with IBM model 4, but did not notice significant differences over our experiments; an observation consistent with the findings of Koehn et al. (2003) . Each parameter in a PBM can be scored in several ways. We considered its relative frequency as well as its IBM-model 1 score (where the transfer probabilities were taken from an IBM model 2 transfer table). The language model we used was the one provided within the shared task. We obtained baseline performances by tuning the engine on the top 500 sentences of the development corpus. Since we only had a few parameters to tune, we did it by sampling the parameter space uniformly. The best performance we obtained, i.e., the one which maximizes the BLEU metric as measured by the mteval script 2 is reported for each pair of languages in Table 1 . Smoothing PBMs with WORDNET Among the things we tried but which did not work well, we investigated whether smoothing the transfer table of an IBM model (2 in our case) with WORDNET would produce better estimates for rare words. We adapted an approach proposed by Cao et al. (2005) for an Information Retrieval task, and computed for any parameter (e i , f j ) be-longing to the original model the following approximation: ṗ(e i |f j ) ≈ e∈E p wn (e i |e) × p n (e|f j ) where E is the English vocabulary, p n designates the native distribution and p wn is the probability that two words in the English side are linked together. We estimated this distribution by cooccurrence counts over a large English corpus 3 . To avoid taking into account unrelated but cooccurring words, we used WORDNET to filter in only the co-occurrences of words that are in relation according to WORDNET. However, since many words are not listed in this resource, we had to smooth the bigram distribution, which we did by applying Katz smoothing (Katz, 1997) : p katz (e i |e) = ċ(e i ,e|W,L) P e j c(e j ,e|W,L) if c(e i , e|W, L) > 0 α(e)p katz (e i ) otherwise where ċ(a, b|W, L) is the good-turing discounted count of times two words a and b that are linked together by a WORDNET relation, co-occur in a window of 2 sentences. We used this smoothed model to score the parameters of our PBM instead of the native transfer table. The results were however disappointing for both the G-E and S-E translation directions we tested. One reason for that, may be that the English corpus we used for computing the co-occurrence counts is an out-of-domain corpus for the present task. Another possible explanation lies in the fact that we considered both synonymic and hyperonymic links in WORDNET; the latter kind of links potentially introducing too much noise for a translation task. The German-English task We identified two major problems with our approach when faced with this pair of languages. First, the tendency in German to put verbs at the end of a phrase happens to ruin our phrase acquisition process, which basically collects any box of aligned source and target adjacent words. This can be clearly seen in the alignment matrix of figure 1 where the verbal construction could clarify is translated by two very distant German words könnten and erläutern. Second, there are many compound words in German that greatly dilute the various counts embedded in the PBM Figure 1 : Bidirectional alignment matrix. A cross in this matrix designates an alignment valid in both directions, while the symbol indicates an uni-directional alignment (for has been aligned with einen, but not the other way round). Moving around German words For the first problem, we applied a memory-based approach to move around words in the German side in order to better synchronize word order in both languages. This involves, first, to learning transformation rules from the training corpus, second, transforming the German side of this corpus; then training a new translation model. The same set of rules is then applied to the German text to be translated. The transformation rules we learned concern a few (five in our case) verbal constructions that we expressed with regular expressions built on POS tags in the English side. Once the locus e v u of a pattern has been identified, a rule is collected whenever the following conditions apply: for each word e in the locus, there is a target word f which is aligned to e in both alignment directions; these target words when moved can lead to a diagonal going from the target word (l) associated to e u−1 to the target word r which is aligned to e v+1 . The rules we memorize are triplets (c, i, o) where c = (l, r) is the context of the locus and i and o are the input and output German word order (that is, the order in which the tokens are found, and the order in which they should be moved). For instance, in the example of Figure 1 , the Verb Verb pattern match the locus could clarify and the following rule is acquired: (sie einen, könnten erläutern, könnten erläutern), a paraphrase of which is: ""whenever you find (in this order) the word könnten and erläutern in a German sentence containing also (in this order) sie and einen, move könnten and erläutern between sie and einen. A set of 124 271 rules have been acquired this way from the training corpus (for a total of 157 970 occurrences). The most frequent rule acquired is (ich herrn, möchte danken, möchte danken), which will transform a sentence like ""ich möchte herrn wynn für seinen bericht danken."" into ""ich möchte danken herrn wynn für seinen bericht."". In practice, since this acquisition process does not involve any generalization step, only a few rules learnt really fire when applied to the test material. Also, we devised a fairly conservative way of applying the rules, which means that in practice, only 3.5% of the sentences of the test corpus where actually modified. The performance of this procedure as measured on the development set is reported in Table 2 . As simple as it is, this procedure yields a relative gain of 7% in BLEU. Given the crudeness of our approach, we consider this as an encouraging improvement. Compound splitting For the second problem, we segmented German words before training the translation models. Empirical methods for compound splitting applied to German have been studied by Koehn and Knight (2003) . They found that a simple splitting strategy based on the frequency of German words was the most efficient method of the ones they tested, when embedded in a phrase-based translation engine. Therefore, we applied such a strategy to split German words in our corpora. The results of this approach are shown in Table 2 . Note: Both the swapping strategy and the compound splitting yielded improvements in terms of BLEU score. Only after the deadline did we find time to train new models with a combination of both techniques; the results of which are reported in the last line of Table 2 . The Finnish-English task The worst performances were registered on the Finnish-English pair. This is due to the agglutinative nature of Finnish. We tried to segment the Finnish material into smaller units (substrings) by making use of the frequency of all Finnish substrings found in the training corpus. We maintained a suffix tree structure for that purpose. We proceeded by recursively finding the most promising splitting points in each Finnish token of C characters F C 1 by computing split(F C 1 ) where: split(F j i ) =    |F j i | if j − i < 2 max c∈[i+2,j−2] |F c i |× split(F j c+1 ) otherwise This approach yielded a significant degradation in performance that we still have to analyze. Submitted translations At the time of the deadline, the best translations we had were the baselines ones for all the language pairs, except for the German-English one where the moving of words ranked the best. This defined the configuration we submitted, whose results (as provided by the organizers) are reported in Table 3 Conclusion We found that, while comprehensible translations were produced for pairs of languages such as French-English and Spanish-English; things did not go as well for the German-English pair and especially not for the Finnish-English pair. We had a hard time improving our baseline performance in such a tight schedule and only managed to improve our German-English system. We were less lucky with other attempts we implemented, among them, the smoothing of a transfer table with WORDNET, and the segmentation of the Finnish corpus into smaller units.",9374715,177bfa5b71e3e4fff964c4c96f08d971885b41a9,3,https://aclanthology.org/W05-0824,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Langlais, Philippe  and
Cao, Guihong  and
Gotti, Fabrizio",{RALI}: {SMT} Shared Task System Description,137--140,,,,,,,inproceedings,langlais-etal-2005-rali,,,
35,C80-1091,,"length is discussed. The vocabulary growth is treated as a probabilistic process governed by the principle of ""the restriction of variety"" of lexics. Proceeding from the basic model of the vocabulary-text relation a formula with good descriptive power is constructed. The statistical fit and the possibilities of extrapolation beyond the limits of observable data are illustrated on the material of several languages belonging to different typological groups. by deducing the relation between V and N from some other important quantitative characteristics of text such as Zipf's law and Yule's distribution (Kalinin, Orlov) 3. The author underlines the importance of these conceptions for the theory of quantitative linguistics on the whole, but points out their insufficiency in solving some practical linguo-statistical problems where greater exactness and reliability are needed (style-statistical analysis, text attribution, extrapolation beyond the limits of observable data, etc.). i. There are a great number of attempts to construct an appropriate mathematical model which would express the dependence of the size of the size of vocabulary (V) on the size of text (N). This is not only of practical importance for the resolution of a series of problems in the automatic processing of texts, but it is also connected with the theoretical explanation of some important aspects of text generation. In practice one often makes use of various empirical formulae which describe the growth of vocabulary with sufficient precision in the case of concrete texts and languages l, though such formulae do not have any general significance. Of special interest are some ""complex"" models derived from theoretical considerations, e.g., by basing one*s considerations on the hypothesis about the lognormal distribution of words in a text (Carroll) 2 or 2. Instead of the ""complex"" mode/s a ""direct"" method is proposed where the relation between V and N is regarded as the primary component with its own immanent properties in the statistical organization of text. The relation between V and N has to be analyzed ua the background of some essential inner factors of text generation. The dynamics of vocabulary growth is considered as the result of the interaction of several linguistic and extra, linguistic factors which in an integral way are governed by the principle of ""the restriction of variety"" of lexics (an analogue of the principle of the decrease of entropy in self-regulating systems). The concept of the variety of lexics is defined as the relation between the size of vocabulary and the size of text in the form of V/N (type-token ratio, or coefficient of variety) or N/V (average frequency of word occurrences). The coefficient of variety is supposed to be correlated with the probabilistic process of choosing ""new"" (unused) and ""old"" (already used in the text) words at each stage of text generation. The steady decrease of the degree of variety V/N = p is attended by the increase of its counterpart: (N-V)/N-I-V/N= q (p, q-1), which can be interpreted as the ""pressure of recurrency"" of words in real texts (analogous to the concept of re- ties of extrapolation in both directions (from the beginning up to a text of about N = lO 7) has been verified on the basis of experimental material taken from several languages belonging to different typological groups (Estonian, Kazakh, Latvian, Russian, Polish, Czech, Rumanian, English). The function may be applied to the analysis of individual texts as well as composite homogeneous (similar) texts and the size of vocabulary (V) may be determined by counting either word forms of lexemes. (See Tables 1 and 2 .) This seems to corroborate the assumption about the existence of a universal law (presumably of phylogenetic origin) which governs the process of text formation on the quantitative level. Table i The empirical size (V) and the teoretical size (V') of vocabulary plotted against the length of the text (N). The formula: V"" = Ne -a(ln N) B a) Latvian newspapers (lexemes) 6 N V V"" 50000",17153466,a8d2a181ba74c8ae93347c733324bd8260142c24,4,https://aclanthology.org/C80-1091,,,1980,,{COLING} 1980 Volume 1: The 8th International Conference on Computational Linguistics,"Tuldava, Juhan",A Mathematical Model of the Vocabulary-Text Relation,,,,,,,,inproceedings,tuldava-1980-mathematical,,,
36,W16-4705,"In this paper, we propose a method of augmenting existing bilingual terminologies. Our method belongs to a ""generate and validate"" framework rather than extraction from corpora. Although many studies have proposed methods to find term translations or to augment terminology within a ""generate and validate"" framework, few has taken full advantage of the systematic nature of terminologies. A terminology of a domain represents the conceptual system of the domain fairly systematically, and we contend that making use of the systematicity fully will greatly contribute to the effective augmentation of terminologies. This paper proposes and evaluates a novel method to generate bilingual term candidates by using existing terminologies and delving into their systematicity. Experiments have shown that our method can generate much better term candidate pairs than the existing method and give improved performance for terminology augmentation.","In this paper, we propose a method of augmenting existing bilingual terminologies. Our method belongs to a ""generate and validate"" framework rather than extraction from corpora. Although many studies have proposed methods to find term translations or to augment terminology within a ""generate and validate"" framework, few has taken full advantage of the systematic nature of terminologies. A terminology of a domain represents the conceptual system of the domain fairly systematically, and we contend that making use of the systematicity fully will greatly contribute to the effective augmentation of terminologies. This paper proposes and evaluates a novel method to generate bilingual term candidates by using existing terminologies and delving into their systematicity. Experiments have shown that our method can generate much better term candidate pairs than the existing method and give improved performance for terminology augmentation. Introduction In this paper, we propose a new way of generating new bilingual multi-word term pairs for augmenting existing bilingual terminologies. There is growing demand for properly managed terminologies in many areas of society, e.g. in document authoring and management, in technical translation, in knowledge transfer and education, and in IR/NLP (Sager, 1990; Wright and Wright, 1997; Budin, 2008; Kockaert and Steurs, 2015) . With the constant introduction of new terms in many domains, timely augmentation and update of terminologies is critical for proper terminology management, and automatic assistance for this process is greatly needed (Kockaert and Steurs, 2015) . Many researchers have proposed various methods to augment terminologies automatically. As we will see in Section 2, these can be divided into two broad approaches, i.e. ""extraction from corpora"" approach and ""generate and validate"" approach. We focus on the latter approach, which fits better for augmenting or expanding existing terminologies, the task which is in strong demand in language industries but has not been much addressed from the NLP point of view. A term in a terminology of a domain represents a concept of that domain. Majority of terms are complex in most domains in most languages. These complex terms represent concepts analytically, with each constituent element representing an important feature of the concept. A terminology, i.e. the set of terms of a domain, represents the structure of concepts of that domain more or less systematically. Although the extent of systematicity differ from language to language and from domain to domain, new terms are generally formed systematically within the conceptual system of the domain. If we can take into account this aspect of term formation for generating term candidates in the task of augmenting terminologies, we would be able to develop an effective way of help augmenting existing terminologies. Against this backdrop, this paper proposes a new method of generating bilingual term candidates by taking advantage of the structural feature of terminology. The basic idea is as follows: define terminological network that reflects conceptual systematicity; identify ""motivated"" subnetworks within which term formation is supposed to be activated, and generate term candidates for each subnetwork. The rest of this paper is organised as follows. Section 2 looks at related work and places the present work in context. Section 3 explains our proposed method. Sections 4 and 5 introduce the experimental setup and the results, respectively. Section 6 summarises the results and discusses remaining issues. Related work Automatic extraction/augmentation of bilingual terms/terminology Bilingual term extraction from parallel or comparable corpora has been actively pursued since the 1990s (Dagan and Church, 1997; Fung and Mckeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Laroche and Langlais, 2010) , most of which use contextual information such as co-occurrence within aligned segments or contextual similarity. Research into the improvement of quality of corpora is also pursued (Morin et al., 2010; Li and Gaussier, 2010) . The European project TTC (Terminology extracting Translation Tools and Comparable Corpora) is the culmination of this trend of research (Blancafort et al., 2010) . Some use the correspondence at the level of constituent elements of terms in finding term translations (Grefenstette, 1999; Tonoike et al., 2005; Tonoike et al., 2006; Daille and Morin, 2008) , i.e. they generate term candidates in target language by translating constituent elements and validate their existence. These studies partly adopt the ""generate and validate"" framework. Sato et al. (2013) generated multi-word term pairs as bilingual term candidates by considering all possible pairs of constituent elements of terms in a terminology. The generated pairs are then validated by using web documents. Our method adopts this ""generate and validate"" framework. More specifically, we take Sato et al. (2013) as a point of departure as the aim of this work is the same as the present work, i.e. extending existing bilingual terminologies. The method proposed by Sato et al. (2013) takes advantage of a general tendency that if one term is a compound, a part of the term is a term and a part of the term can be changed. For example, if a terminological lexicon contains, ""linear programming"", ""linear optimization"", ""linear function"", ""convex programming"" and ""convex function"", they can expect that the term ""convex optimization"" exists, even if this term is not listed in the lexicon. They generate term candidates consisting of two constituents by defining head-modifier bipartite graph and interpolate missing edges. Figure 1 shows this idea graphically. The problems we identify with their method are (a) if applied straightforwardly, a huge number of bilingual term candidates are generated, and (b) the Kernighan-Lin algorithm they adopted (Kernighan and Lin, 1970) to partition head-modifier bipartite graph in order to reduce term candidates does not reflect systematic structure of terminologies. Following theoretical research in terminology (Sager, 1990; Kageura, 2002) , we understand that new terms are formed within the conceptual-terminological subsystem surrounding the new concepts. So our main task is concerned with consolidating these subsystems consisting of tightly-related or ""motivated"" terms/concepts within which new terms are formed. Structural nature of terminology Terminologies in most languages contain a substantial number of complex terms (Cerbah, 2000; Nomura and Ishii, 1989 ). Research has shown that complex terms tend to show conceptual relationships systematically, with each constituent element representing an important feature of concepts represented by terms (Felber, 1984; Sager, 1990; Kageura, 2002) . 2012 ) examined the systematic nature of terminologies by introducing terminological network, the vertices of which are terms and the edges of which consist of common constituent elements between terms. For instance, a putative terminology consisting of six terms, ""information"", ""information retrieval"", ""information extraction"", ""document retrieval"", ""document processing"", and ""information processing"" makes a network as shown in Figure 2 . Kageura and Abekawa (2007) applied partitive clustering over the terminological network to obtain sub-groups of terminologies. Asaishi and Kageura (2011) comparatively analysed the formal nature of terminological structure by defining terminological networks of English and Japanese bilingual terminologies of several domains. Iwai et al. (2016) have shown that there is a reasonable amount of cross-lingual correspondence between sub-groups of English and Japanese terms identified by using community detection algorithms over the terminological network. As stated above, to identify conceptual subsystems consisting of closely-related concepts in terminology constitutes an essential part of our method. Iwai et al. (2016) showed that meaningful conceptual subsystems can be identified and extracted by applying relevant network partition algorithms to terminological networks. Method of term candidate generation Starting from a given terminology, the method of term candidate generation we propose consists of two steps: 1. Dividing a terminology into subgroups each of which consists of terms representing closely related concepts; and 2. Generating bilingual term candidates by generating possible combinations of constituent elements of terms included in each subgroup. Figure 3 shows an outline of the method. While Sato et al. (2013) firstly considered all possible head-modifier pairs for all terms in terminology and then reduced the number of term candidates by applying Kernighan-Lin algorithm to the head-modifier bipartite graph, our method first consolidate subgroups of terms and generate term candidates for each subgroup separately. Note that this is not just a methodological alternative, but reflects theoretical understanding of how new terms are formed, as stated above. Identifying ""motivated"" sub-groups of terms We first construct terminological networks (Kageura, 2012) , and then apply partitive clustering or community detection algorithm to the network. This manipulation identifies motivated sub-groups of terms within a given terminology. As terms are formed within subsystems of concepts, this serves for reducing the number of generated term candidates while at the same time increasing the plausibility of candidates. After dividing terminological networks into sub-groups or clusters, we generated a head-modifier set for each cluster. The steps for this process are as follows: 1. Decompose each term into its constituent elements; 2. Generate terminological network with terms as vertices and common constituent elements as edges; 3. Divide the generated terminological network into clusters using a community detection algorithm. For step 1, we used MeCab 1 with UniDic 2 to decompose Japanese terms into constituent elements. For English terms, we decompose terms using spaces and other punctuations and then apply stemming and lemmatisation of constituent words using a lemmatiser 3 . Although POS taggers, such as Stanford POS Tagger 4 , are widely used for pre-processing English sentences or phrases, we used here the lemmatiser because (a) our aim is to extract semantically identical units by removing inflectional (and sometimes derivational) variations and (b) we do not need POS-information. Previous work has shown that approximately matching units can be extracted for English and Japanese terminologies by applying these pre-processing steps (Asaishi and Kageura, 2011) . For step 2, we used python igraph library 5 to generate terminology networks for English and Japanese. We removed functional words (symbols, numbers, prepositions and articles for English; symbols, numbers, particles and auxiliary verbs for Japanese) as they do not represent conceptual characteristics. For step 3, we adopted Potts spin glass algorithm to divide the terminology networks into clusters. Many community detection algorithms have been proposed (Clauset et al., 2004; Rosvall and Bergstrom, 2008; Raghavan et al., 2007; Blondel et al., 2008; Pons and Latapy, 2006; Newman, 2006) . After examining several commonly used methods, we decided to adopt Potts spinglass-based method (Reichardt and Bornholdt, 2006) , which works by solving the global optimization problem (Kirkpatrick, 1984) . Not only is this method reported to work well in several experiments, the underlying concept reflects nicely the task of extracting motivated sub-groups of terminologies (Kageura and Abekawa, 2007) . Generating bilingual term candidates After obtaining clusters on sub-groups of terms, we generated bilingual term candidates as follows: 1. Identify corresponding English and Japanese terms contained in each cluster. As English and Japanese clusters do not match completely (Iwai et al., 2016) , we generated term candidate pairs in three different ways in step 1: (a) based on Japanese clusters (Japanese), (b) based on English clusters (English), and (c) based on the intersections of Japanese and English clusters (mix). 2. Generate bilingual pairs of constituent elements (henceforth constituent pairs). This is carried out first by identifying single-word term pairs and then subtracting them from multi-word terms and making remaining elements as pairs recursively. 3. Generate head-modifier pairs for constituent elements of source language terms, as shown in Figure 4 . We identify head-modifier relations by identifying constituents on the left as modifiers and on the right as heads, as English (and Japanese, for that matter) complex terms are head final. We also assumed that if a term constitutes more than three words, two constituent elements can replace as one semantics unit. For example, we can consider that ""data"" is the modifier and ""processing 4 . However, we considered only head-modifier pairs by minimum unit in this time. We set English as source language for convenience of processing; there is no inherent technical reason for us to make the process directional in terms of languages. 4. Generate a bipartite graph based on the head-modifier pairs of the source language, as shown in Figure 5 . 5. Take the direct product of the head and modifier vertices to generate extended head-modifier pairs from that bipartite graph. 6. Create new bilingual term pairs by taking translations for each constituent elements of the headmodifier pairs using constituent pairs. The candidate term pairs generated through this process are then validated using web documents. 4 Experimental setup Seed terminologies For evaluation, we used two terminological dictionaries, i.e. one in the field of computer science (Aiso, 1993) and the other in the field of economics (Yuhikaku, 1986) . These are two of the five terminological dictionaries used in Sato et al. (2013) . Table 1 shows the number and ratio of terms by length in each terminology, i.e. single terms, terms with two constituents, terms with three constituents and terms with four or more constituents. ""Dom."" stands for domain, ""Lang."" stands for language, and ""T"" indicates the number of terms. From Table 1 , we can observe that these terminologies contain many complex terms. Terminological network and candidate generation We constructed terminological networks for English and for Japanese separately for these two datasets. Table 2 shows the quantitative nature of the terminological networks, in which N stands for the number of constituent elements, V the number of vertices, E the numbers of edges, and S the number of isolated terms. We can observe that each network consists of a single giant component (max subgraph) and several small components (others) including isolated vertices. We then extracted max subgraph and divided it into clusters. The number of clusters was set in two ways, i.e. 25 and 10. These numbers were decided heuristically, referring to the number of subdomains listed in handbooks and in academic societies. The number of candidates generated from these clusters is given in Table 3 , which also provides the number of candidates generated from the method by Sato et al. (2013) . Note that our method produces smaller number of term candidates. Collecting web documents for validation Web documents are collected separately for two languages and stored in a database. To avoid collecting irrelevant web pages, we used domain keywords (the name of the domain such as ""computer science"") together with individual terms for collecting documents. Web documents for computer science were collected in October and November 2014, by using terms and the domain keywords ""computer science"" (English) and ""情報科学"" (""information science"" for Japanese) (see 3.1). Web documents for economics were collected at the end of December 2014, with domain keywords ""economics"" (English) and ""経済学"" (""economics"" for Japanese). Table 4 shows the basic quantities of the collected documents. We extracted 200 pages randomly from the English data and manually checked the number of technical documents. The result is shown in Table 5 . Approximately 60 % of the documents were technical in both domains. Evaluation We evaluated our method in two ways. First, we compared our result with Sato et al. (2013) in terms of the number of retained candidates after validation. Second, to evaluate precision, we extracted top 100 candidates ranked according to (a) the sum of English and Japanese occurrences and (b) the Jaccard coefficient. Note that we do not make comparison between our approach and the approach of extracting terms from corpora, because their experimental setups are very different to each other. Comparison of the number of retained candidates after validation The candidate term pairs generated in six different ways (two cluster sizes of 10 and 25 by based on Japanese clusters, based on English clusters, and based on the intersections of Japanese and English clusters) were validated by 2 steps using the web documents (see 4.3). 1. Searching bilingual term candidates from collected web documents and retaining candidate pairs of which both English part and Japanese part occur at least once in the documents. 2. Calculating a Jaccard coefficient by using retained candidate pairs. In step 1, instead of using the web search directly, we first pool the web documents relevant to the two domain. It is to avoid repeatedly searching the web for every candidate pairs. In step 1, we validate English and Japanese terms separately, as we can assume that the candidates are aligned. However, it is still useful to validate the bilingual co-occurrences in the web documents. In order to observe that, we used Jaccard coefficient. Table 6 : The result of validation (filtering) Filtering by using collected web documents We first did the filtering by using collected web documents to reduce the number of generated bilingual term candidates. Candidate pairs of which both English part and Japanese part occur at least once in the corpus were retained as validated terms. Table 6 shows the result. The first line in each domain shows the number of validated candidates. The second line shows their percentage against the number of candidate pairs given in Table 3 . It shows that the number of terms retained after validation is generally larger in our methods than Sato et al. (2013) , with exceptions (""mix"" for 10 clusters, and ""Ja"" and ""mix"" for 25 clusters in economics). In all cases, the ratio of retained candidates is much higher in our method than Sato et al. (2013) . These results indicate that our proposed method: • performs both more effectively in terms of computational cost and in terms of recall, assuming that the validated terms have roughly the same level of pairing precision and termhood precision; and • enables us to control the balance between recall and precision, by changing the number of clusters as well as the pairing methods. The first point indicates that our method successfully captures the conceptual subsystems/terminological subgroups within the dynamics of which new terms are formed. The second point shows that our method gives us applicational flexibility. Calculating Jaccard coefficient After filtering by collected web documents, we searched retained bilingual term candidates with search engine and calculated Jaccard coefficient by using the number of hit. In order to keep the comparison with Sato et al. ( 2013 ) sensible, we chose the validated candidates generated from ""mix"" for 25 clusters, as the number of validated terms in the two domains is close to that by Sato et al. (2013) (although ""Ja"" pairing for 10 clusters is the closest in economics, we chose the same setting for the two domains). Jacard coefficient is defined as: Jaccard(L1, L2) = H(L1) ∧ H(L2) H(L1) ∨ H(L2) = H(L1) ∧ H(L2) H(L1) + H(L2) − H(L1 ∧ L2) , where L1 and L2 indicate English and Japanese parts (or vice versa) of a candidate pair in our case, and H(x) is the number of documents in which they occur. If the number of hits is zero, the Jaccard coefficient is defined to be zero. In filtering by using collected web documents, the process retained candidate pairs that either English part or Japanese part occur. Therefore, it is considered that nonparallel candidate pairs are retained. By calculating Jaccard coefficient with the number of hit in search engine and retaining candidate pairs that Jaccard coefficient is positive, we finally extract candidate pairs that is validated parallel. We used Bing search API as search engine. Precision of top 100 candidates The top 100 candidates generated by ""mix"" for 25 clusters, ranked according to the sum of English and Japanese occurrences and to the Jaccard coefficient, were manually evaluated for each domain. The evaluation was carried out from two points of view, i.e. (a)whether the Japanese and English matches or not (pairing), and (b)whether the Japanese candidates can be regarded as a term in the domain in question (term). For (b), we also counted partial-terms (partial). The evaluation was carried out by one of the authors. Table 8 shows the result, together with the corresponding results given in Sato et al. (2013) (in bracket). Table 8 shows that except for ""pairing"" by Jaccard in computer science, our method is consistently better than Sato et al. (2013) in terms of precision as well. Conclusion and future work In this paper we proposed a method of augmenting existing bilingual terminological lexicon. We introduced a way of generating candidate term pairs which reflect the conceptual system/terminological group within which new terms are formed, by taking advantage of the ""motivated"" structure of terminologies. Compared with the method proposed so far, our method consistently shows higher performance, which indicates that our method succeeded in identifying, to a reasonable extent, the conceptual subsystem/terminological subgroups within which terms are formed. The method also has more applicational flexibility. We are currently addressing the following issues: • Extending our method so that it can generate and validate terms with more than three constituent elements. For example, if a term consists of more than three words, it is natural to decompose it into 2 words as one unit and the other one word from the point of semantic structure. In this way, we try to apply generating bilingual term candidates that consists of more than three words. • Improving the pairing module. As of now, we examined English as source language and Japanese as target language. However, we can consider reverse pattern in our proposed method. Directional property of language and correspondence of translation words are one of the points of that we need to address in the future. • Analysing non-validated candidates (error analysis). Now that it was shown that the proposed method can capture, to a reasonable extent, conceptual subsystem within which new terms are generated, it is important to analyse non-validated candidates to obtain further insights into candidate generation process. • Finding a way of suggesting reasonable number of clusters. As can be inferred from Tables 4 and  7 , the best number of clusters may differ from domain to domain. In addition, we are planning to extend our research into the following directions: • Applying our method to different language pairs. We are planning to apply our method to Chinese-English and Korean-English pairs. • Clarifying the difference between the ""generate and validate"" framework and extraction from parallel or comparable corpora. Although the comparison of these two approaches are difficult, because not only the theoretical assumption and the range of relevant applications but also the range of data which can be used differ greatly (the ""generate and validate"" approach in general can use wider variety of data as they are used for validation rather than sources from which terms are extracted), it would still be interesting to examine the relationship between these two approaches on the empirical basis. Acknowledgements This work is partly supported by JSPS Grant-in-Aid (A) 25240051 ""Archiving and using translation knowledge to construct collaborative translation training aid system.""",18622276,b74bf37df909d2baf4b14f69a953aa5ccc2a9386,3,https://aclanthology.org/W16-4705,The COLING 2016 Organizing Committee,"Osaka, Japan",2016,December,Proceedings of the 5th International Workshop on Computational Terminology (Computerm2016),"Iwai, Miki  and
Takeuchi, Koichi  and
Kageura, Kyo  and
Ishibashi, Kazuya",A Method of Augmenting Bilingual Terminology by Taking Advantage of the Conceptual Systematicity of Terminologies,30--40,,,,,,,inproceedings,iwai-etal-2016-method,,,
37,R13-1054,"Many sentiment-analysis methods for the classification of reviews use training and test-data based on star ratings provided by reviewers. However, when reading reviews it appears that the reviewers' ratings do not always give an accurate measure of the sentiment of the review. We performed an annotation study which showed that reader perceptions can also be expressed in ratings in a reliable way and that they are closer to the text than the reviewer ratings. Moreover, we applied two common sentiment-analysis techniques and evaluated them on both reader and reviewer ratings. We come to the conclusion that it would be better to train models on reader ratings, rather than on reviewer ratings (as is usually done).","Many sentiment-analysis methods for the classification of reviews use training and test-data based on star ratings provided by reviewers. However, when reading reviews it appears that the reviewers' ratings do not always give an accurate measure of the sentiment of the review. We performed an annotation study which showed that reader perceptions can also be expressed in ratings in a reliable way and that they are closer to the text than the reviewer ratings. Moreover, we applied two common sentiment-analysis techniques and evaluated them on both reader and reviewer ratings. We come to the conclusion that it would be better to train models on reader ratings, rather than on reviewer ratings (as is usually done). Introduction There is a growing volume of product reviews on the web which help customers to make decisions when planning to travel or buying a product. Sentiment-analysis tools try to discover user opinions in these reviews by converting the text to numerical ratings. Building these tools requires a large set of annotated data to train the classifiers. Most developers compile a training and test corpus by collecting reviews from web sites on which customers post their reviews and give a star rating. They test and train their tools against these reviewer ratings assuming that they are an accurate measure of the sentiment of the review. However, when reading reviews and comparing them with the reviewer ratings there does not always seem to be a clear and consistent relation between these ratings and the text (cf. also Carrillo de Albornoz et al., 2011) . That is, from a reader's perspective, there is a discrepancy between what the reviewer expresses with the numerical rating and what is expressed in text. For example, the following hotel review was rated '7' (weakly positive), whereas possible guests probably would not go to the hotel after having read the review. The hotel seems rather outdated. The breakfast room is just not big enough to cope with the Sunday-morning crowds. This mismatch between the reviewer's rating and the review's sentiment may lead to problems. For example, reviews are often ranked according to their reviewer's ratings from highly positive to highly negative. If the review text is not in accordance with its ranking, the rankings may become ineffective. In the area of sentiment analysis and opinion mining the mismatch may lead to methodological problems. Testing and training of sentimentanalysis tools on reviewer ratings may lead to the wrong results if the mismatch between the ratings and the text proves to be a common phenomenon. We assume that one of the most important sources of this mismatch is the fact that the reviewer writes the review and, separately, rates the experience (i.e., with the book he read, with the hotel he stayed at, with a product he bought). Of course, both text and rating are based on the same experience but they do not necessarily express the same aspects of it. If we have a closer look at the hotel review above, the reviewer probably rates the hotel with a '7', because there may be some positive aspects which he does not mention in his review. We hypothesize that reader ratings which express the reader's perceptions of the sentiment of a text are a good alternative. As the reader's judgment is based solely on the text of the review, we assume that its rating is closer to the sentiment of the text than the reviewer's rating. In this study we investigate whether the observed mismatch between reviewer rating and the sentiment of the review is a common and whether reader ratings could be a more reliable measure of this sentiment than reviewer ratings. The next section presents related work. In section 3, the reliability of reviewer and reader ratings as a measure of a review's sentiment is further investigated by performing an annotation study. In section 4, we study the effect of the different types of ratings on the performance of two widely used sentiment-analysis techniques. Finally, we conclude with a discussion of our findings. Related Work There is a large body of work concerning sentiment analysis of customer reviews (Liu, 2012) . Most of these studies regard sentiment analysis as a classification problem and apply supervised learning methods where the positive and negative classes are determined by reviewer ratings. Studies propose additional annotations only when focusing on novel information which is not reflected in the user ratings (Toprak et al., 2010, Ando and Ishizaki, 2012) . The issue of a possible mismatch between reviewer ratings and review text is usually not addressed. Much attention is paid to the customer's (or reader's) perspective in studies in the area of business and social science. Mahony et al. (2010) and Ghose et al. (2012) study product reviews in relation to customer behavior. Their aim is to identify reviews which are considered helpful to customers and to know what kind of reviews affect sales. Their work is similar to ours because of the focus on the effect of the review text on the customer/reader, but they also include other types of information such as transaction data, consumer browser behavior and customer preferences. However, none of these studies focus on the relationship between reviewer rating and review text. As far as we know, Carrillo de Albornoz et al. ( 2011 ) is the only study which mentions the mismatch between rating and text. They ignore reviewer ratings and employ a new set of ratings for the training and testing of their system. From their work, however, it is neither clear to what extent the new ratings differ from the user ratings as they do not report inter-annotator agreement scores nor what the effect is of the different ratings on classifier performance. Reviewer and reader annotations To get a better understanding of the relationship between reviewer ratings, review text and reader ratings, we perform an annotation study which allows us to answer the following research questions: (1) To what extent are mismatches between reviewers' ratings and sentiments common? And (2) Can reader ratings be employed to measure review sentiment more reliably? Hotel review corpus For the annotation study we compiled a corpus of Dutch hotel reviews. The corpus consists of 1,171 reviews extracted from four different booking sites during the period 2010-2012. The reviews have been collected in such a way that they are evenly distributed among the following categories:  They are collected from different booking site like Tripadvisor.com, zoover.com, hotelnl.com and booking.com  They include most frequent text formats: pro-con (where boxes are provided for positive and negative remarks) and free text.  They include reviews on hotels from all over the world (although the majority is Dutch).  They include reviewer's ratings ranging from strong negative to strong positive Each review contains the following information:  Reviewer rating: a user rating given by the reviewer translated to a scale ranging from 0 to 10 (very negative to very positive) describing the overall opinion of the hotel customer.  Review text: a brief text describing the reviewer's opinion of the hotel.  Reader ratings: ratings of two readers on a scale ranging from 0 to 10. These ratings are described in more detail in the next section. Reader ratings and agreement scores Two annotators (R1 and R2), both native speakers of Dutch and with no linguistic background, added a reader rating to each review. They were asked to read the review and rate the text on a scale from 1-10 (very negative to very positive), answering the question whether the reviewer would advise them to choose the hotel, or not. They were asked to ignore their own preferences as much as possible. We measured the Pearson Correlation Coefficient (r) between the 10-point numerical rating scales of each annotator pair (R1, R2 and reviewer), regarding the reviewer (REV) also as an annotator. As correlation can be high without necessarily high agreement on absolute values, we also performed evaluations on categorical values. A 2-class evaluation was performed by translating 1 to 5 ratings to 'positive' and 6 to10 ratings to 'negative'; a 4class evaluation is performed by translating 1-3 ratings to 'strong negative', 4 to 5 ratings to 'weak negative', 6 to 7 ratings to 'weak positive' and 8 to 10 to 'strong positive'. Agreement was measured between each annotator pair in terms of percentage of agreement (%) and kappa agreement (κ). raters 1/10 2-class 4-class REV-R1 0.82 r 0.81 κ 0.90% 0.51 κ 0.63% REV-R2 0.83 r 0.82 κ 0.91% 0.53 κ 0.65% R1-R2 0.92 r 0.92 κ 0.96% 0.71 κ 0.78% Table 1 . Inter-annotator agreement. Table (1) shows that inter-annotator agreement is quite high between all raters, both when correlation is measured on the 10-point-scale (r >= 0.82) and when agreement is measured with the 2-class annotation sets (κ >= 0.81). Agreement on the 4 class annotations is much lower (κ >= 0.51) showing that polarity strength is difficult to annotate. However, given the purpose of this study, we are not interested in agreement as such. Our focus is on the differences in agreement between readers and reviewers. From that perspective it is interesting to note that, according to all measures, the reviewer is an outlier. Agreement between each individual reader and the reviewer (REV-R1 and REV-R2, respectively) is consistently lower than agreement between both readers (R1-R2). The differences already become important when measuring agreement on 2-class annotations, but even more prominent when measuring agreement on 4-class annotations. All observed differences ranging from 5 up to 15%, are statistically significant (p < 0.01). On the basis of these results, we can answer our research questions (cf. section 3). We infer that the observed mismatch between the sentiment of the review and reviewer rating is a relatively common phenomenon. With respect to at least 10% (cf. table 1, row 2, column 4) of the reviews (when reviews are categorized in 2 categories) up to approx. 37% (cf. table 1, row 1, column 6) of the reviews (when reviews are categorized in more fine-grained categories) readers do not agree with the reviewer. Secondly, the fact that readers have higher agreement with each other than with the reviewer confirms our hypothesis that reader ratings are a more accurate measure of the review's sentiment than reviewer ratings. Implications for sentiment analysis We investigated how automated sentiment analysis methods perform with the different sets of annotations by applying two widely used approaches to document-level sentiment classification. Classifier accuracy is measured against the three sets of ratings (R1, R2 and REV) we described in the previous section. The lexicon-based approach The first method is a lexicon-based approach which starts from a text which is lemmatized with the Dutch Alpino-parser 1 .The approach is similar to the ""vote-flip-algorithm"" proposed by Choi and Cardie (2008) . The intuition about this algorithm is simple: for each review the number of matched positive and negative words from the sentiment lexicon are counted. If polar words are preceded by a negator, their polarity is flipped; if polar words are preceded by an intensifier, their polarity is doubled. We then assign the majority polarity to the review. In the case of a tie (being zero or higher than zero), we assign neutral polarity. The sentiment lexicon used in this approach is an automatically derived general language sentiment lexicon obtained by WordNet propagation (Maks and Vossen, 2011) . The machine-learning approach The second method is a machine learning approach that also starts from a text that is lemmatized by the Dutch Alpino-parser. After lemmatization the text is transformed to a word-vector representation by applying Weka's StringToWord Vector with frequency representation (instead of binary). We used Weka's NaiveBayesMultinominal (NBM) classifier to classify the reviews. The NBM was chosen because our review texts are rather short (with an average of 68 words) and, according to Wang and Manning (2012) , NBM classifiers perform well on short snippets of Results on different types of ratings Results are evaluated against the whole set of 1,172 reviews (cf. table 2 'all'). As many approaches to sentiment analysis do not use the class of weak sentiment (Liu, 2012) , we also evaluated against a subset of strong negative (ratings 1 to 3) and strong positive (ratings 8 to 10) reviews (cf. table 2, 'strong'). Table ( 2) shows the classification results in terms of accuracy, obtained by the lexicon-based approach (LBA, row 1, 2, 3) and the machinelearning approach (NBM, row 4, 5, 6) . The results show that both approaches perform well against all ratings. Classification of the strong sentiment reviews seems considerably easier than classification of the whole review set. Interestingly, both sentiment analysis approaches appear to perform better on reader ratings than on reviewer ratings. The better performance holds across both selections of reviews and with both approaches. Differences are statistically significant (chi-square test, p<0.05) in all cases but the LBA approach on the whole dataset which is almost statistically significant. Discussion and Conclusions We performed an annotation study that showed that the observed mismatch between reviewer ratings and review's sentiment is a rather frequent phenomenon. Considerable part of the reviews (ranging from 9 to 37% depending on the granularity of the classification) is classi-fied by the reviewer in the wrong sentiment class. The annotation study also showed that reader ratings are a more accurate measure. We already expected reader ratings to be closer to the text because they are exclusively based on it. In addition, the annotation study shows that readers agree in their ratings and that the review's sentiment can be reliably annotated by readers. Our experiments in section 4 show that sentiment-analysis tools perform better with reader ratings than with reviewer ratings. This should probably not surprise us as sentiment analysis behaves like a reader whose only source of information is the review text. As such, this is a promising result. However, since reviewer ratings are widely available and come for free with the text, they will often be used to evaluate the tools. Likewise, training and fine-tuning will be done with reviewer ratings rather than with reader ratings. We think that researchers and system developers should be aware of the differences between reviewer and reader ratings and their effects on the system they develop. Recently, many sentiment analysis tools perform a more in-depth analysis identifying aspects of products (and services) and their sentiments (Liu, 2012) . Again, reviewer ratings are used to train and test these systems. In view of our findings, it seems advisable that researchers and system developers make the effort to collect a set of reader ratings and train and test their tools with them. The additional value of sentiment analysis should be sought in finding the sentiment of the text rather than in finding the sentiment of its writer. Acknowledgements This research is supported by the European Unions 7 th Framework Programme via the OpeNER (Open polarity enhanced Named Entity Recognition) Project (ICT-296451).",7036291,57ac89edb6a460880b4bb400e60e593f0271e88e,22,https://aclanthology.org/R13-1054,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Maks, Isa  and
Vossen, Piek",Sentiment Analysis of Reviews: Should we analyze writer intentions or reader perceptions?,415--419,,,,,,,inproceedings,maks-vossen-2013-sentiment,,,"Sentiment Analysis, Stylistic Analysis, and Argument Mining"
38,W05-0827,"Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). In this paper we study different improvements to the standard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" (ACL Workshop on Parallel Texts 2005).","Nowadays, most of the statistical translation systems are based on phrases (i.e. groups of words). In this paper we study different improvements to the standard phrase-based translation system. We describe a modified method for the phrase extraction which deals with larger phrases while keeping a reasonable number of phrases. We also propose additional features which lead to a clear improvement in the performance of the translation. We present results with the EuroParl task in the direction Spanish to English and results from the evaluation of the shared task ""Exploiting Parallel Texts for Statistical Machine Translation"" (ACL Workshop on Parallel Texts 2005). Introduction Statistical Machine Translation (SMT) is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which has to be learned from a bilingual text corpus. Thus, the translation of a source sentence f can be formulated as the search of the target sentence e that maximizes the translation probability P (e|f ), ẽ = argmax e P (e|f ) (1) If we use Bayes rule to reformulate the translation probability, we obtain, ẽ = argmax e P (f |e)P (e) (2) This translation model is known as the sourcechannel approach [1] and it consists on a language model P (e) and a separate translation model P (f |e) [5] . In the last few years, new systems tend to use sequences of words, commonly called phrases [8] , aiming at introducing word context in the translation model. As alternative to the source-channel approach the decision rule can be modeled through a log-linear maximum entropy framework. The features functions, h m , are the system models (translation model, language model and others) and weigths, λ i , are typically optimized to maximize a scoring function. It is derived from the Maximum Entropy approach suggested by [13] [14] for a natural language understanding task. It has the advantatge that additional features functions can be easily integrated in the overall system. This paper addresses a modification of the phrase-extraction algorythm in [11] . It also combines several interesting features and it reports an important improvement from the baseline. It is organized as follows. Section 2 introduces the baseline; the following section explains the modification in the phrase extraction; section 4 shows the different features which have been taken into account; section 5 presents the evaluation framework; and the final section shows some conclusions on the experiments in the paper and on the results in the shared task. Baseline The baseline is based on the source-channel approach, and it is composed of the following models which later will be combined in the decoder. The Translation Model. It is based on bilingual phrases, where a bilingual phrase (BP ) is simply two monolingual phrases (M P ) in which each one is supposed to be the translation of each other. A monolingual phrase is a sequence of words. Therefore, the basic idea of phrase-based translation is to segment the given source sentence into phrases, then translate each phrase and finally compose the target sentence from these phrase translations [17] . During training, the system has to learn a dictionary of phrases. We begin by aligning the training corpus using GIZA++ [6] , which is done in both translation directions. We take the union of both alignments to obtain a symmetrized word alignment matrix. This alignment matrix is the starting point for the phrase based extraction. Next, we define the criterion to extract the set of BP of the sentence pair (f j2 j1 ; e i2 i1 ) and the alignment matrix A ⊆ J * I, which is identical to the alignment criterion described in [11] . BP (f J 1 , e I 1 , A) = {(f j2 j1 , e i2 i1 ) : ∀(j, i) A : j 1 ≤ j ≤ j 2 ↔ i 1 ≤ i ≤ i2 ∧∃(j, i) A : j 1 ≤ j ≤ j 2 ∧ i 1 ≤ i ≤ i2} The set of BP is consistent with the alignment and consists of all BP pairs where all words within the foreign language phrase are only aligned to the words of the English language phrase and viceversa. At least one word in the foreign language phrase has to be aligned with at least one word of the English language. Finally, the algorithm takes into account possibly unaligned words at the boundaries of the foreign or English language phrases. The target language model. It is combined with the translation probability as showed in equation (2) . It gives coherence to the target text obtained by the concatenated phrases. Phrase Extraction Motivation. The length of a M P is defined as its number of words. The length of a BP is the greatest of the lengths of its M P . As we are working with a huge amount of data (see corpus statistics), it is unfeasible to build a dictionary with all the phrases longer than length 4. Moreover, the huge increase in computational and storage cost of including longer phrases does not provide a significant improve in quality [8] . X-length In our system we considered two length limits. We first extract all the phrases of length 3 or less. Then, we also add phrases up to length 5 if they cannot be generated by smaller phrases. Empirically, we chose 5, as the probability of reappearence of larger phrases decreases. Basically, we select additional phrases with source words that otherwise would be missed because of cross or long alignments. For example, from the following sentence, Cuando el Parlamento Europeo , que tan frecuentemente insiste en los derechos de los trabajadores y en la debida protección social , (...) NULL ( ) When ( 1 ) the ( 2 ) European ( 4 ) Parliament ( 3 4 ) , ( 5 ) that ( 6 ) so ( 7 ) frequently ( 8 ) insists ( 9 ) on ( 10 ) workers ( 11 15 ) ' ( 14 ) rights ( 12 ) and ( 16 ) proper ( 19 ) social ( 21 ) protection ( 20 ) , ( 22 ) (...) where the number inside the clauses is the aligned word(s). And the phrase that we are looking for is the following one. los derechos de los trabajadores # workers ' rights which only could appear in the case the maximum length was 5. Phrase ranking Conditional probability P (f |e) Given the collected phrase pairs, we estimated the phrase translation probability distribution by relative frecuency. P (f |e) = N (f, e) N (e) (4) where N(f,e) means the number of times the phrase f is translated by e. If a phrase e has N > 1 possible translations, then each one contributes as 1/N [17] . Note that no smoothing is performed, which may cause an overestimation of the probability of rare phrases. This is specially harmful given a BP where the source part has a big frecuency of appearence but the target part appears rarely. For example, from our database we can extract the following BP : ""you # la que no"", where the English is the source language and the Spanish, the target language. Clearly, ""la que no"" is not a good translation of ""you"", so this phrase should have a low probability. However, from our aligned training database we obtain, P (f |e) = P (you|la que no) = 0.23 This BP is clearly overestimated due to sparseness. On the other, note that ""la que no"" cannot be considered an unusual trigram in Spanish. Hence, the language model does not penalise this target sequence either. So, the total probability (P (f |e)P (e)) would be higher than desired. In order to somehow compensate these unreiliable probabilities we have studied the inclusion of the posterior [12] and lexical probabilities [1] [10] as additional features. Feature P (e|f ) In order to estimate the posterior phrase probability, we compute again the relative frequency but replacing the count of the target phrase by the count of the source phrase. P (e|f ) = N (f, e) N (f ) (5) where N'(f,e) means the number of times the phrase e is translated by f. If a phrase f has N > 1 possible translations, then each one contributes as 1/N. Adding this feature function we reduce the number of cases in which the overall probability is overestimated. This results in an important improvement in translation quality. IBM Model 1 We used IBM Model 1 to estimate the probability of a BP . As IBM Model 1 is a word translation and it gives the sum of all possible alignment probabilities, a lexical co-ocurrence effect is expected. This captures a sort of semantic coherence in translations. Therefore, the probability of a sentence pair is given by the following equation. P (f |e; M 1) = 1 (I + 1) J J j=1 I i=0 p(f j |e i ) (6) The p(f j |e i ) are the source-target IBM Model 1 word probabilities trained by GIZA++. Because the phrases are formed from the union of source-totarget and target-to-source alignments, there can be words that are not in the P (f j |e i ) table. In this case, the probability was taken to be 10 −40 . In addition, we have calculated the IBM −1 Model 1. P (e|f ; M 1) = 1 (J + 1) I I I=1 J j=0 p(e i |f j ) (7) Language Model The English language model plays an important role in the source channel model, see equation (2) , and also in its modification, see equation (3) . The English language model should give an idea of the sentence quality that is generated. As default language model feature, we use a standard word-based trigram language model generated with smoothing Kneser-Ney and interpolation (by using SRILM [16] ). The phrase penalty is a constant cost per produced phrase. Here, a negative weight, which means reducing the costs per phrase, results in a preference for adding phrases. Alternatively, by using a positive scaling factors, the system will favor less phrases. Word and Phrase Penalty Evaluation framework Corpus Statistics Experiments were performed to study the effect of our modifications in the phrases. The training material covers the transcriptions from April 1996 to September 2004. This material has been distributed by the European Parlament. In our experiments, we have used the distribution of RWTH of Aachen under the project of TC-STAR 1 . The test material was used in the first evaluation of the project in March 2005. In our case, we have used the development divided in two sets. This material corresponds to the transcriptions of the sessions from October the 21st to October the 28th. It has been distributed by ELDA 2 . Results are reported for Spanish-to-English translations. 1 http://www.tcstar.org/ 2 http://www.elda.org/ Experiments The decoder used for the presented translation system is reported in [2] . This decoder is called MARIE and it takes into account simultaneously all the 7 features functions described above. It implements a beam-search strategy. As evaluation criteria we use: the Word Error Rate (WER), the BLEU score [15] and the NIST score [3] . As follows we report the results for several experiments that show the performance of: the baseline, adding the posterior probability, IBM Model 1 and IBM1 −1 , and, finally, the modification of the phrases extraction. Optimisation. Significant improvements can be obtained by tuning the parameters of the features adequately. In the complet system we have 7 parameters to tune: the relatives frecuencies P (f |e) and P (e|f ), IBM Model 1 and its inverse, the word penalty, the phrase penalty and the weight of the language model. We applied the widely used algorithm SIMPLEX to optimise [9] . In Table 2 (line 5th), we see the final results. Baseline. We report the results of the baseline. We use the union alignment and we extract the BP of length 3. As default language model feature, we use the standard trigram with smoothing Kneser-Ney and interpolation. Also we tune the parameters (only two parameters) with the SIM-PLEX algorithm (see Table 2 ). Posterior probability. Table 2 shows the effect of using the posterior probability: P (e|f ). We use all the features but the P (e|f ) and we optimise the parameters. We see the results without this feature decrease around 1.1 points both in BLEU and WER (see line 2rd and 5th in Table 2 ). IBM Model 1. We do the same as in the paragraph above, we do not consider the IBM Model 1 and the IBM1 −1 . Under these conditions, the translation's quality decreases around 1.3 points both in BLEU and WER (see line 3th and 5th in Table 2 ). Modification of the Phrase Extraction. Finally, we made an experiment without modification of the phrases' length. We can see the comparison between: (1) the phrases of fixed maximum length of 3; and (2) including phrases with a maximum length of 5 which can not be generated by smaller phrases. We can see it in Table 2 (lines 4th and 5th). We observe that there is no much difference between the number of phrases, so this approach does not require more resources. However, we get slightly better scores. Shared Task This section explains the participation of ""Exploiting Parallel Texts for Statistical Machine Translation"". We used the EuroParl data provided for this shared task [4] . A word-to-word alignment was performed in both directions as explained in section 2. The phrase-based translation system which has been considered implements a total of 7 features (already explained in section 4). Notice that the language model has been trained with the training provided in the shared task. However, the optimization in the parameters has not been repeated, and we used the parameters obtained in the subsection above. We have obtained the results in the Table 3 . Conclusions We reported a new method to extract longer phrases without increasing the quantity of phrases (less than 0.5%). We also reported several features as P (e|f ) which in combination with the functions of the source-channel model provides significant improvement. Also, the feature IBM1 in combination with IBM1 −1 provides improved scores, too. Finally, we have optimized the parameters, and we provided the final results which have been presented in the Shared Task: Exploiting Parallel Acknowledgements The authors want to thank José B. Mariño, Adrià de Gispert, Josep M. Crego, Patrik Lambert and Rafael E. Banchs (members of the TALP Research Center) for their contribution to this work.",12386445,142a06b58eaa48bd4e53ade28dd175befe7a4124,6,https://aclanthology.org/W05-0827,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Ruiz Costa-juss{\`a}, Marta  and
Fonollosa, Jos{\'e} A. R.",Improving Phrase-Based Statistical Translation by Modifying Phrase Extraction and Including Several Features,149--154,,,,,,,inproceedings,ruiz-costa-jussa-fonollosa-2005-improving,,,
39,2009.mtsummit-posters.17,"Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domain monolingual texts to modify the probability distribution of the phrase-table and to learn new task-specific phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair.","Most of the existing, easily available parallel texts to train a statistical machine translation system are from international organizations that use a particular jargon. In this paper, we consider the automatic adaptation of such a translation model to the news domain. The initial system was trained on more than 200M words of UN bitexts. We then explore large amounts of in-domain monolingual texts to modify the probability distribution of the phrase-table and to learn new task-specific phrase-pairs. This procedure achieved an improvement of 3.5 points BLEU on the test set in an Arabic/French statistical machine translation system. This result compares favorably with other large state-of-the-art systems for this language pair. Introduction Adaptation of a statistical machine translation system (SMT) is a topic of increasing interest during the last years. Statistical (n-gram) language models are used in many domains and several approaches to adapt such models were proposed in the literature, for instance in the framework of automatic speech recognition. Many of these approaches were successfully used to adapt the language model of an SMT system. On the other hand, it seems more challenging to adapt the other components of an SMT system, namely the translation and reordering model. In this work we consider the adaptation of the translation model of a phrase-based SMT system. While rule-based machine translation rely on rules and linguistic resources built for that purpose, SMT systems can be developed without the need of any language-specific expertise and are only based on bilingual sentence-aligned data (""bitexts"") and large monolingual texts. However, while monolingual data is usually available in large amounts and for a variety of tasks, bilingual texts are a sparse resource for most language pairs. Current parallel corpora mostly come from one domain (proceedings of the Canadian or European Parliament, or of the United Nations). This is problematic when SMT systems trained on such corpora are used for general translations, as the language jargon heavily used in these corpora is not appropriate for everyday life translations or translations in some other domain. This problem could be attacked by either searching for more in-domain training data, e.g. by exploring comparable corpora or the WEB, or by adapting the translation model to the task. In this work we consider translation model adaptation without using additional bilingual data. One can distinguish two types of translation model adaptation: first, adding new source words or/and new translations to the model; and second, modifying the probabilities of the existing model to better fit the topic of the task. These two directions are complementary and could be simultaneously applied. In this work we focus on the second type of adaptation. A common way to modify a statistical model is to use a mixture model and to optimize the coefficients to the adaptation domain. This was investigated in the framework of SMT by several authors, for instance for word alignment (Civera and Juan, 2007) , for language modeling (Zhao et al., 2004; Koehn and Schroeder, 2007) and to a lesser extent for the translation model (Foster and Kuhn, 2007; Chen et al., 2008) . This mixture approach has the advan-tage that only few parameters need to be modified, the mixture coefficients. On the other hand, many translation probabilities are modified at once and it is not possible to selectively modify the probabilities of particular phrases. Comparable corpora are commonly used to find additional parallel texts, candidate sentences being often identified with help of information retrieval techniques, for instance (Hildebrand et al., 2005) . Recently, a similar idea was applied to adapt the translation and language model using monolingual texts in the target language (Snover et al., 2008) . Cross-lingual information retrieval was applied to find texts in the target language that are related to the domain of the source texts. However, it was difficult to get the alignments between the source and target phrases and an over-generalizing IBM1-style approach was used. Another direction of research is self-enhancing of the translation model. This was first proposed by (Ueffing, 2006) . The idea is to translate the test data, to filter the translations with help of a confidence score and to use the most reliable ones to train an additional small phrase table that is jointly used with the generic phrase table. This could be also seen as a mixture model with the in-domain component being build on-the-fly for each test set. In practice, such an approach is probably only feasible when large amounts of test data are collected and processed at once, e.g. a typical evaluation set up with a test set of about 50k words. This method of self-enhancing the translation model seems to be more difficult to apply for on-line SMT, e.g. a WEB service, since often the translation of some sentences only is requested. In follow up work, this approach was refined (Ueffing, 2007). Domain adaptation was also performed simultaneously for the translation, language and reordering model (Chen et al., 2008) . A somehow related approach was named lightlysupervised training (Schwenk, 2008) . In that work an SMT system is used to translate large amounts of monolingual texts, to filter them and to add them to the translation model training data. We could obtain small improvements in the BLEU score in a French/English translation system. Although this technique seems to be close to self enhancing as proposed by (Ueffing, 2006) , there is a conceptual difference. We do not use the test data to adapt the translation model, but large amounts of monolingual training data in the source language and we create a complete new model that can be applied to any test data without additional modification of the system. This kind of adapted system can be used in WEB service. In this paper, we use the same type of approach to adapt an generic Arabic/French translation system to the news domain. This task is interesting for several reasons: there is only a limited amount of in-domain bitexts available (about 1.2M words), but large amounts of out-of-domain bitexts (≈150M words of UN data) and both languages have a rich morphology. Usually, the Arabic source words are decomposed to detach pre-and suffixes. This helps to significantly reduce the size of the translation vocabulary and is reported to improve the translation quality. This morphological decomposition also results in many different and infrequent phrases which may lead to bad relative frequency estimates of the phrase translation probabilities. We are aiming in improving those estimates by using large amount of monolingual in-domain data. Finally, there seems to be a real need to translate between Arabic and French for the population in the Mediterranean area. This paper is organized as follows. In the next section we first describe the considered task and the available bilingual and monolingual resources. Section 3 describes the baseline SMT systems. The following section describe our adaptation technique. Results are summarized in section 5 and the paper concludes with a discussion and perspectives of this work. Task Description and resources In this paper, we consider the translation of news texts from Arabic into French. We are not aware of easily available aligned parallel corpora for this language pair. Fortunately, Arabic and French are both official languages of the United Nations. We crawled data from various sources of the United Nations over the period 1988-2008. This totals in almost 150M Arabic words. The Arabic and French texts were automatically sentence aligned. This amount of parallel texts is usually considered as more than sufficient to train an SMT system. Note however, that a particular jargon is used in the UN texts that is not appropriate for news-text translation. The French TRAMES 1 project considered the translation of Arabic Speeches to French. In the framework of this project, about 90h of Arabic TV and radio broadcast news were recorded, transcribed and translated into French. The sources are Orient, Qatar, BBC, Alarabiya, Aljazeera and Alalam. These high-quality domain specific bitexts of about 262k Arabic words were made available to us by the DGA. 2  Additional bilingual training data was obtained from the Project Syndicate WEB-site. 3 This data source is already used to build SMT systems to translate between European languages, in particular in the framework of the evaluations organized in junction with the workshops on statistical machine translation (Callison-Burch et al., 2007; Callison-Burch et al., 2008) . Some of the texts are also translated into Arabic. The scripts to access this WEBsite were kindly made available by P. Koehn. We crawled and aligned a total of 1.6M words. Note that these texts are not exactly broadcast news texts. The characteristics of the translation model training data is summarized in table 1. The number of words is given after tokenization. Arabic French Words Vocab Words Vocab DGA TRAMES 262k 30k 400k 18k News commentary 1.1M 67k 1.3M 41k UN 149M 712k 212M 420k The DGA also provided a test set that was created in the same way than the in-domain bitexts. Four high-quality reference translations are available. We randomly split this data into a development set for system tuning and an internal test set. The details of the development and test set are given in Table 2 . We are only aware of one other large Arabic/French news translation system, the one that was developed during the TRAMES project (Hasan and Ney, 2008) 3 Baseline system The baseline system is a standard phrase-based SMT system based on the the Moses SMT toolkit (Koehn et al., 2007) . It uses fourteen features functions, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty, and a target language model. It constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008) . 4 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. All the bitexts were concatenated. The parameters of Moses are tuned on the development data using the CMERT tool. Tokenization There is a large body of work in the literature showing that a morphological decomposition of the Arabic words can improve the word coverage and by these means the translation quality, see for instance (Habash and Sadat, 2006) . It is clear that such a decomposition is most helpful when the translation model training data is limited, but this is less obvious for tasks where several hundreds of millions of words of bitexts are available. Most of the published work is based on the freely available tools, like the Buckwalter transliterator and the MADA and TOKAN tools for morphological analysis from Columbia University. In this work, we compare two different tokenization of the Arabic source text: a full word mode and the morphological decomposition provided by the sentence analysis module of SYSTRAN's rulebased Arabic/English translation software. Sentence analysis represents a large share of the computations in a rule-based system. This process first applies decomposition rules coupled with a word dictionary. For words that are not known in the dictionary, the most likely decomposition is guessed. In general, all possible decompositions of each word are generated and then filtered in the context of the sentence. This step uses lexical knowledge and a global analysis of the sentences. This morphological decomposition drastically reduced the vocabulary of the Arabic bitexts: it was almost divided by two. The French texts were tokenized using the tools of the Moses suite. Punctuation and case were preserved. LM training In contrast to the translation model, many resources are available to train a LM optimized on French broadcast news texts. We used the French side of the bitexts, data form the European and the Canadian parliament, news-data crawled in the framework for the 2009 WMT evaluation, 5 other WEB data collected by ourselves and finally LDC's Gigaword corpus. Separate 4-gram back-off LMs were build on each data source with the SRI LM toolkit (Stolcke, 2002) coefficients with an EM procedure. The perplexities of these LMs are given in Table 3 . Translation model adaptation by lightly-supervised training The goal of this work is to adapt the translation model without using additional bilingual data. Instead we will use in-domain monolingual data in the source language. Usually it is relatively easy to obtain large collections of such data, in particular in the news domain as considered in this work. We use parts of the LDC Arabic Gigaword corpus, but more recent texts could be easily found on the Internet. These texts are translated by an initial, unadapted SMT system. We then need to filter the automatic translations in order to keep only the ""good ones"" for addition to the translation model training data. This selection could take advantage of word-based confidence scores (Ueffing, 2007) . We use the sentencelength normalized log-likelihoods of the decoder. These selected translations are used as additional indomain bitexts and the standard procedure to build a new SMT system is performed, i.e. word alignment with GIZA++, phrase extraction and tuning of the system parameters. Alternately, we could reuse the alignments established by the translation process since the Moses decoder is able to output the phrase and word alignments. This would speed up the process of creating the adapted SMT system since we skip the timeconsuming word alignment performed by GIZA++. It could also be that the decoder-induced word alignments are more appropriate than those performed by GIZA++. This was partially investigated in the framework of pivot translation to produce artificially bitexts in another language (Bertoldi et al., 2008) . Finally, instead of only using the 1-best translation we could also use the n-best list. LDC's Arabic and French Gigaword corpora are described in Table 4 . There is only one source that does exist in both languages: the AFP collection. It is likely that the Arabic and French texts partially cover the same facts, but they are usually not direct translations. 6 In fact, we were informed that journalists at AFP have the possibility to freely change the sentences when they report on a fact based on text already available in another language. Nevertheless, it can be expected that using these texts in the target language model helps the SMT system to produce good translations. This language model training data can be considered as some form of light supervision and we will therefore use the term lightly-supervised training (Schwenk, 2008) . This can be compared to the research in speech recognition where the same term was used for supervision that either comes from approximate transcriptions of the audio signal (closed captions) or related language model training data. Source Arabic French AFP 145M 570M APW -200M ASB 7M - HYT 175M - NHR 188M - UMH 1M - XIN 58M - In this work, we have processed the AFP Arabic text only, but the other texts (ASB, HYT, . . .) could be processed in the same manner. In fact it is an interesting question whether the availability of related or even ""comparable"" texts for the target language model is a necessary condition for our approach to work. All these issues will be explored in future research. Experimental Evaluation We first performed experiments using different amounts of bitexts to train the translation model and analyzed the benefits of the morphological decomposition of the Arabic words. The results are summarized in Table 5 . The TRAMES training corpus contains several lines with more than 100 words. These can't be processed by the GIZA++ tool and the were discarded. This was the case for about 6% of the data. In future research, we will try to split those lines into shorter sentences. As expected, the morphological decomposition of the Arabic words is very helpful when only a small amount of training data is available: using only the TRAMES and news-commentary in-domain data we observed an improvement of 4.6 BLEU points (first and second line in Table 5 ). Note that in this case we have actually less training data since the morphological decomposition leads to longer phrases out of which many are discarded by the 100 words limit of GIZA++. Somewhat surprisingly, the morphological decomposition still achieves a significant improvement of 1 BLEU point when more than 200M words of bitexts are available (last two lines in Table 5). Translation model adaptation The best system we were able to build with all human provided translations was used to translate all the AFP news texts from Arabic to French. The phrase table of whole AFP Gigaword corpus with such a large system is a computational challenge since it is impossible to charge the whole phrase table into memory. The Moses system supports two procedures to deal with this problem: filtering of the phrase table or binary on-disk phrase tables. Neither technique can be applied here. The phrase table is still too big in the first case and the binary representation of the whole phrase table occupies too much space on disk. This problem could be eventually approached with a distributed representation of the data structures. We finally adopted a combination of both techniques: the AFP corpus is split into parts with 50k lines (approximately 1.5M words), the phrase table is filtered for this data and then binarized. This made it possible to load the LM into memory and to have a process size of less then 20GB. The total translation time was more than 2700 hours. 7 These automatic translations were filtered according to the sentence-length normalized log-likelihood and the most likely ones were used as bitexts. Different amounts of data can be obtained by varying the threshold on the likelihood. The BLEU scores on the development and test data in function of the total size of the bitexts are shown in figure 1 . In these experiments we only use the in-domain humanprovided bitexts (TRAMES and news-commentary) -the UN data being replaced by the automatic translations. The best value on the development data was obtained for a total of 48M words of bitexts. The BLEU score on the development data is 45.44 and 43.68 on the test data respectively (see also table 6 ). This is an improvement of 3.5 BLEU points on both data sets. We analyzed the phrase table of the original system trained on all human provided data, including UN, and the one of the automatically adapted system. This is summarized in table 7. The original phrase table had 329M entries out of which 22.9M could be potentially applied on the test data. The phrase table of the adapted system on the other hand used only 700k out of a total of 8.6M phrases. It seems clear that the phrase table obtained by training on the UN data contains many entries that are not useful, or eventually even correspond to wrong translations. Surprisingly, the phrase table of the adapted system is not only substantially smaller, but even contains about 11% more entries (18029 with respect to 16263). All these entries correspond to new sequences of known words since lightlysupervised training cannot extend the source or target side vocabulary. We conjecture that this is particularly important with the morphological decomposition of the Arabic words. This decomposition reduces the vocabulary size of the source language, but produces on the other hand many possible se- Source: ‫ق.‬ ِ ‫ساب‬ ّ ‫ال‬ ‫ي‬ ّ ِ ‫راق‬ َ ِ ‫لع‬ َ ‫ا‬ ‫ِيس‬ ‫رئ‬ َ ‫ل‬ َ ‫ا‬ ّ ‫ضد‬ ِ ‫م‬ َ ‫ُه‬ ‫ت‬ ِ ‫حة‬ َ ِ ‫لئ‬ َ ِ ‫جيه‬ ِ ‫َو‬ ‫ت‬ ِ ‫ب‬ ‫ِيل‬ ‫َل‬ ‫ق‬ ُ ‫منذ‬ ُ ‫ت‬ َ ‫َأ‬ ‫َد‬ ‫ب‬ ‫ّة‬ ‫ِي‬ ‫راق‬ َ ِ ‫الع‬ ‫ة‬ ُ ‫م‬ َ َ ‫محك‬ َ ‫ل‬ َ ‫ا‬ Base: le tribunal irakien a commencé depuis peu par la direction du règlement des accusations contre l'ancien président irakien. Adapt: le tribunal irakien a commencé depuis peu une liste d'accusations contre l'ancien président irakien. Ref: La Cour irakienne a commencé à dresser la liste des inculpations de l'ancien président irakien. Source: ‫ِي‬ ‫ف‬ ‫ه‬ ّ ‫لل‬ َ ‫ا‬ ‫رام‬ َ ‫ِي‬ ‫ف‬ ً ‫شطا‬ ِ ‫َا‬ ‫ن‬ ً ‫يل‬ َ ‫ل‬ ‫ل‬ َ َ ‫َق‬ ‫ِعت‬ ‫ا‬ ‫ي‬ ّ ِ ‫ِيل‬ ‫رائ‬ َ ‫س‬ ِ ‫ال‬ ‫ش‬ َ ‫جي‬ َ ‫ال‬ ‫ن‬ ّ َ ‫أ‬ ‫ّة‬ ‫ِي‬ ‫ِيل‬ ‫رائ‬ َ ‫س‬ ِ ‫إ‬ ‫ة‬ ٌ ّ ‫ي‬ ِ ‫ر‬ َ ‫َسك‬ ‫ع‬ ‫ر‬ ُ ِ ‫صاد‬ َ ‫م‬ َ ‫َت‬ ‫َاد‬ ‫ف‬ َ ‫أ‬ ‫ن‬ َ ‫رو‬ ُ ‫ض‬ ّ ‫ح‬ َ ُ ‫ي‬ ‫ُوا‬ ‫ان‬ َ ‫ك‬ ‫رين‬ َ ‫خ‬ َ ‫آ‬ ‫ن‬ ِ ‫ي‬ َ ‫شط‬ ِ ‫َا‬ ‫ن‬ ‫ل‬ ُ ‫َا‬ ‫ِق‬ ‫اعت‬ ‫م‬ ّ َ ‫ت‬ ‫ما‬ َ َ ‫ك‬ ‫ّة‬ ‫ـي‬ ِ ‫َرب‬ ‫الغ‬ ِ ‫ّة‬ ‫ضف‬ ّ ‫ال‬ Base: De source militaire israélienne a indiqué que l'armée israélienne a arrêté dans la nuit militants à Ramallah en Cisjordanie ont été arrêtés autres militants qui ... Adapt: Selon des sources militaires israéliennes, l'armée israélienne a arrêté dans la nuit de militants à Ramallah, en Cisjordanie, a également été arrêté deux autres activistes qui ... Ref: Des sources militaires israéliennes ont indiqué que l'armée israélienne a arrêté de nuit un activiste à Ramallah en Cisjordanie, ainsi que deux autres activistes qui ... Source: ‫من.‬ َ َ ‫لي‬ َ ‫ا‬ ‫َة،‬ ‫حاف‬ َ ‫ص‬ ّ ‫ال‬ ‫ة‬ ُ َ ‫جول‬ َ ‫ي،‬ ِ ‫َار‬ ‫ُب‬ ‫الغ‬ ُ ‫مد‬ ّ ‫ح‬ َ ‫م‬ ُ Base: Mohammed du brouillard, le cycle de la presse, au Yémen. Adapt: Mohammed, une tournée de la presse le Yémen. Ref: Mohamed Al-Ghobari, tour de la presse, Yémen. Source:  . quences of tokens. It seems important to a include sequences of these tokens in the phrase table that appear in in-domain data. As a side effect, the smaller phrase-table of the adapted system also leads to a 40% faster translation. We compared the translations of the unadapted and adapted systems: the TER is about 30 in both directions, meaning that the outputs differ substantially. Some example translations are shown in figure 2. The adapted system clearly produces better output in these examples. There remain of course some errors in these sentences, but we argue that the quality is high enough for an human being to capture most of the meaning of the sentences. ‫راق.‬ َ ِ ‫الع‬ ‫ن‬ َ ‫م‬ ِ ‫َا‬ ‫ِه‬ ‫ُود‬ ‫جن‬ ُ ‫ب‬ ِ ‫سح‬ َ ‫ِي‬ ‫ف‬ ً ‫يضا‬ َ ‫أ‬ ‫لند‬ َ ‫َاي‬ ‫ت‬ ‫َت‬ ‫رع‬ َ ‫ش‬ َ ‫رى‬ َ ‫خ‬ ُ ‫أ‬ ٍ ‫ة‬ َ ‫جه‬ ِ ‫من‬ ِ Base: d' Conclusion Statistical machine translation is today used to rapidly create automatic translations systems for a variety of tasks. In principle, we only need aligned example translations and monolingual data in the target language. However, for many application domains and language pairs there are no appropriate in-domain parallel texts to train the translation model. On the other hand, large generic bitexts may be available. In this work we consider such a configuration: the translation of broadcast news texts from Arabic to French. We have a little more than 1M words of in-domain bitexts and about 150M words of generic bitexts from the United Nations. This system is automatically adapted to the news domain by using large amounts of monolingual texts, namely LDC's collection of Arabic AFP texts from 1994 to 2006. These texts were processed by the initial SMT system and the most reliable automatic translations were added to the bitexts and a new system was trained. By these means we achieved an improvement in the BLEU score of 3.5 points on the test set. This system actually uses less bitexts than the generic one since the generic UN bitexts are not used any more. An analysis of the created phrase table seems to indicate that the adaptation of the translation model leads to much smaller and more concise phrases. Our best system achieves a BLEU score of 43.68 on the test set which compares favorably with other large state-of-the-art systems for this language pair. The proposed algorithm is generic and could be applied to other language pairs and applications domains. We only need a good initial generic SMT system and in-domain monolingual texts in the source language.r It is interesting to compare our approach to self-learning as proposed in (Ueffing, 2006) Selflearning was applied to small amounts of test data only while we use several hundreds of million words of training data in the source language. We build a complete new phrase table instead of interpolating a small ""adapted phrase table"" with a generic one. Finally, self-learning was applied during the translation process and must be repeated for each new test data. This is computationally expensive and is difficult to use in on-line translation. The approach proposed in this paper applies translation model adaptation once and builds a new SMT system that can be then applied to any test data (ideally from the same domain). Several extensions of the proposed approach can be envisioned, namely improved confidence scores for filtering the most reliable translations, processing of n-best lists instead of using the most likely translation only, and reuse of the decoder-induced word alignments instead of rerunning GIZA++. We are currently working on these issues. Acknowledgments The Arabic/French corpus of broadcasts news as well as the corresponding test set were made available to us by the DGA. This work has been partially funded by the French Government under the project INSTAR (ANR JCJC06 143038) and the European Commission under the project EuromatrixPlus.",28006881,53d6c70d1a4871a8a8164f86d824313ef655a8b7,41,https://aclanthology.org/2009.mtsummit-posters.17,,"Ottawa, Canada",2009,August 26-30,Proceedings of Machine Translation Summit XII: Posters,"Schwenk, Holger  and
Senellart, Jean",Translation Model Adaptation for an {A}rabic/{F}rench News Translation System by Lightly- Supervised Training,,,,,,,,inproceedings,schwenk-senellart-2009-translation,,,
40,R13-1053,"Translating from English, a morphologically poor language, into morphologically rich languages such as Persian comes with many challenges. In this paper, we present an approach to rich morphology prediction using a parallel corpus. We focus on the verb conjugation as the most important and problematic phenomenon in the context of morphology in Persian. We define a set of linguistic features using both English and Persian linguistic information, and use an English-Persian parallel corpus to train our model. Then, we predict six morphological features of the verb and generate inflected verb form using its lemma. In our experiments, we generate verb form with the most common feature values as a baseline. The results of our experiments show an improvement of almost 2.1% absolute BLEU score on a test set containing 16K sentences.","Translating from English, a morphologically poor language, into morphologically rich languages such as Persian comes with many challenges. In this paper, we present an approach to rich morphology prediction using a parallel corpus. We focus on the verb conjugation as the most important and problematic phenomenon in the context of morphology in Persian. We define a set of linguistic features using both English and Persian linguistic information, and use an English-Persian parallel corpus to train our model. Then, we predict six morphological features of the verb and generate inflected verb form using its lemma. In our experiments, we generate verb form with the most common feature values as a baseline. The results of our experiments show an improvement of almost 2.1% absolute BLEU score on a test set containing 16K sentences. Introduction One of the main limitations of statistical machine translation (SMT) is the sensitivity to data sparseness, due to the word-based or phrased-based approach incorporated in SMT (Koehn et al., 2003) . This problem becomes severe in the translation from or into a morphologically rich language, where a word stem appears in many completely different surface forms. Therefore, morphological analysis is an important phase in the translation from or into such languages, because it reduces the sparseness of model. So, modeling rich morphology in machine translations (MT) has received a lot of research interest in several studies. In this paper, we present a novel approach to rich morphology prediction for Persian as target language. We focus on the verb conjugation as a highly inflecting class of words and an important part of morphological processing in Persian. Our model incorporates decision tree classifier (DTC) (Quinlan, 1986) , which is an approach to multistage decision making. In order to train DTC, we use both English and Persian linguistic information such as syntactic parse tree and dependency relations obtained from an English-Persian parallel corpus. Morphological features which we predict and use to generate the inflected form of verb are voice (VOC), mood (MOD), number (NUM), tense (TEN) , negation (NEG) and person (PER). Our proposed model can be used as a component to generate rich morphology for any kind of languages and MTs. The reminder of the paper is organized as follows: Section 2 briefly reviews some challenges in Persian verb conjugation, Section 3 presents our proposed approach to generate rich morphology, in Section 4 our experiments and results are presented, in Section 5 we cover conclusions and future work, and finally, in Section 6 we describe related works. Morphology Challenges of the Persian Verbs Verbs in Persian have a complex inflectional system (Megerdoomian, 2004) . This complexity appears in the following aspects: • Different verb forms • Different verb stems • Affixes marking inflections • Auxiliaries used in certain tenses Simple form and compound form are two forms used in Persian verbal system. Simple form is broken into two categories according to the stem used in its formation. Compound form refers to those that require an auxiliary to form a correct verb. Two stems are used to construct a verb: present stem and past stem. Each of which is used in creating of specific tenses. We cannot derive the two stems from each other due to different surface forms they usually have. Therefore, they treated as distinct characteristics of verbs. Several affixes are combined with stems to mark MOD, NUM, NEG and PER inflections. Auxiliaries are used to make a compound form in certain tenses to indicate VOC and TEN inflections, similar to HAVE and BE in Approach Our proposed approach is broken into two main steps: DTC training and Morphology prediction. Then we can generate a verb form using a finite state automaton (Megerdoomian, 2004) , if we are given the six morphological features of the verb. In the next subsections we describe these steps more precisely. DTC Training To make train and test set, we use an English-Persian parallel corpus containing 399K sentences Table 2 : Some statistics about the English-Persian parallel corpus (Mansouri and Faili, 2012) . (367K to train,16K to validate and 16K to test). More details about this corpus, which is used by Mansouri and Faili (2012) to build an SMT, are presented in Table 2 . Giza++ (Och and Ney, 2003) is used to word alignment. We only select such an alignment that is most probable to translate both from English to Persian and Persian to English among those assigned to each verb. With this heuristic we ignore a lot of alignments to produce a high quality data set. We selected 100 sentences randomly and evaluated the alignments manually, so that 27% recall and 93% precision were obtained. Then, we define a set of syntactic features on English side as DTC learning features. These features consist of several language-specific features such as English part-of-speech tag (POS) of the verb, dependency relationships of the verb and POS of subject of the verb. English is parsed using Stanford Parser (Klein and Manning, 2003) . After that, we can produce training data set by analyzing the Persian verb aligned to each English verb using (Rasooli et al., 2011) , in which two unsupervised learning methods have been proposed to identify compound verbs with their corresponding morphological features. The first one which is extending the concept of pointwise mutual information, uses a bootstraping method and the second one uses K-means clustering algorithm to detect compound verbs. However, as we have the verb, we only use their proposed method to determine VOC, MOD, NUM, TEN, NEG and PER for a given verb as our class labels. Also, we use their tool to extract the lemma of the verb (in Figure 1 ""Verb lemmatizer"" refers to this tool in which there is a lookup table to find the lemma of a verb). This lemma is used to generate an inflected verb form using FSA. phemes. Unlike these approaches, we predict morphological features like El Kholy and Habash (2012a and b) . Using our training data set, we build six language specific DTCs to predict each of the morphological features. Each DTC uses a subset of our feature set and predicts corresponding morphology feature independently. Then, we use a FSA to generate an inflected verb form using these six morphological features. Figure 1 , shows the general schema of verb generation process. Morphology Prediction Table 3 shows Correct Classification Ratio (CCR) of each DTC learned on our train data containing 178782 entries and evaluated on a test set containing more than 20k verbs. The most common feature value is used as our baseline for each classifier. The most improvement is achieved in the prediction of MOD and NUM. Others have high CCR but they also have very high baselines. Experiments In this section, we present the results of our experiments on a test set containing 16K sentences selected from an English-Persian parallel corpus. As the main goals of our experiments, we are interested in knowing the effectiveness of our approach to rich morphology prediction and the contribution each feature has. To do so, like Kholy and Habash (2012) , who use aligned sentence pair of reference translations (reference experiments) instead of the output of an MT system as input, we also perform reference experiments because they are golden in terms of word order, lemma choice and morphological features. Table 4 shows detailed n-gram BLEU (Papineni et al., 2002) precision (for n=1,2,3,4), BLEU and TER (Snover et al., 2006) scores for morphology generation using gold lemma with the most common feature values (LEM) as a baseline and other gold morphological features and their combinations as our reference experiments. In this experiment, we replace each sentence verb with predicted verb generated by FSA using gold lemma plus the most common feature values as a baseline. In comparison with the baseline used by El Kholy and Habash (2012) , this baseline is more stringent. As another baseline we have used a rule-based morphological analyzer which determines morphological features of the verb grammatically and generates inflected verb form (this rule-based morphological analyzer uses syntactic parse, POS tags and dependency relationships of English sentence). We use each gold feature separately to investigate the contribution each feature has. Finally, we combine gold features incrementally based on their CCR. Adding more features improve BLEU and TER scores. Since, there are some cases in which with the same morphological features it is possible to generate different but correct verb forms, the maximum BLEU score of 100 is hard to be reached even if we are given the gold features. So, the best result (97.90 of BLEU and 0.0114 of TER) could be considered as an upper bound for proposed approach. Note that, these results are obtained from our reference experiments in which a reference is duplicated and modified by our approach. In fact, there is no translation task here and a reference is evaluated by its modified version. We perform the same reference experiments on the same data using predicted features instead of the gold features. Table 5 reports the results of detailed n-gram BLEU precision, BLUE and TER scores. According to the results, our approach outperforms the baselines in all configurations. The best configuration uses all predicted features and shows an improvement of about 2.1% absolute BLEU score and 0.102% absolute TER against our first baseline. Also, in comparison with our second baseline, rule-based approach, we achieve improvements of about 1.6% absolute BLEU score and 0.103% absolute TER. Conclusions and Future Work In this paper we present a supervised approach to rich morphology prediction. We focus on verb inflections as a highly inflecting class of words in Persian, a morphologically rich language. Using different combination of morphological features to generate inflected verb form, we evaluate our approach on a test set containing 16K sentences and obtain better BLEU and TER scores compared with our baseline, morphology generation with lemma plus the most common feature values. Our proposed approach predicts each morphological feature independently. In the future, we plan to investigate how the features affect each other to present an order in which a predicted morphological feature is used as a learning feature for the next one. Furthermore, we also plan to use our approach as a post processing morphology generation to improve machine translation output. 6 Related Work In this section we introduce the main approaches to morphology generation. The first approach is based on factored models, an extension of phrased-based SMT model (Koehn and Hoang, 2007) . In this approach each word is annotated using morphology tags on morphologically rich side. Then, morphology generation is done based on the word level instead of phrase level, which is also the limitation of this approach. A similar approach is used by Avramidis and Koehn (2008) to translate from English into Greek and Czech. They especially focus on noun cases and verb persons. Mapping from syntax to morphology in factored model is used by Yeniterzi and Oflazer (2010) to improve English-Turkish SMT. Hierarchical phrase-based translation, an extension of factored translation model, proposed by Subotin (2011) to generate complex morphology using a discriminative model for Czech as the target laguage. Maximum entropy model is another approach used by Minkov et al. (2007) for English-Arabic and English-Russian MT. They proposed a postprocessing probabilistic framework for morphology generation utilizing a rich set of morphological knowledge sources. There are some similar approaches used by Toutanova et al. (2008) for Arabic and Russian as the target languages and by Clifton and Sarkar (2011) for English-Finnish SMT. In these approaches, the model of morphol-ogy prediction is an independent process of the SMT system. Segmentation is another approach that improves MT by reducing the data sparseness of translation model and increasing the similarity between two sides (Goldwater and McClosky, 2005; Luong et al., 2010; Oflazer, 2008) . This method analyzes morphologically rich side and unpacks inflected word forms into simpler components. Goldwater and McClosky (2005) showed that modifying Czech as the input language using 'pseudowords' improves the Czech-English machine translation system. Similar approaches are used by Oflazer (2008) for English to Turkish SMT, Luong et al. (2010) for translating from English into Finnish and Namdar et al. (2013) to improve Persian-English SMT. Recently, a novel approach to generate rich morphology is proposed by El Kholy and Habash (2012) . They use SMT to generate inflected Arabic tokens from a given sequence of lemmas and any subset of morphological features. They also have used their proposed method to model rich morphology in SMT (El Kholy and Habash, 2012) . Since we use lemma and the most common feature values as our baseline, the results of their experiments is somewhat comparable to ours. However, they use only lemma with no prediction as their baseline. So, our baseline is more stringent than the baseline used by El Kholy and Habash (2012) . Our work is conceptually similar to that of de Gispert and Marino (2008) , in which they incorporate a morphological classifier for Spanish verbs and define a collection of context dependent linguistic features (CDLFs), and predict each morphology feature such as PER or NUM. However, we use a different set of CDLFs and incorporate DTC to predict the morphology features of Persian verbs. Acknowledgment This work has been partially funded by Iran Telecom Research Center (ITRC) under contract number 9513/500.",15414615,fbf09b3df0673d93a182d3cbc99a56b7fc3c0b6d,2,https://aclanthology.org/R13-1053,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Mahmoudi, Alireza  and
Arabsorkhi, Mohsen  and
Faili, Heshaam",Supervised Morphology Generation Using Parallel Corpus,408--414,,,,,,,inproceedings,mahmoudi-etal-2013-supervised,,,Machine Translation and Multilinguality
41,W05-0828,"We motivate our contribution to the shared MT task as a first step towards an integrated architecture that combines advantages of statistical and knowledge-based approaches. Translations were generated using the Pharaoh decoder with tables derived from the provided alignments for all four languages, and for three of them using web-based and locally installed commercial systems. We then applied statistical and heuristic algorithms to select the most promising translation out of each set of candidates obtained from a source sentence. Results and possible refinements are discussed.","We motivate our contribution to the shared MT task as a first step towards an integrated architecture that combines advantages of statistical and knowledge-based approaches. Translations were generated using the Pharaoh decoder with tables derived from the provided alignments for all four languages, and for three of them using web-based and locally installed commercial systems. We then applied statistical and heuristic algorithms to select the most promising translation out of each set of candidates obtained from a source sentence. Results and possible refinements are discussed. Motivation and Long-term Perspective ""The problem of robust, efficient and reliable speech-to-speech translation can only be cracked by the combined muscle of deep and shallow processing approaches."" (Wahlster, 2001) Although this statement has been coined in the context of VerbMobil, aiming at translation for direct communication, it appears also realistic for many other translation scenarios, where demands on robustness, coverage, or adaptability on the input side and quality on the output side go beyond today's technological possibilities. The increasing availability of MT engines and the need for better quality has motivated considerable efforts to combine multiple engines into one ""super-engine"" that is hopefully better than any of its ingredients, an idea pionieered in (Frederking and Nirenburg, 1994) . So far, the larger group of related publications has focused on the task of selecting, from a set of translation candidates obtained from different engines, one translation that looks most promising (Tidhar and Küssner, 2000; Akiba et al., 2001; Callison-Burch and Flournoy, 2001; Akiba et al., 2002; Nomoto, 2004) . But also the more challenging problem of decomposing the candidates and re-assembling from the pieces a new sentence, hopefully better than any of the given inputs, has recently gained considerable attention (Rayner and Carter, 1997; Hogan and Frederking, 1998; Bangalore et al., 2001; Jayaraman and Lavie, 2005) . Although statistical MT approaches currently come out as winners in most comparative evaluations, it is clear that the achievable quality of methods relying purely on lookup of fixed phrases will be limited by the simple fact that for any given combination of topic, application scenario, language pair, and text style there will never be sufficient amounts of pre-existing translations to satisfy the needs of purely data-driven approaches. Rule-based approaches can exploit the effort that goes into single entries in their knowledge repositories in a broader way, as these entries can be unfolded, via rule applications, into large numbers of possible usages. However, this increased generality comes at significant costs for the acquisition of the required knowledge, which needs to be encoded by specialists in formalisms requiring extensive training to be used. In order to push the limits of today's MT technology, integrative approaches will have to be developed that combine the relative advantages of both paradigms and use them to compensate for their disadvantages. In particular, it should be possible to turn single instances of words and constructions found in training data into internal representations that allow them to be used in more general ways. In a first step towards the development of integrated solutions, we need to investigate the relative strengths and weaknesses of competing systems on the level of the target text, i.e. find out which sentences and which constructions are rendered well by which type of engine. In a second step, such an analysis will then make it possible to take the outcomes of various engines apart and re-assemble from the building blocks new translations that avoid errors made by the individual engines, i.e. to find integrated solutions that improve over the best of the candidates they have been built from. Once this can be done, the third and final step will involve feed back of corrections into the individual systems, such that differences between system behaviour can trigger (potentially after manual resolution of unclear cases) system updates and mutual learning. In the long term, one would hope to achieve a setup where a group of MT engines can converge to a committee that typically disagrees only in truly difficult cases. In such a committee, remaining dissent between the members would be a symptom of unresolved ambiguity, that would warrant the cost of manual intervention by the fact that the system as a whole can actually learn from the additional evidence. We expect this setup to be particularly effective when existing MT engines have to be ported to new application domains. Here, a rule-based engine would be able to profit from its more generic knowledge during the early stages of the transition and could teach unseen correspondences of known words and phrases to the SMT engine, whereas the SMT system would bring in its abilities to apply known phrase pairs in novel contexts and quickly learn new vocabulary from examples. Collecting Translation Candidates Setting up Statistical MT In the general picture laid out in the preceding section, statistical MT plays an important role for several reasons. On one hand, the construction of a relatively well-performing phrase-based SMT system from a given set of parallel corpora is no more overly difficult, especially if -as in the case in this shared task -word alignments and a decoder are provided. Furthermore, once the second task in our chain will have been surmounted, it will be relatively easy to feed back building blocks of improved translations into the phrase table, which constitutes the central resource of the SMT system Therefore, SMT facilitates experiments aiming at dynamic and interactive adaptation, the results of which should then also be applicable to MT engines that represent knowledge in a more condensed form. In order to collect material for testing these ideas, we constructed phrase tables for all four languages, following roughly the procedure given in (Koehn, 2004 ) but deviating in one detail related to the treatment of unaligned words at the beginning or end of the phrases 1 . We used the Pharaoh decoder as described on http://www.statmt.org/wpt05/mt-sharedtask/ after normalization of all tables to lower case. Using Commercial Engines As our main interest is in the integration of statistical and rule-based MT, we tried to collect results from ""conventional"" MT systems that had more or less uniform characteristics across the languages involved. We could not find MT engines supporting all four source languages, and therefore decided to drop Finnish for this part of the experiment. We sent the texts of the other three languages through several incarnations of Systran-based MT Web-services 2 and through an installation of Lernout & Hauspie Power Translator Pro, Version 6.43. 3 1 We used slightly more restrictive conditions that resulted in a 5.76% reduction of phrase table size 2 The results were incomplete and different, but sufficiently close to each other so that it did not seem worthwhile to explore the differences systematically. Instead we ranked the services according to errors in an informal comparison and took for each sentence the first available translation in this order. 3 After having collected or computed all translations, we observed that in the case of French, both systems were quite sensitive to the fact that the apostrophes were formatted as separate tokens in the source texts (l ' homme instead of l'homme). We therefore modified and retranslated the French texts, but did not explore possible effects of similar transformations in the other languages. Heuristic Selection Approach We implemented two different ways to select, out of a set of alternative translations of a given sentence, one that looks most promising. The first approach is purely heuristic and is limited to the case where more than two candidates are given. For each candidate, we collect a set of features, consisting of words and word n-grams (n ∈ {2, 3, 4}). Each of these features is weighted by the number of candidates it appears in, and the candidate with the largest feature weight per word is taken. This can be seen as the similarity of each of the candidate to a prototypical version composed as a weighted mixture of the collection, or as being remotely related to a sentence-specific language model derived from the candidates. The heuristic measure was used to select ""favorite"" from each group of competing translations obtained from the same source sentence, yielding a fourth set of translations for the sentences given in DE, FR, and ES. A particularity of the shared task is the fact that the source sentences of the development and test sets form a parallel corpus. Therefore, we can not only integrate multiple translations of the same source sentence into a hopefully better version, but we can merge the translations of corresponding parts from different source languages into a target form that combines their advantages. This approach, called triangulation in (Kay, 1997) , can be motivated by the fact that most cases of translation for dissemination involve multiple target languages; hence one can assume that, except for the very first of them, renderings in multiple languages exist and can be used as input to the next step 4 . See also (Och and Ney, 2001) for some related empirical evidence. In order to obtain a first impression of the potential of triangulation in the domain of parliament debates, we applied the selection heuristics to a set of four translations, one from Finnish, the other three the result of the selections mentioned above. Results and Discussion The BLEU scores (Papineni et al., 2002) 1 . These results show that in each group of translations for a given source language, the statistical engine came out best. Furthermore, our heuristic approach for the selection of the best among a small set of candidate translations did not result in an increase of the measured BLEU score, but typically gave a score that was only slightly better than the second best of the ingredients. This somewhat disappointing result can be explained in two ways. Apparently, the selection heuristic does not give effective estimates of translation quality for the candidates. Furthermore, the granularity on which the choices have to bee made is too coarse, i.e. the pieces for which the symbolic engines do produce better translations than the SMT engine are accompanied by too many bad choices so that the net effect is negative. Statistical Selection The other score we used was based on probabilities as computed by the trigram language model for English provided by the organizers of the task, in a representation compatible with the SRI LM toolkit (Stolcke, 2002) . However, a correct implementation for obtaining these estimates was not available in time, so the selections generated from the statistical language model could not be used for official submissions, but were generated and evaluated after the closing date. The results, also displayed in Table 1 , show that this approach can lead to slight improvements of the BLEU score, which however turn out not to be statistically sigificant in then sense of (Zhang et al., 2004) . Next Steps When we started the experiments reported here, the hope was to find relatively simple methods to select the best among a small set of candidate translations and to achieve significant improvements of a hybrid architecture over a purely statistical approach. Although we could indeed measure certain improvements, these are not yet big enough for a conclusive ""proof of concept"". We have started a refinement of our approach that can not only pick the best among translations of complete sentences, but also judge the quality of the building blocks from which the translations are composed. First informal results look very promising. Once we can replace single phrases that appear in one translation by better alternatives taken from a competing candidate, chances are good that a significant increase of the overall translation quality can be achieved. Acknowledgements This work has been funded by the Deutsche Forschungsgemeinschaft. We want to thank two anonymous reviewers for numerous pointers to relevant literature, Bogdan Sacaleanu for his help with the collection of translations from on-line MT engines, as well as the organizers of the shared task for making these interesting experiments possible.",9766288,cb55a2758d26d96dadcf301a11dadee9f703f3c0,18,https://aclanthology.org/W05-0828,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Eisele, Andreas",First Steps towards Multi-Engine Machine Translation,155--158,,,,,,,inproceedings,eisele-2005-first,,,
42,2013.mtsummit-papers.1,"This paper proposes a way of augmenting bilingual terminologies by using a ""generate and validate"" method. Using existing bilingual terminologies, the method generates ""potential"" bilingual multi-word term pairs and validates their status by searching web documents to check whether such terms actually exist in each language. Unlike most existing bilingual term extraction methods, which use parallel or comparable corpora, the proposed method can take advantage of a wider variety of textual corpora. Experiments using Japanese-English terminologies of five domains show that the method is highly promising.","This paper proposes a way of augmenting bilingual terminologies by using a ""generate and validate"" method. Using existing bilingual terminologies, the method generates ""potential"" bilingual multi-word term pairs and validates their status by searching web documents to check whether such terms actually exist in each language. Unlike most existing bilingual term extraction methods, which use parallel or comparable corpora, the proposed method can take advantage of a wider variety of textual corpora. Experiments using Japanese-English terminologies of five domains show that the method is highly promising. Introduction In this paper we propose a way of detecting new bilingual term pairs for augmenting bilingual terminologies by using a ""generate and validate"" method. Augmenting bilingual terminologies is sine qua non for terminology managers, translators and document managers (Sager, 1990) , and its importance is growing in accordance with the rapid growth of terminologies in many domains. In general, new terms they tend to be created in a systematic way by compounding (Sager, 1990; Ananiadou, 1994; Justeson and Katz, 1995; Cerbah, 2000; Kageura, 2012) , resulting in an abundance of multi-word terms (MWTs). This fact results in a tendency for the correspondences between constituent elements to be retained across languages to a substantial extent. This provides us with a chance to take advantage of the information contained in existing terminologies to augment and enrich terminologies with new terms, based on a simple idea: If a terminological lexicon contains ""linear programming,"" ""linear optimization,"" ""linear function,"" ""convex programming"" and ""convex function,"" we can reasonably assume that the term ""convex optimization,"" which is not listed in the terminology, may, or will come to, exist (Figure 1 ). By generating ""potential"" term candidates and validating their existence by using web data, it should be possible to identify a range of new terms which are not covered in existing terminologies. Assuming bilingual correspondence at the level of constituent units of terms, it is possible to extend this idea to obtain new bilingual term pairs. Based on this idea, we developed a fully operating system for detecting new bilingual term pairs in order to augment bilingual terminologies. The paper is organised as follows. Section 2 briefly looks at related work. Section 3 explains the system arrangement and the methods and algorithms adopted in the modules of the system. In particular, we detail the graph-based generation of term candidate pairs. Experimental results are introduced and discussed in section 4. Section 5 discusses remaining issues. Except for section 2, Japanese-English language pairs are assumed, Related work Since the 1990s, bilingual term extraction from parallel or comparable corpora has been actively pursued (Dagan and Church, 1997; Fung and McKeown, 1997; Gaussier, 1998; Chiao and Zweigenbaum, 2002; Kwong et al., 2004; Tonoike et al., 2005; Bernhard, 2006; Robitaille et al., 2006; Daille and Morin, 2008; Lefever et al., 2009; Sima'an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2-6, 2013), p. 3-10. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. Laroche and Langlais, 2010; Li and Gaussier, 2010; Morin et al., 2010) . Although extracting bilingual term pairs from parallel corpora generally attains higher precision than extracting them from comparable corpora, the problem of limited availability of parallel corpora has led to a great deal of research into bilingual term extraction using comparable corpora. In addition to work that has resulted in the steady improvement of algorithms, there are studies that address improvement of corpus comparability (Morin et al., 2010; Li and Gaussier, 2010) . In the EU, research into corpus-based term extraction culminated in an EU project (TTC, 2012) . While the essential information explored in these methods is the correspondence between two languages (most typically aligned segments in the case of parallel corpora and degree of correspondence between context vectors in the case of comparable corpora), some have taken advantage of the abundance of MWTs and used the translational relationships between constituent units of MWTs (Tonoike et al., 2005; Daille and Morin, 2008) . They partially take a ""generate and validate"" approach, for detecting target language expressions, although the essential framework is still oriented to ""extraction."" These corpus-based approaches have shown steady technical advancement and improvement, but the results are essentially restricted by available corpora and not anchored to existing terminologies. From the point of view of augmenting terminologies for terminological management, more ""terminology-driven"" methods, i.e. those that make use of existing terminologies, are required. Our method takes this approach; it is complementary to existing work. System and methods Overall framework of the system The system consists of three main modules: (a) the module that generates potential term candidate pairs; (b) the module that collects a set of web documents against which the existence of term candidate pairs is validated; (c) the module that validates and ranks term candidate pairs. Figure 2 : Main modules of the system Figure 2 shows the main modules of the system. Among these, module (a) constitutes the core part of our approach. Validating (and ranking) candidate pairs generated in module (a) constitutes an essential part for our method. Currently we use the web as a source against which the existence of candidate term pairs is validated because it contains many new terms, but other sources could also be used for validation. The current system configuration is such that relevant documents are crawled from the web in advance, but it would also be possible to dynamically throw generated term candidate pairs into the web search engine for validation. We did not take this approach for reasons related to search engine api and in order to controlling evaluation and diagnosis. Generating term candidate pairs The following steps are carried out to generate term candidate pairs: 1. Decompose MWTs into components (CUs); 2. Establish correspondences between source language terms (SLT; Japanese in the present context) CUs and target language terms (TLT; English) CUs; 3. Generate head-modifier pairs for SLT CUs; 4. Generate a bipartite graph based on SLT head-modifier pairs; 5. Partition the bipartite graph; 6. For each connected component of the bipartite graph, take the direct product of the head and modifier vertices to generate extended head-modifier pairs; Figure 3 : Extracting head-modifier pairs from an MWT ""data file compression"" 7. For each newly created SLT head-modifier pair, take the corresponding TLT CUs and generate corresponding TLT head-modifier pairs, then generate paired MWTs. For step 1, MeCab 1 and Stanford POS Tagger (Toutanova et al., 2003) 2 are used for decomposing terms and POS-tagging CUs for Japanese and English, respectively. We retained content units, and functional units directly attached to them. For step 2, we start from aligned CU pairs taken from simple term pairs, and extend aligned pairs by iteratively removing aligned pairs from MWT pairs that have the same number of SLT CUs and TLT CUs. In the third step, head-modifier pairs are generated for SLT CUs, using the fact that Japanese MWTs are head final. We extract all possible headmodifier pairs from each MWT, as shown in N ←|SeedT erm| 7: for i ←{1, ..  In step 4, the bipartite graph (as shown in Figure 4 ) is constructed from a set of head-modifier pairs obtained in step 3. Algorithm 1 shows the procedure in pseudo-code. The bipartite graph is generated by using only those SLT CUs which have corresponding TLT CUs (given in step 2). Taking the direct product of the head and modifier vertices for this ""raw"" bipartite graph would generate a great number of head-modifier pairs which are not likely to be possible terms, due to the existence of unmotivated bridges. Assuming that there are reasonable coherent sub-graphs that contain potential term pairs, we thus partition the bipartite graph to create components of a reasonable size in step 5. This is done by: (a) first removing bridges from the graph, and (b) then partitioning large components by using the Kernighan-Lin algorithm (Kernighan and Lin, 1970) . The Kernighan-Lin algorithm is a heuristic algorithm for partitioning connected components of a graph into two connected components of similar size. Figure 5 illustrates the process of partitioning the graph according to this procedure. A wider range of methods could potentially be applied to this step. In step 6, the direct product of the head and modifier vertices is taken for each component, generating potential SLT candidates. Finally, in step 7 term candidate pairs are generated by taking and concatenating corresponding TLT CUs. Collecting web documents Both SL and TL web documents are collected, by using SLTs and TLTs listed in terminology as a query to the search engine (Figure 6 ). To avoid collecting irrelevant documents, therefore, we combined domain keywords (the name of the domain itself, such as ""computer science,"" for example) with each query term. The top 20 documents are collected for each query. As a search engine, we currently use the Yahoo! Japan api 3 . Parallel downloading is carried out to improve speed. The obtained documents are stored using Groonga 4 , which provides efficient full text search functions. Validating term candidate pairs The generated potential term candidate pairs are validated against the web documents, and the pairs for which both the SLT candidate and the TLT candidate occur at least once in the documents are retained. Currently, the result can be ranked in accordance with the number of occurrences of either SLT or TLT candidates, their average, or according to Jaccard similarity coefficient between SLT and TLT, which is defined as: Jaccard(SLT, TLT)= H(SLT ∧ TLT) H(SLT ∨ TLT) where H is the number of hits of the term in the document set. Note that ranking by the number of SLT or TLT candidate hits provides information related to whether or not the candidate is likely to be a valid term, while the ranking by Jaccard similarity coefficient measures how likely it is that the SLT and TLT candidates are actually a corresponding pair. In the proposed framework, using the Jaccard coefficient can be regarded as redundant, as the correspondence between SLT and TLT candidates is kept by CU level correspondences. We still find it important to use the Jaccard coefficient as evaluating experimental results by using Jaccard coefficient enables us to see to what extent we can rely on separate and independent validation for SLT and TLT candidates, which provides an important clue as to the extent to which monolingual domain corpora can be used in the present framework. Experiments and evaluations Experimental setup Terminological dictionaries For evaluation, we used five terminological dictionaries of computer science (henceforth COM) 5 , economics (ECN) 6 ,law(LA W) 7 , physics (PHY) 8 and psychology (PSY) 9 . These terminological dictionaries contain Japanese-English term pairs. The number of terms listed in Table 1 . Table 2 lists the number of terms by length (by the number of constituent units). The number of Japanese-English term pairs of which the number of constituent units is the same for Japanese and English is also listed (CP). There are a few terms with no constituent units; they were produced because the POS-taggers mistakenly judged the constituent units to be functional elements rather than content words. Those listed in the rows ""CP"" with more than two constituent units are the sources of the head-modifier bipartite graph. Collecting web documents The web documents for these domains were collected from November to December 2012. In collecting the web documents, the following domain keywords were used: -‰;J ¶ (keisanki kagaku) and ""computer science"" for COM, &A ¶ (keizaigaku) and ""economics"" for ECN, O ¶ (hougaku) and ""law"" for LAW, úg ¶ (butsurigaku) and ""physics"" for PHY, and úg ¶ (shinrigaku) and ""psychology"" for PSY. Table 3 shows the number of pages obtained from the web search. The ""Japanese"" and ""English"" columns show the number of pages obtained by using Japanese and English terms, respectively. Note that the number of Japanese web pages collected for COM is much smaller than its English counterpart, while in the other five domains they are more balanced. We randomly selected 200 web pages for each domain, without distinguishing between English and Japanese pages, and checked the relevance of the pages to the domain. Table 4 shows the number of pages clearly relevant to the domain in ques- Generating term candidate pairs Table 5 shows the basic statistics of the initial head-modifier bipartite graphs (created from steps 1-4 in section 3.2), in which ""mods"" stands for modifiers, ""# comp"" shows the number of connected components, ""maxcmp"" shows the number of vertices in the maximum component, ""2nd cmp"" shows the number of vertices of the second largest component (other headers should be obvious). As in many real-world networks, these initial graphs consist of one giant component and a number of small components (Newman, 2003; Newman, 2010) . Using these initial graphs for generating potential MWT candidates would be unrealistic; a terminology of a domain cannot reasonably contain terms in the order of millions. Table 6 shows the statistics of head-modifier graphs generated by removing bridges and applying the Kernighan-Lin algorithm. Essentially, the largest components in the initial graphs were partitioned into smaller components with similar sizes, while many previously connected vertices became isolated vertices. As a result, the number of po- tential head-modifier candidates was reduced to the order of tens of thousands. Given the size of the original terminological dictionaries as well as many existing terminological dictionaries, this size seems reasonable. Quantitative evaluations Table 7 shows the number of candidate term pairs after validation (those pairs of which both Japanese and English candidates were validated at least once against the collected web documents were identified as candidate pairs). We manually evaluated (i) 100 top candidate pairs according to the Jaccard coefficient value, (ii) 100 top candidate pairs as calculated by the sum of Japanese and English hits, and (iii) 100 randomly chosen candidate pairs whose Jaccard coefficient was zero. They were evaluated from two points of view: according to (a) whether the Japanese and English matched, and (b) whether the Japanese candidate could be regarded as a term in the domain in question. For (b), we took into account cases in which the candidate was not in itself a term but could be a part of a longer term. Evaluation was carried out by two people; the results of the first evaluator were cross-checked by the other 10 . The results are listed in Table 8 . The Jaccard coefficient gave the highest performance both in terms of bilingual correspondence (pairing) and in terms of validity to the domain. This indicates that the co-occurrence of SLT and TLT in the same document provides strong evidence for a pair being both a valid pair as well as valid terms. The low performance of law was due to the fact that the terminology of law we used contained many verbal expressions, which led to CU level mismatches (see section 4.4). Unfortunately, the number of candidate pairs with a non-zero Jaccard coefficient was limited, as indicated in Table 7 . However, it can be observed that the number of hits is also useful as evidence. In the present experiment we only used the sum of Japanese and English hits; we may be able to obtain more efficient information by taking into account the balance between the hits in the two languages. Lastly, there are still relevant terms among the 100 randomly selected candidates, though the ratio of correct term pairs is much lower. To take full advantage of the proposed method, further filtering of the relevant terms from this range of candidates will be necessary. Diagnosis Changes in algorithms and parameter settings of the method, such as the bipartite graph partition algorithm or the selection of domain keywords for collecting web documents (note the substantially smaller number of Japanese computer science documents), will affect the behaviour of the system. In addition, it may be useful to make further use of the information which can be derived from the graph, such as the degree of vertices. These need to be examined systematically to optimise the performance vis-à-vis the nature of terminologies, which will be our future task in methodological front. In addition, some general error patterns were observed upon closer qualitative observation: • Especially for terms in the domain of law, errors arising from the mistreatment of postpositions and delimiting symbols in Japanese and prepositions in English were observed (e.g. the output ""«…waeˆ"" (""behaviour of consumption"") is not a valid MWT, but ""« …aeˆ"" (""consumption behaviour"") without the postposision ""w"" (""of"") could be. This problem can be solved by introducing MWT patterns or rules to restrict valid MWT forms to filter out candidates with invalid patterns. • ""Partial"" terms were often generated and validated which in themselves are not valid MWTs but constitute a part of certain longer MWTs. This problem arises from the limitation of our method in which we only generate term candidates with two constituent elements. This shortcoming can be overcome by detecting maximum MWT patterns in the web documents. Rich accumulation of research in pattern-based MWT extraction can be directly relevant for this purpose (Ananiadou, 1994; Daille et al., 1994; Justeson and Katz, 1995; Nakagawa, 2000; Takeuchi et al., 2004) . • Some candidates which were judged as nonterms consist of two CUs which both represent generic concepts. To avoide this type of error, it will be useful to make use of the weight of vertices in the ogirinal graph, as well as using ontological information or introducing the idea of ""stop words."" • Some errors arising from incorrect CU level pairing were observed as well. These can be avoided, at least partially, by introducing dictionary-based pairing of source language and target language CUs. Conclusions and outlook We proposed a way of augmenting bilingual terminologies by using a ""generate and validate"" method, taking advantage of the characteristics of terms and terminologies. The results of our experiments indicate that the method will be useful for collecting term candidate pairs to be included in existing terminological dictionaries. The system which carries out this task is fully operational, although there is an uncertainty as to the free availability of the search engine api in future. For our system to be used in the real world in a Japanese-English setting, it should be complemented by methods which can detect and collect Japanese borrowed terms written in katakana,a s they tend to be used for introducing new singular terms (Kageura, 2012) . For this task, the ""collect and validate"" framework proposed by (Sato, 2010) would be useful, even though it was developed for collecting proper names. In the next stage, we will evaluate the usefulness of the system in vivo rather than in vitro,i n cooperation with dictionary companies, academic societies managing terminologies, and document management divisions of companies. A Japanese dictionary company has already expressed interest in trying our system in the process of revising some of its dictionaries.",43469028,3a3068d34762834a49222bf15b666fe772e2a705,5,https://aclanthology.org/2013.mtsummit-papers.1,,"Nice, France",2013,September 2-6,Proceedings of Machine Translation Summit XIV: Papers,"Sato, Koichi  and
Takeuchi, Koichi  and
Kageura, Kyo",Terminology-driven Augmentation of Bilingual Terminologies,,,,,,,,inproceedings,sato-etal-2013-terminology,,,
43,2009.mtsummit-posters.16,,ÄÁAE ¸AE ÒØ × ÍÒ Ú Ö× ØÝ¸¾¸ÖÙ Ð ÀÓÙ×× Ò Ö ¸ È ¾¾¼ ¿¾¾ AE ÒØ × Ü ¼¿ ÝÙ ºÒ ÓÙÒ Ú¹Ò ÒØ ×º Ö ×ØÖ Ø Ò ÓØ ÐÐÝ¸ËØ Ø ×Ø Ð Å Ò ÌÖ Ò×Ð Ø ÓÒ ÛÓÖ × ÑÙ ØØ Ö Ò ×ÓÑ Ð Ò Ù Ô Ö× Ø Ò ÓØ Ö×¸ ÙØ Ñ Ø Ó ÓÐÓ Ð ÔÖÓ Ð Ñ× Ñ Ò Ø Ø Ø × ¢ÙÐØ ØÓ Ö Û Ö ÓÒ¹ ÐÙ× ÓÒ×º ÁÒ Ô ÖØ ÙÐ Ö¸ Ø × Ò Ö ÐÐÝ ÙÒ¹ Ð Ö Û Ø Ö ØÖ Ò×Ð Ø ÓÒ× Ò Ô Ö ÐÐ Ð ØÖ Ò¹ Ò ÓÖÔÓÖ Ú Ò ÔÖÓ Ù Ù× Ò ÕÙ Ú¹ Ð ÒØ ÓÒÚ ÒØ ÓÒ×º ÁÒ Ø × Ô Ô Ö¸Û Ö ÔÓÖØ ÓÒ Ò ÜÔ Ö Ñ ÒØ Û Ö ×Ñ ÐÐ¹ÚÓ ÙÐ ÖÝ ÑÙÐØ Ð Ò Ù Ð ÒØ ÖÐ Ò Ù ¹ × ØÖ Ò×Ð Ø ÓÒ ×Ý×¹ Ø Ñ Û × Ù× ØÓ Ò Ö Ø Ø ØÓ ØÖ Ò ËÅÌ ÑÓ Ð× ÓÖ Ø ½¾ Ô Ö× ÒÚÓÐÚ Ò Ø Ð Ò¹ Ù × Ò Ð × ¸ Ö Ò ¸Â Ô Ò × ¸ Ö º Ý ÓÒ×ØÖÙØ ÓÒ¸Ø Ø Ò ××ÙÑ ×ØÖÓÒ ÐÝ ÙÒ ÓÖÑº × ÜÔ Ø ¸ØÖ Ò×Ð Ø ÓÒ ØÛ Ò Ò Ð × Ò Ö Ò Ò ÓØ Ö ¹ Ø ÓÒ× Ô Ö ÓÖÑ ÑÙ ØØ Ö Ø Ò ØÖ Ò×Ð Ø ÓÒ ÒÚÓÐÚ Ò Â Ô Ò × º Ä ×× Ó Ú ÓÙ×ÐÝ¸ØÖ Ò×Ð ¹ Ø ÓÒ ÖÓÑ Ò Ð × Ò Ö Ò ØÓ Ö Ô Ö¹ ÓÖÑ ÔÔÖÓÜ Ñ Ø ÐÝ × Û ÐÐ × ØÖ Ò×Ð Ø ÓÒ ØÛ Ò Ò Ð × Ò Ö Ò ¸ Ò ØÖ Ò×Ð Ø ÓÒ ØÓ Â Ô Ò × Ô Ö ÓÖÑ ØØ Ö Ø Ò ØÖ Ò×Ð Ø ÓÒ ÖÓÑ Â Ô Ò × º ½ ÁÒØÖÓ ÙØ ÓÒ Ë Ò Ø× ÒØÖÓ ÙØ ÓÒ Ò Ø ÖÐÝ ¼×¸Û Ò Ø Û × Ö Ö × Ù ÓÙ× ÓÙØ× Ö¸ËØ Ø ×Ø Ð Å Ò ÌÖ Ò×Ð Ø ÓÒ ´ËÅÌµ × Ö Ô ÐÝ Ò ÖÓÙÒ ÙÒØ Ð Ø × ÒÓÛ ÓÒ× Ö Ø Ñ Ò×ØÖ Ñ ÔÔÖÓ º Ì Ö ×¸ ÓÛ Ú Ö¸ Ò Ö Ð Ö Ñ ÒØ Ø Ø ×ÓÑ Ð Ò Ù ¹ Ô Ö× ÛÓÖ ÑÙ ØØ Ö Ø Ò ÓØ Ö×º ÁÒ Ø ÔÓ× Ø Ú Ö Ø ÓÒ¸Ø ÖÐÝ ×Ù ×× × Ö ÔÓÖØ Ý Ø Á Å ÖÓÙÔ ´ ÖÓÛÒ Ø Ðº¸½ ¼µ Ù× Ö Ò ¹ Ò Ð × Û × ÒÓÛ ÒÓÛÒ ØÓ Ò ÙÒÙ×Ù ÐÐÝ ×Ù Ø Ð Ô Ö ´ÃÓ Ò Ò ÅÓÒÞ¸¾¼¼ ÃÓ Ò Ò ÅÓÒÞ¸¾¼¼ µº Ò ÓØ ÐÐÝ¸ØÖ Ò×Ð Ø Ò ØÛ Ò ØÛÓ ÙÖÓÔ Ò Ð Ò¹ Ù × × × Ö Ø Ò ØÖ Ò×Ð Ø Ò ØÛ Ò ÙÖÓ¹ Ô Ò Ò ÒÓÒ¹ ÙÖÓÔ Ò Ð Ò Ù ¸ Ò ×ÓÑ Ð Ò¹ Ù ×¸Ð Â Ô Ò × ¸ Ö Û ÐÝ ××ÙÑ ØÓ ¹ ¢ÙÐØº Ú Ò Ø ×Ø ÐÝ ÒÖ × Ò ÑÔÓÖØ Ò Ó ÅÌ Ø ÒÓÐÓ Ý¸ Ø × Ó Ø Ò ÑÔÓÖØ ÒØ ØÓ Ð ØÓ Ñ Ö ×ÓÒ Ð Ù ×× Ø ÓÛ Û ÐÐ ËÅÌ Û ÐÐ ÛÓÖ ÓÖ Ò Û Ð Ò Ù ¹Ô Öº ÓØ ËÅÌ Ò Ê ÅÌ Ö ¹ ÕÙ Ö Ð Ö ÒÚ ×ØÑ ÒØ Ó ÓÖØ ÓÖ ÒÝ Ú ÐÙ¹ Ð ×Ý×Ø Ñ Ñ Ö × Û Ò ÔÐ ÒÒ Ò ÔÖÓ Ø¸ ÓØ Ö Ø ØÙÖ × Ö Ò ÔÖ Ò ÔÐ ÔÓ×× Ð ¸ Ò Ø × × Ö¹ Ð ØÓ Ð ØÓ Ñ Ò Ò ÓÖÑ Ó ØÛ Ò Ø Ñ Ø Ò ÖÐÝ ×Ø º Ö ÒØ Ð Ö ¹× Ð ×ØÙ Ý ´ Ö Ø Ðº¸¾¼¼ µ¸Ù×¹ Ò Ø ½½¼ Ð Ò Ù ¹Ô Ö× ÓÚ Ö Ý Ø ÙÖÓÔ ÖÐ ÓÖÔÙ×¸ ÓÙÒ Ø Ø Ø ØÙÖ × ÑÓ×Ø ÔÖ Ø Ú Ó ËÅÌ ØÖ Ò×Ð Ø ÓÒ ÕÙ Ð ØÝ Û Ö Ø Ö Ø Ð Ò Ù ÚÓ¹ ÙÐ ÖÝ × Þ ¸Ð Ü Ó×Ø Ø Ð Ö Ð Ø Ò ×× ´Ñ ×ÙÖ Ò Ø ÖÑ× Ó ÔÖÓÔÓÖØ ÓÒ Ó Ó Ò Ø ÛÓÖ ×µ¸ Ò × Ñ¹ Ð Ö ØÝ Ò ÛÓÖ ÓÖ Öº Ì × Ñ ×ØÙ Ý¸ ÓÛ Ú Ö¸ Ð×Ó Ð Ø Ø Ñ Ø Ó ÓÐÓ Ð ÔÖÓ Ð Ñ× Ò Ö¹ ÒØ Ò ÖÖÝ Ò ÓÙØ Ø × ØÝÔ Ó ÓÑÔ Ö ×ÓÒº × Ð¹ Ö Ý ÒÓØ ¸Ø Ö Ø ÚÓ ÙÐ ÖÝ × Þ ØÙÖÒ ÓÙØ ØÓ Ø ÑÓ×Ø ÔÖ Ø Ú ØÙÖ º ÎÓ ÙÐ ÖÝ × Þ ¸ ÓÛ¹ Ú Ö¸ Ô Ò × ÖÙ ÐÐÝ ÓÒ ÓÛ ÑÓÖÔ ÓÐÓ Ý × Ø Ò ÒØÓ ÓÙÒØº ÓÖ Ü ÑÔÐ ¸´ Ö Ø Ðº¸¾¼¼ µ ÓÒ¹ × Ö Ø Ø ËÛ × ÑÙ Ð Ö Ö ÚÓ ÙÐ ÖÝ × Þ Ø Ò Ò Ð × ¸ ÙØ Ø × × ÐÑÓ×Ø ÒØ Ö ÐÝ Ù ØÓ Ø Ø Ø Ø ËÛ × ¸Ð ÖÑ Ò¸ÛÖ Ø × ÓÑ¹ ÔÓÙÒ ÒÓÑ Ò Ð× Û Ø ÓÙØ ÒØ ÖÚ Ò Ò ×Ô ×º Ì ×ØÖÙØÙÖ Ó Ø × ÒÓÑ Ò Ð×¸ ÓÛ Ú Ö¸ × Ó Ø Ò Ú ÖÝ × Ñ Ð Ö ØÓ Ø Ø Ó Ø ÓÖÖ ×ÔÓÒ Ò Ò Ð × Ô Ö × ×º Ì ÔÖÓ Ð Ñ ÓÑ × ÑÓÖ ÙØ Ò Ð Ò Ù Ð Â Ô Ò × ¸Û × ÒÓÖÑ ÐÐÝ ÛÖ ØØ Ò Û Ø ÒÓ ÛÓÖ ÓÙÒ Ö × Ø ÐÐº ÒÓØ Ö × Ø Ó ××Ù × Ö × ÖÓÑ Ø Ù× Ó Ô Ö Ð¹ Ð Ð ÙÑ Ò¹ØÖ Ò×Ð Ø ÓÖÔÓÖ ¸Û Ö Ø × Ò Ö ÐÐÝ ¢ÙÐØ ØÓ ÒÓÛ Û Ø Ö Ø Ø × ØÖÙÐÝ ÙÒ ÓÖÑº ÉÙ Ð ØÝ Ò ×ØÝÐ Ó ØÖ Ò×Ð Ø ÓÒ Ò Ú ÖÝ Û ÐÝ¸Û Ø ØÖ Ò×Ð ØÓÖ× Ù× Ò Ö ÒØ Ù Ð Ò ×º ÁÒ Ô ÖØ ÙÐ Ö× ÓÑ ØÖ Ò×Ð ØÓÖ× Û ÐÐ ÔÖ Ö ÑÓÖ Ð Ø Ö Ð ×ØÝÐ Ø Ò ÓØ Ö×º ÁØ × Ð×Ó ÓÑÑÓÒ ØÓ Ñ Ü Ò ÐÓÛ¹ÕÙ Ð ØÝ Ø Ö ÕÙ ÒØ Ó × ØÖ Ò×Ð Ø ÓÒ× Ø Ò ÖÓÑ Ø Ö Ú Ö× Ð Ò Ù ¹Ô Ö¸Û Ø Ø ×ÓÙÖ Ò Ø Ö Ø ×Û ÔÔ ÖÓÙÒ º ËÓÑ Ö ÒØ ×ØÙ × ´ÇÞ ÓÛ× 3¼¼ µ Ú Ò Ø ×Ù ×Ø Ø Ø Ø × Ò Ó ÐÓÛ¹ ÕÙ Ð ØÝ ÙÐØ Ö Ø ÓÒ Ò Ó ÑÓÖ ÖÑ Ø Ò ÓÓ º ÓÒÚ Ö× ÐÝ¸ÓØ Ö ÔÖ Ø Ø ÓÒ Ö× Ó ËÅÌ Ú ÔÓ ÒØ ØÓ Ø Ô Ö ÓÖÑ Ò Ò× Ø Ø Ò Ú Ý Ö ÙÐ Ð Ò Ò Ó Ø Ø º Ï Ø ÓÙØ ÓÒØÖÓÐÐ Ò ÓÖ ÐÐ Ø × ØÓÖ×¸ Ø × Ö ØÓ ÒÓÛ ÓÛ Ò Ö Ð Ø Ö ×ÙÐØ× Ó ÓÑÔ Ö Ø Ú ×ØÙ ¹ × Ö ÐÐÝ Ö º ÐØ ÓÙ ´ Ö Ø Ðº¸¾¼¼ µ × Ò ÙÒ¹ Ù×Ù ÐÐÝ Ö ×ÔÓÒ× Ð Ò Ö ÙÐ Ô Ó ÛÓÖ ¸Ø ÙØ ÓÖ× ÔÓ ÒØ ÓÙØ Ø Ø Ö ÑÓÚ Ð Ó Ø ÓÙØÐ Ö Ð Ò¹ Ù ´ ÒÒ × µ ×Ù ×Ø ÒØ ÐÐÝ Ò × Ø ÓÚ Ö ÐÐ ÓÒÐÙ× ÓÒ× Ø × ÔÖÓ ÐÝ ÒÓØ Ó Ò Ò Ø Ø ÒÒ × Û × Ð×Ó Ø ÓÒÐÝ ÒÓÒ¹ÁÒ Ó¹ ÙÖÓÔ Ò Ð Ò¹ Ù Ù× Ò Ø ×ØÙ Ýº ÁÒ Ø × Ô Ô Ö¸Û ÔÖ × ÒØ Ø Ö ×ÙÐØ× Ó ÒÓÚ Ð ØÝÔ Ó ÓÑÔ Ö Ø Ú ×ØÙ Ý ÖÖ ÓÙØ Ù× Ò Å ËÄÌ¸ ×Ñ ÐÐ¹ÚÓ ÙÐ ÖÝ ÒØ ÖÐ Ò Ù ¹ × ÑÙÐØ Ð Ò Ù Ð ×Ô ØÖ Ò×Ð Ø ÓÒ ×Ý×Ø Ñ ÓÖ Ñ Ð ÓÑ Òº Ï Ò Ö Ø Ô Ö ÐÐ Ð ÓÖÔÓÖ ÓÖ ÐÐ ½¾ Ô Ö× ÒÚÓÐÚ Ò Ø ×ÓÙÖ Ð Ò Ù × Ò Ð × ¸ Ö Ò ¸Â Ô Ò × ¸ Ö ¸¢Ö×Ø Ù× Ò Ø ×ÓÙÖ Ð Ò Ù Ö ÑÑ Ö× ØÓ Ò Ö Ø Ö ØÖ ÖÝ ÑÓÙÒØ× Ó ×ÓÙÖ ¹Ð Ò Ù Ø Ø Ò¸ ÓÖ Ø Ö Ø Ð Ò Ù ¸Ô ×× Ò Ø Ø ÖÓÙ Ø Ö Ð Ú ÒØ ØÖ Ò×Ð Ø ÓÒ ÖÙÐ × ØÓ Ò Ö Ø Ø Ö Ø Ð Ò Ù ÜÔÖ ×× ÓÒ×º Í× Ó ÒØ ÖÐ Ò Ù ¹ × ØÖ Ò×Ð Ø ÓÒ Ò¹ ÓÖ × ÙÒ ÓÖÑ ØÖ Ò×Ð Ø ÓÒ ×ØÝÐ º Ì ×Ñ ÐÐ Ó¹ Ñ Ò¸Û Û ÓÑÔÐ Ø ÐÝ ÓÒØÖÓÐ¸Ñ Ø ÔÓ×× Ð ØÓ Ò ÓÖ ÙÒ ÓÖÑ × ÓÒ× ÓÙØ ÓÛ ÑÓÖÔ ÓÐÓ Ý × ØÖ Ø º ÓÖ Ü ÑÔÐ ¸Û Ò Ö ØÓ ×ÔÐ Ø Ó Ø ¢Ò Ø ÖØ Ð Ð¸ÒÓÖÑ ÐÐÝ ¢Ü ØÓ Ø ÓÐ¹ ÐÓÛ Ò ÒÓÙÒ¸ Ò ØÖ Ø Ø × × Ô Ö Ø ÛÓÖ º ÓÖ × Ñ Ð Ö Ö ×ÓÒ×¸Û Ð×Ó ØÖ Ø Â Ô Ò × Ø Ò× Ò ÔÓÐ Ø Ò ×× ¢Ü × × × Ô Ö Ø ÛÓÖ ×º Ì Ù× ÛÓÖ Ð Ó ÓÖ Ñ × Ø ´ã ÔÔ Ò äµ × ×ÔÐ Ø ÙÔ × Ó ÓÖ Ñ × Ø ´ã ÔÔ Ò È ËÌ¹ÈÇÄÁÌ äµº ÇÒ Û Ö Ø Ø Ô Ö ÐÐ Ð ÓÖÔÓÖ ¸Û ØÖ Ò ËÅÌ ÑÓ ¹ Ð×¸ Ò Ú ÐÙ Ø Ø ÕÙ Ð ØÝ Ó Ø ØÖ Ò×Ð Ø ÓÒ× Ø Ý ÔÖÓ Ù º × ÜÔ Ø ¸ØÖ Ò×Ð Ø ÓÒ ØÛ Ò Ò Ð × Ò Ö Ò Ò ÓØ Ö Ø ÓÒ× Ô Ö ÓÖÑ ÑÙ Ø¹ Ø Ö Ø Ò ØÖ Ò×Ð Ø ÓÒ ÒÚÓÐÚ Ò Â Ô Ò × º Ï Û Ö ¸ ÓÛ Ú Ö¸ ÒØ Ö ×Ø ØÓ ×ÓÚ Ö Ø Ø ØÖ Ò×Ð Ø ÓÒ ÖÓÑ Ò Ð × Ò Ö Ò ØÓ Ö Ô Ö ÓÖÑ × Û ÐÐ × ØÖ Ò×Ð Ø ÓÒ ØÛ Ò Ò Ð × Ò Ö Ò ¸ Ò Ø Ø ØÖ Ò×Ð Ø ÓÒ ØÓ Â Ô Ò × Ô Ö ÓÖÑ ØØ Ö Ø Ò ØÖ Ò×¹ Ð Ø ÓÒ ÖÓÑ Â Ô Ò × º Ì Ö ×Ø Ó Ø Ô Ô Ö × ÓÖ Ò × × ÓÐÐÓÛ×º Ë ¹ Ø ÓÒ ¾ ÔÖÓÚ × ÖÓÙÒ ÓÒ Ø Å ËÄÌ ×Ý×¹ Ø Ñº Ë Ø ÓÒ ¿ ×Ö × Ø ÜÔ Ö Ñ ÒØ Ð Ö Ñ ¹ ÛÓÖ ¸ Ò Ë Ø ÓÒ Ø Ö ×ÙÐØ× Ó Ø Ò º Ë Ø ÓÒ ÓÒÐÙ ×º ¾ Ì Å ËÄÌ ËÝ×Ø Ñ Å ËÄÌ ´ ÓÙ ÐÐÓÒ Ø Ðº¸¾¼¼ µ × Ñ ÙÑ¹ ÚÓ ÙÐ ÖÝ ÒØ ÖÐ Ò Ù ¹ × ÇÔ Ò ËÓÙÖ ×Ô ØÖ Ò×Ð Ø ÓÒ ×Ý×Ø Ñ ÓÖ ÓØÓÖ¹Ô Ø ÒØ Ñ Ð Ü¹ Ñ Ò Ø ÓÒ ÕÙ ×Ø ÓÒ×¸Û ÔÖÓÚ × ÒÝ¹Ð Ò Ù ¹ ØÓ¹ ÒÝ¹Ð Ò Ù ØÖ Ò×Ð Ø ÓÒ Ô Ð Ø × ÓÖ ÐÐ Ð Ò¹ Ù × Ò Ø × Ø Ò Ð × ¸ Ö Ò ¸Â Ô Ò × ¸ Ö ¹ ¸ Ø Ð Ò º ÓØ ×Ô Ö Ó Ò Ø ÓÒ Ò ØÖ Ò×¹ Ð Ø ÓÒ Ö ÖÙÐ ¹ × º ËÔ Ö Ó Ò Ø ÓÒ ÖÙÒ× ÓÒ Ø AEÙ Ò º Ö Ó Ò Ø ÓÒ ÔÐ Ø ÓÖÑ¸Û Ø Ö ÑÑ Ö¹ × Ð Ò Ù ÑÓ Ð× Ù ÐØ Ù× Ò Ø ÇÔ Ò ËÓÙÖ Ê ÙÐÙ× ÓÑÔ Ð Öº × ×Ö Ò ´Ê ÝÒ Ö Ø Ðº¸¾¼¼ µ¸ ÓÑ Ò¹×Ô ¢ Ð Ò Ù ÑÓ Ð × ÜØÖ Ø ÖÓÑ Ò Ö Ð Ö ×ÓÙÖ Ö ÑÑ Ö Ù×¹ Ò ÓÖÔÙ×¹ × Ñ Ø Ó × Ö Ú Ò Ý × ÓÖ¹ ÔÙ× Ó ÓÑ Ò¹×Ô ¢ Ü ÑÔÐ ×º Ì × ÓÖÔÙ×Û ØÝÔ ÐÐÝ ÓÒØ Ò× ØÛ Ò ¼¼ Ò ½ ¼¼ ÙØ¹ Ø Ö Ò ×¸ × Ø Ò Ù× × ÓÒ Ø Ñ ØÓ ÔÖÓ ¹ Ð ×Ø Û Ø× ØÓ Ø Ö ÑÑ Ö ÖÙÐ × Ø × ×Ù ×Ø Ò¹ Ø ÐÐÝ ÑÔÖÓÚ × Ö Ó Ò Ø ÓÒ Ô Ö ÓÖÑ Ò ´Ê ÝÒ Ö Ø Ðº¸¾¼¼ ¸Ü½½º µº È Ö ÓÖÑ Ò Ñ ×ÙÖ × ÓÖ ×Ô Ö Ó Ò Ø ÓÒ Ò Ø Ø Ö Ð Ò Ù × Û Ö × Ö ÓÙ× Ú ÐÙ Ø ÓÒ× Ú Ò ÖÖ ÓÙØ Ö × ÓÛÒ Ò Ì ¹ Ð ½º Ø ÖÙÒ¹Ø Ñ ¸Ø Ö Ó Ò × Ö ÔÖÓ Ù × ×ÓÙÖ ¹ Ð Ò Ù × Ñ ÒØ Ö ÔÖ × ÒØ Ø ÓÒº Ì × × ¢Ö×Ø ØÖ Ò×¹ Ð Ø Ý ÓÒ × Ø Ó ÖÙÐ × ÒØÓ Ò ÒØ ÖÐ Ò Ù Ð ÓÖÑ¸ Ò Ø Ò Ý × ÓÒ × Ø ÒØÓ Ø Ö Ø Ð Ò Ù Ö Ô¹ Ö × ÒØ Ø ÓÒº Ø Ö Ø¹Ð Ò Ù Ê ÙÐÙ× Ö ÑÑ Ö ÓÑÔ Ð ÒØÓ Ò Ö Ø ÓÒ ÓÖÑ¸ØÙÖÒ× Ø × ÒØÓ ÓÒ ÓÖ ÑÓÖ ÔÓ×× Ð ×ÙÖ ×ØÖ Ò ×¸ Ø Ö Û × Ø Ó Ò Ö Ø ÓÒ ÔÖ Ö Ò × Ô × ÓÒ ÓÙØº Ò ÐÐÝØ × Ð Ø ×ØÖ Ò × Ö Ð × Ò ×ÔÓ Ò ÓÖÑº ÊÓ¹ Ù×ØÒ ×× ××Ù × Ö Ö ×× Ý Ñ Ò× Ó ¹ÙÔ Ä Ò Ù Ï Ê Ë Ñ Ê Ò Ð × ± ½½± Ö Ò ± ½¼± Â Ô Ò × ¿± ± Ì Ð ½ Ê Ó Ò Ø ÓÒ Ô Ö ÓÖÑ Ò ÓÖ Ò Ð × ¸ Ö Ò Ò Â Ô Ò × Å ËÄÌ Ö Ó Ò × Ö×º ãÏ Êä ÏÓÖ ÖÖÓÖ Ê Ø ÓÖ ×ÓÙÖ Ð Ò Ù Ö Ó Ò × Ö¸ÓÒ Ò¹ÓÚ Ö Ñ ¹ Ø Ö Ð ãË Ñ Êä × Ñ ÒØ ÖÖÓÖ Ö Ø ´ÔÖÓÔÓÖØ ÓÒ Ó ÙØ¹ Ø Ö Ò × Ð Ò ØÓ ÔÖÓ Ù ÓÖÖ Ø ÒØ ÖÐ Ò Ù µ ÓÖ ×ÓÙÖ Ð Ò Ù Ö Ó Ò × Ö¸ÓÒ Ò¹ÓÚ Ö Ñ Ø Ö Ðº ×Ø Ø ×Ø Ð Ö Ó Ò × Ö¸Û Ö Ú × ÖÓ Ù×Ø Ñ ¹ ÐÔ ×Ý×Ø Ñº Ì ÔÙÖÔÓ× Ó Ø ÐÔ ×Ý×Ø Ñ ´ ØÞ Ö × ¢× Ø Ðº¸¾¼¼ µ × ØÓ Ù Ø Ù× Ö ØÓ¹ Û Ö × ×ÙÔÔÓÖØ ÓÚ Ö Ø Ô Ö ÓÖÑ× ÔÔÖÓÜ Ñ Ø Ñ Ø Ò Ó ÓÙØÔÙØ ÖÓÑ Ø ×Ø Ø ×Ø Ð Ö Ó Ò × Ö Ò Ð Ö ÖÝ Ó × ÒØ Ò × Û Ú Ò Ñ Ö × ÓÖÖ ØÐÝ ÔÖÓ ×× ÙÖ Ò ×Ý×Ø Ñ Ú ÐÓÔÑ ÒØ¸ Ò Ø Ò ÔÖ × ÒØ× Ø ÐÓ× ×Ø Ñ Ø × ØÓ Ø Ù× Öº Ü ÑÔÐ × Ó ØÝÔ Ð Ò Ð × ÓÑ Ò × ÒØ Ò × Ò Ø Ö ØÖ Ò×Ð Ø ÓÒ× ÒØÓ Ö Ò ¸ Ö Ò Â Ô Ò × Ö × ÓÛÒ Ò ÙÖ ¾º ¿ ÜÔ Ö Ñ ÒØ Ð Ö Ñ ÛÓÖ ÁÒ Ø Ð Ø Ö ØÙÖ ÓÒ Ð Ò Ù ÑÓ ÐÐ Ò ¸Ø Ö × ÒÓÛÒ Ø Ò ÕÙ ÓÖ ÓÓØ×ØÖ ÔÔ Ò ×Ø Ø ×Ø Ð Ð Ò¹ Ù ÑÓ Ð ´ËÄÅµ ÖÓÑ Ö ÑÑ Ö¹ × Ð Ò¹ Ù ÑÓ Ð ´ ÄÅµº Ì Ö ÑÑ Ö Û ÓÖÑ× Ø × × Ó Ø ÄÅ × × ÑÔÐ Ö Ò ÓÑÐÝ Ò ÓÖ¹ Ö ØÓ Ö Ø Ò Ö ØÖ Ö ÐÝ Ð Ö ÓÖÔÙ× Ó Ü Ñ¹ ÔÐ × Ø × Ü ÑÔÐ × Ö Ø Ò Ù× × ØÖ Ò Ò ÓÖ¹ ÔÙ× ØÓ Ù Ð Ø ËÄÅ ´ÂÙÖ × Ý Ø Ðº¸½ ÂÓÒ×ÓÒ3 ¼¼ µº Ï ÔØ Ø × ÔÖÓ ×× Ò ×ØÖ Ø ÓÖÛ Ö Û Ý ØÓ ÓÒ×ØÖÙØ Ò ËÅÌ ÑÓ Ð ÓÖ Ú Ò Ð Ò Ù Ô Ö¸Ù× Ò Ø ×ÓÙÖ Ð Ò Ù Ö ÑÑ Ö¸Ø ×ÓÙÖ ¹ ØÓ¹ ÒØ ÖÐ Ò Ù ØÖ Ò×Ð Ø ÓÒ ÖÙÐ ×¸Ø ÒØ ÖÐ Ò Ù ¹ØÓ¹ Ø Ö Ø¹Ð Ò Ù ÖÙÐ ×¸ Ò Ø Ø Ö Ø Ð Ò Ù Ò Ö¹ Ø ÓÒ Ö ÑÑ Öº Ï ×Ø ÖØ Ò Ø × Ñ Û Ý¸Ù× Ò Ø ×ÓÙÖ Ð Ò Ù Ö ÑÑ Ö ØÓ Ù Ð Ö Ò ÓÑÐÝ Ò¹ Ö Ø ×ÓÙÖ Ð Ò Ù ÓÖÔÙ× × × ÓÛÒ Ò ´ÀÓ Ý Ø Ðº¸¾¼¼ µ¸ Ø × ÑÔÓÖØ ÒØ ØÓ Ú ÔÖÓ Ð ×¹ Ø Ö ÑÑ Öº Ï Ø Ò Ù× Ø ÓÑÔÓ× Ø ÓÒ Ó Ø ÓØ Ö ÓÑÔÓÒ ÒØ× ØÓ ØØ ÑÔØ ØÓ ØÖ Ò×Ð Ø ×ÓÙÖ Ð Ò Ù × ÒØ Ò ÒØÓ Ø Ö Ø Ð Ò Ù ÕÙ Ú Ð ÒØ¸ × Ö Ò Ø Ü ÑÔÐ × ÓÖ Û ÒÓ ØÖ Ò×Ð Ø ÓÒ × ÔÖÓ Ù º Ì Ö ×ÙÐØ × Ò Ð Ò Ð Ò Ù Ð ÓÖÔÙ× Ó Ö ØÖ ÖÝ × Þ ¸Û Ò Ù× ØÓ ØÖ Ò Ò ËÅÌ ÑÓ Ðº Ï Ù× Ø × Ñ Ø Ó ØÓ Ò Ö Ø Ð Ò ÓÖÔÓÖ ÓÖ ½¾ Å ËÄÌ Ð Ò Ù Ô Ö× Û Ø ×ÓÙÖ Ò Ø Ö¹ Ø Ð Ò Ù × Ø Ò ÖÓÑ Ø × Ø Ò Ð × ¸ Ö Ò Â Ô Ò × ¸ Ö º ÓÖ Ð Ò Ù Ô Ö¸Û ¢Ö×Ø Ò Ö Ø ÓÒ Ñ ÐÐ ÓÒ ×ÓÙÖ ¹Ð Ò Ù ÙØØ Ö¹ Ò × Û Ò ÜØ ¢ÐØ Ö Ø Ñ ØÓ Ô ÓÒÐÝ Ü ÑÔÐ × Û Û Ö ÙÐÐ × ÒØ Ò ×¸ × ÓÔÔÓ× ØÓ ÐÐ ÔØ Ð Ô Ö × ×¸ Ò Ù× Ø ØÖ Ò×Ð Ø ÓÒ ÖÙÐ × Ò Ø Ö Ø¹ Ð Ò Ù Ò Ö ØÓÖ× ØÓ ØØ ÑÔØ ØÓ ØÖ Ò×Ð Ø × Ò¹ Ø Ò º Ì × Ö Ø ØÛ Ò ¾ ¼Ã Ò ¿½¼Ã Ð Ò × ÒØ Ò ¹Ô Ö× ÓÖ Ð Ò Ù ¹Ô Öº ÁÒ ÓÖ Ö ØÓ Ñ ÓÚ Ö ÙÒ ÓÖÑ ÓÖ ×ÓÙÖ Ð Ò Ù Û ÔØ ÓÒÐÝ Ø Ô Ö× ÓÖ Û Ø ×ÓÙÖ × ÒØ Ò ØÖ Ò×Ð Ø ÓÒ× Ò ÐÐ Ø Ö Ø Ð Ò Ù ×º Ì × Ñ × Ø ÔÓ×× Ð ØÓ ÓÑÔ Ö ÖÐÝ ØÛ Ò Ð Ò Ù ¹Ô Ö× Û Ø Ø × Ñ ×ÓÙÖ ¹Ð Ò Ù º ÁÒ ÓÒØÖ ×Ø¸ Ø Ô¹ Ô Ö× ØÓ Ù× Ø Ø Ø × Ð ×× ×ØÖ Ø ÓÖÛ Ö ØÓ ÓÑ¹ Ô Ö ÖÓ×× Ð Ò Ù ¹Ô Ö× Û Ø Ö ÒØ ×ÓÙÖ ¹ Ð Ò Ù ×¸× Ò Ø Ö × ÒÓ Ó Ú ÓÙ× Û Ý ØÓ × ÖØ Ò Ø Ø Ø ØÛÓ ×ÓÙÖ ¹Ð Ò Ù ÓÖÔÓÖ Ö Ó ÓÑÔ ¹ Ö Ð ¢ÙÐØÝº Ì × Þ × Ó Ø ¢Ò Ð ×ÓÙÖ Ð Ò Ù ÓÖÔÙ× ÓÖ Ó Ø Ø Ö ×ÓÙÖ Ð Ò Ù × × × ÓÛÒ Ò Ì ¹ Ð ¿º Ï Ö Ò ÓÑÐÝ Ð ÓÙØ ¾º ± Ó Ó Ø × × Ø× × Ú ÐÓÔÑ ÒØ Ø ¸ Ò ¾º ± × Ø ×Ø Ø º Í×¹ Ò Þ ••¸ÅÓ× × Ò ËÊÁÄÅ ´Ç Ò AE Ý¸¾¼¼¼ ÃÓ Ò Ø Ðº¸¾¼¼ ËØÓÐ ¸¾¼¼¾µ¸Û ØÖ Ò ËÅÌ ÑÓ Ð× ÖÓÑ ÒÖ × Ò ÐÝ Ð Ö ×Ù × Ø× Ó Ø ØÖ Ò¹ Ò ÔÓÖØ ÓÒ¸Ù× Ò Ø Ú ÐÓÔÑ ÒØ ÔÓÖØ ÓÒ Ò Ø Ù×Ù Ð Û Ý ØÓ ÓÔØ Ñ Þ Ô Ö Ñ Ø Ö Ú ÐÙ ×º Ò ÐÐÝ¸Û Ù× Ø Ö ×ÙÐØ Ò ÑÓ Ð× ØÓ ØÖ Ò×Ð Ø Ø Ø ×Ø ÔÓÖ¹ Ø ÓÒº Ï Ô Ö ÓÖÑ Ø Ø ×Ø× Û Ø ×Ù ÓÖÔÓÖ Ó ¹ Ö ÒØ × Þ × Ò ÓÖ Ö ØÓ × Ø × Ý ÓÙÖ× ÐÚ × Ø Ø Ô Ö ÓÖ¹ Ñ Ò ØÓÔÔ ÓÙØ¸ Ò Ø Ø Ò Ö Ø ÓÒ Ó ÙÖØ Ö ØÖ Ò Ò Ø ÛÓÙÐ ÒÓØ ÑÔÖÓÚ Ô Ö ÓÖÑ Ò º ÙÐÐ Ø Ð× Ö ÔÖ × ÒØ Ò ´Ê ÝÒ Ö Ø Ðº¸¾¼¼ µº Ä Ò Ù Ë ÒØ Ò × ÏÓÖ × ÎÓ º Ò ¾¿ ¿ ¼ ½ ½¾ ¿ ¿ Ö ½ ½¾¼ ¿¼ Ö ¾¿¿½ ½ ½ ¼ ¾ ¿ Â Ô ¾¼ ½ ½½ ½¼ ¿¿ Ì Ð ¿ ËØ Ø ×Ø × ÓÖ ¢Ò Ð ÙØÓ¹ Ò Ö Ø ×ÓÙÖ Ð Ò¹ Ù ÓÖÔÓÖ ÓÖ ×ÓÙÖ Ð Ò Ù × ÒÙÑ Ö Ó × Ò¹ Ø Ò ×¸ÒÙÑ Ö Ó ÛÓÖ ×¸ Ò × Þ Ó ÚÓ ÙÐ ÖÝ Ò Ð × À Ú ÝÓÙ Ø Ô Ò ÓÖ ÑÓÖ Ø Ò ÑÓÒØ Ö Ò Ú Þ¹ÚÓÙ× Ñ Ð ÔÙ × ÔÐÙ× ³ÙÒ ÑÓ × Ö À Ð Ø Ù× Ð Ð Ñ ÑÓÙÒ ÓÙ Ø Ö Ñ Ò Ö Û Â Ô Ò × Á Ø×Ù ÓÙ Ø Ñ Û Ø×ÙÞÙ Ñ × Ø Ò Ð × Ï Ò Ó Ø × Ù×Ù ÐÐÝ ÔÔ Ö Ö Ò ÉÙ Ò Ú Þ¹ÚÓÙ× ØÙ ÐÐ Ñ ÒØ ÚÓ× Ñ ÙÜ Ø Ø Ö Å Ø Ø Ù× Ð ×ÓÙ Ø Ò Â Ô Ò × Ø Ø×Ù Ø Ñ Û Ø Ñ Ñ ×Ù Ò Ð × Á× Ø Ô Ò ××Ó Ø Û Ø Ò Ù× Ö Ò Ú Þ¹ÚÓÙ× × Ò Ù×ð × ÕÙ Ò ÚÓÙ× Ú Þ Ð ÓÙÐ ÙÖ Ö À Ð ØÓÙÖ Ò Ø Ø Õ Ý Ò Ñ Ø Ù× Ð Ð Ñ Â Ô Ò × ÁØ ÑÙ ØÓ Û Ó ÓÖ Ñ ×Ù Ò Ð × Ó × Ö Ø Ð Ø Ñ Ø ÛÓÖ× Ö Ò ÎÓ× Ñ ÙÜ Ø Ø ×ÓÒØ Ð× Ö Úð × Ô Ö Ð ÐÙÑ Ö Ö À Ð Ý Ø ÓÙ Ð ×ÓÙ ¢ Ð Û Â Ô Ò × ÖÙ Ö ÛÓ Ñ ÖÙ ØÓ ÞÙØ×Ù Û Ó Ù Ò Ö Ñ ×Ù Ì Ð ¾ Ü ÑÔÐ × Ó Ò Ð × ÓÑ Ò × ÒØ Ò ×¸Û Ø ×Ý×Ø Ñ ØÖ Ò×Ð Ø ÓÒ× ÒØÓ Ö Ò ¸ Ö Ò Â Ô Ò × º ÇÙÖ Ñ ØÖ × Ñ ×ÙÖ Ø ÜØ ÒØ ØÓ Û Ø ¹ Ö Ú Ú Ö× ÓÒ× Ó Ø ËÅÌ Û Ö Ð ØÓ ÔÔÖÓÜ Ñ Ø Ø ÓÖ Ò Ð Ê ÅÌ ÓÒ Ø Û Û × Û Ø Ò Ø Ê ÅÌ³× ÓÚ Ö º Ì ÑÓ×Ø ×ØÖ Ø ÓÖÛ Ö Û Ý ØÓ Ó Ø × × × ÑÔÐÝ ØÓ ÓÙÒØ Ø × ÒØ Ò × Ò Ø Ø ×Ø × Ø Û Ö Ú Ö ÒØ ØÖ Ò×Ð Ø ÓÒ× ÖÓÑ Ø Ê ÅÌ Ò Ø ËÅÌº Ú Ö ÒØ × ØÓ ¢Ò ÒÓÒ¹×Ø Ò Ö Ú Ö× ÓÒ Ó Ø Ä Í Ñ ØÖ ´È Ô Ò Ò Ø Ðº¸¾¼¼½µÛ Ø Ø Ê ÅÌ³× ØÖ Ò×Ð Ø ÓÒ Ø Ò × Ø Ö Ö Ò º Ì × Ñ Ò× Ø Ø Ô Ö Ø ÓÖÖ ×ÔÓÒ Ò ØÛ Ò Ø ØÛÓ ØÖ Ò×Ð Ø ÓÒ× ÛÓÙÐ Ý Ð ÒÓÒ¹×Ø Ò Ö Ä Í ×ÓÖ Ó ½º¼º ÓÖ ÐÐ Ø × Ñ ØÖ ×¸ Ø × ÑÔÓÖØ ÒØ ØÓ Ö Ò Ò ÙÑ Ò Ù × Ø ×ÓÑ ÔÓ ÒØ¸Ù× Ò Ø Ñ ØÓ Ú ÐÙ Ø Ø × × Û Ö Ø ËÅÌ Ò Ê ÅÌ Öº Á ¸ Ò Ø × × ×¸ Ø ØÖ Ò×Ô Ö Ø Ø ÙÑ Ò Ù × ØÝÔ ÐÐÝ Ø ÓÙ Ø Ø Ø Ø ËÅÌ Û × × ÓÓ × Ø Ê ÅÌØ Ò Ø Ñ ØÖ × ÛÓÙÐ ÒÓØ Ù× ÙÐº Ï Ò ØÓ × Ø × Ý ÓÙÖ× ÐÚ × Ø Ø ÙÑ Ò Ù × ØÝÔ ÐÐÝ ×Ö Ö Ò × ØÛ Ò ËÅÌ Ò Ê ÅÌ ØÓ × ÓÖØÓÑ¹ Ò × Ò Ø ËÅÌ Ö Ø Ö Ø Ò Ò Ø Ê ÅÌº ÓÒÖ Ø ÐÝ¸Û ÓÐÐ Ø ÐÐ Ø Ö ÒØ ËÓÙÖ ȨÅÌ¹ØÖ Ò×Ð Ø ÓÒ¸Ê ÅÌ¹ØÖ Ò×Ð Ø ÓÒ ØÖ ÔÐ × ÔÖÓ¹ Ù ÙÖ Ò Ø ÓÙÖ× Ó Ø ÜÔ Ö Ñ ÒØ×¸ Ò Ü¹ ØÖ Ø Ø Ó× ØÖ ÔÐ × Û Ö Ø ØÛÓ ØÖ Ò×Ð Ø ÓÒ× Û Ö Ö ÒØº Ï Ö Ò ÓÑÐÝ × Ð Ø ØÖ ÔÐ × ÓÖ × Ð Ø Ð Ò Ù Ô Ö×¸ Ò × ÙÑ Ò Ù × ØÓ Ð ×× Ý Ø Ñ ÒØÓ ÓÒ Ó Ø ÓÐÐÓÛ Ò Ø ÓÖ × ¯Ê ÅÌ ØØ Ö Ì Ê ÅÌ ØÖ Ò×Ð Ø ÓÒ Û × ØØ Ö¸ Ò Ø ÖÑ× Ó ÔÖ × ÖÚ Ò Ñ Ò Ò Ò »ÓÖ Ò Ö ÑÑ Ø ÐÐÝ ÓÖÖ Ø ¯ËÅÌ ØØ Ö Ì ËÅÌ ØÖ Ò×Ð Ø ÓÒ Û × ØØ Ö¸ Ò Ø ÖÑ× Ó ÔÖ × ÖÚ Ò Ñ Ò Ò Ò »ÓÖ Ò Ö ÑÑ Ø ÐÐÝ ÓÖÖ Ø ¯Ë Ñ Ð Ö ÓØ ØÖ Ò×Ð Ø ÓÒ× Û Ö ÓÙØ ÕÙ ÐÐÝ ÓÓ ÇÊ Ø ×ÓÙÖ × ÒØ Ò Û × Ñ Ò Ò Ð ×× Ò Ø ÓÑ Òº ÁÒ ÓÖ Ö ØÓ × ÓÛ Ø Ø ÓÙÖ Ñ ØÖ × Ö ÒØÙ Ø Ú ÐÝ Ñ Ò Ò ÙÐ¸ Ø × ×Ù ¢ ÒØ ØÓ ÑÓÒ×ØÖ Ø Ø Ø Ø Ö ÕÙ ÒÝ Ó ÓÙÖÖ Ò Ó Ê ÅÌ ØØ Ö × ÓØ Ð Ö Ò ÓÑÔ Ö ×ÓÒ ØÓ Ø Ø Ó ËÅÌ ØØ Ö¸ Ò ¹ ÓÙÒØ× ÓÖ ×Ù ×Ø ÒØ Ð ÔÖÓÔÓÖØ ÓÒ Ó Ø ØÓØ Ð ÔÓÔÙ¹ Ð Ø ÓÒº ÁÒ Ø Ò ÜØ × Ø ÓÒ¸Û ÔÖ × ÒØ Ø Ö ×ÙÐØ× Ó Ø Ú Ö ÓÙ× ÜÔ Ö Ñ ÒØ× Û Ú Ù×Ø ×Ö º Ê ×ÙÐØ× Ì Ð × ¸ Ò ÔÖ × ÒØ Ø Ñ Ò Ö ×ÙÐØ×¸×ÙÑÑ Ö ×¹ Ò Ø ÜØ ÒØ ØÓ Û ËÅÌ Ò Ê ÅÌ ØÖ Ò×Ð Ø ÓÒ× Ö ÓÖ Ø ½¾ Ð Ò Ù ¹Ô Ö×º Ë Ò Ø ØÖ Ò¹ Ò Ò Ø ×Ø Ø Ö Ò Ô Ò ÒØÐÝ × ÑÔÐ ÖÓÑ Ø ×ÓÙÖ Ö ÑÑ Ö¸ Ò Ø ÓÑ Ò × ÕÙ Ø ÓÒ¹ ×ØÖ Ò ¸Ø Ý ÓÚ ÖÐ Ôº Ì × × Ò ØÙÖ Ð¸× Ò ¸ Ò Ø × Ð Ñ Ø ÓÑ Ò¸ Ø × ØÓ ÜÔ Ø Ø Ø ×ÓÑ ØÖ Ò Ò × ÒØ Ò × Û ÐÐ Ð×Ó ÓÙÖ Ò Ø ×Ø Ø × ÕÙ ×Ø ÓÒ× Ð ãÏ Ö × Ø Ô Ò ä Û ÐÐ Ò Ö¹ Ø Û Ø Ö ÕÙ ÒÝ Ý Ø ÔÖÓ Ð ×Ø ×ÓÙÖ Ð Ò Ù ÑÓ Ð¸ Ò Û ÐÐ Ø Ò ØÓ ÓÙÖ Ò ÒÝ ×Ù ¹ ×Ø ÒØ Ð Ò Ô Ò ÒØÐÝ Ò Ö Ø × Ø¸ Ò ÓØ Ò Ø ×Ø Ò ØÖ Ò Ò º Ï Ò ÓÙÒØ Ò Ú Ö ÒØ ØÖ Ò×Ð ¹ Ø ÓÒ× Ò Ê ÅÌ Ò ËÅÌ ÓÙØÔÙØ¸Û ÒÓÒ Ø Ð ×× ÔÖ × ÒØ × Ô Ö Ø ÐÝ Ö ×ÙÐØ× ÓÖ Ø ×Ø Ø Ø Ø Ó × ÒÓØ ÓÚ ÖÐ Ô Û Ø ØÖ Ò Ò Ø ´Ì Ð µ Ò ÓÖ Ø ×Ø Ø Ø Ø Ó × ÓÚ ÖÐ Ô Û Ø ØÖ Ò Ò Ø ´Ì Ð µ¸ÓÒ Ø ÖÓÙÒ × Ø Ø Ø ¢ ÙÖ × Ö ¸ × Ù×Ù Ð¸Ú ÖÝ Ö ÒØ ÓÖ Ø ØÛÓ Ò × Ó Ñ Ø Ö Ðº Ì × ØÛÓ Ø Ð × Ø Ù× ×ÙÑÑ Ö × Ö Ñ ÒØ ØÛ Ò ËÅÌ Ò Ê ÅÌ Ø Ø × ÒØ Ò Ð Ú Ðº Ì Ð × ÓÛ× Ø ÒÓÒ¹×Ø Ò Ö Ä Í ×ÓÖ ×¸Û Ö Ø Ê ÅÌ ØÖ Ò×Ð Ø ÓÒ× Ú Ò Ù× × Ø Ö Ö Ò Ø × Ú Ô ØÙÖ Ó Ö Ñ ÒØ ØÛ Ò Ø ØÛÓ ØÝÔ × Ó ØÖ Ò×Ð Ø ÓÒ Ø Ø Ò¹ Ö Ñ Ð Ú Ðº ÄÓÓ Ò Ò Ô ÖØ ÙÐ Ö Ø Ì Ð ¸Û × Ø Ø Ø ¢ ÙÖ × ÐÐ ÒØÓ Ø Ö ×Ø ÒØ ÖÓÙÔ×º ÓÖ Ð Ò Ù ¹Ô Ö× ÒÚÓÐÚ Ò ÓÒÐÝ Ð Ò Ù × Ò Ø ÖÓÙÔ Ò Ð × ¸ Ö Ò ¸ Ö ¸ËÅÌ Ò Ê ÅÌ Ö ÓÒ ÓÙØ ¼± ØÓ ¼± Ó Ø × ÒØ Ò ×º ÓÖ ØÖ Ò×Ð Ø ÓÒ ÖÓÑ Ò Ð × Ò Ö Ò ØÓ Â Ô Ò × ¸Ø ØÛÓ ØÝÔ × Ó ØÖ Ò×Ð Ø ÓÒ Ö ÓÒ ÓÙØ ¾ ± Ó Ø × ÒØ Ò ×º ÓÖ ØÖ Ò×Ð Ø ÓÒ ÖÓÑ Â Ô Ò × ÒØÓ Ø ÓØ Ö Ø Ö Ð Ò Ù ×¸ Ò ÓÖ Ö ÒØÓ Â Ô Ò × Û ÓÒÐÝ Ø Ö Ñ ÒØ ÓÒ ÓÙØ ½¿± ØÓ ½ ± Ó Ø × ÒØ Ò ×º Ì × Ú × ÓÒ× ÔÔ Ö ØÓ × ÓÛ Ð Ö ÕÙ Ð Ø Ø Ú Ö Ò ×º ËÓÙÖ Ì Ö Ø Ò Ö Ö Â Ô Ò ÜÜÜ º º ¾ º Ö º½ ÜÜÜ ¾º ¾ º Ö º º½ ÜÜÜ ½¿º Â Ô ½ º ½ º ½¾º ÜÜÜ Ì Ð È Ö ÒØ Ó ØÖ Ò×Ð Ø ÓÒ× Û Ö ËÅÌ ØÖ Ò×Ð ¹ Ø ÓÒ Ó Ò × Û Ø Ê ÅÌ ØÖ Ò×Ð Ø ÓÒ¸ÓÚ Ö Ø ×Ø × ÒØ Ò × ÒÓØ ÓÙÖÖ Ò Ò ØÖ Ò Ò Ø º × ×Ù×× Ò Ø ÔÖ Ú ÓÙ× × Ø ÓÒ¸× ÑÔÐÝ ÓÙÒØ Ò Ö Ò × ØÛ Ò ËÅÌ Ò Ê ÅÌ × Ý× ÒÓØ Ò ÓÒ Ø× ÓÛÒ Ø × Ð×Ó Ò ×× ÖÝ ØÓ ×Ø ¹ Ð × Û Ø Ø × Ö Ò × Ñ Ò Ò Ø ÖÑ× Ó Ù¹ Ñ Ò Ù Ñ ÒØ×º Ï Ô Ö ÓÖÑ Ú ÐÙ Ø ÓÒ× Ó Ø × Ò ÓÖ ØÛÓ Ö ÔÖ × ÒØ Ø Ú Ð Ò Ù ¹Ô Ö× Û Ö Û ËÓÙÖ Ì Ö Ø Ò Ö Ö Â Ô Ò ÜÜÜ º ¾º º Ö º ÜÜÜ º º Ö º¾ ¼º ÜÜÜ º¼ Â Ô º½ ½º º ÜÜÜ Ì Ð È Ö ÒØ Ó ØÖ Ò×Ð Ø ÓÒ× Û Ö ËÅÌ ØÖ Ò×Ð ¹ Ø ÓÒ Ó Ò × Û Ø Ê ÅÌ ØÖ Ò×Ð Ø ÓÒ¸ÓÚ Ö Ø ×Ø × ÒØ Ò × ÓÙÖÖ Ò Ò ØÖ Ò Ò Ø º ËÓÙÖ Ì Ö Ø Ò Ö Ö Â Ô Ò ÜÜÜ ¼º ½ ¼º ¾ ¼º Ö ¼º ¿ ÜÜÜ ¼º ¾ ¼º Ö ¼º ¼º ÜÜÜ ¼º Â Ô ¼º ¼ ¼º ¿ ¼º ÜÜÜ Ì Ð AEÓÒ¹×Ø Ò Ö Ä Í ×ÓÖ × ´Ê ÅÌ ØÖ Ò×Ð Ø ÓÒ× Ù× × Ö Ö Ò µ¸ ÐÐ Ø º ÓÙÒ Ø ×Ý ØÓ ÐÓ Ø Ð Ò Ù Ð Ù ×º Ö×Ø¸Ì ¹ Ð × ÓÛ× Ø Ø ÓÖ × Ø ÓÒ¸ ÓÖ Ò ØÓ Ø Ö ¹ Ø Ö ÓÙØÐ Ò Ø Ø Ò Ó Ë Ø ÓÒ ¿¸ ÓÖ ¼¼ Ò¹ Ð × Ö Ò Ô Ö× Ö Ò ÓÑÐÝ × Ð Ø ÖÓÑ Ø × Ø Ó Ü ÑÔÐ × Û Ö Ê ÅÌ Ò ËÅÌ Ú Ö ÒØ Ö ×ÙÐØ× Û × Ø Ö Ù × ØÓ Ú ÐÙ Ø Ø Ñ Ò¹ Ô Ò ÒØÐÝ¸ Ò ÓÑ Ò Ø Ö Ù Ñ ÒØ× Ý Ñ ¹ ÓÖ ØÝ × ÓÒ Û Ö ÔÔÖÓÔÖ Ø º Ï Ó × ÖÚ Ú ÖÝ ÚÝ × ØÓÛ Ö × Ø Ê ÅÌ¸Û Ø ÙÒ Ò ÑÓÙ× Ö Ñ ÒØ Ø Ø Ø Ê ÅÌ ØÖ Ò×Ð Ø ÓÒ Û × ØØ Ö Ò ¾¼½» ¼¼ × ×¸ Ò ¾¹½ Ö Ñ ÒØ Ò ÙÖØ Ö ½¾ º ÁÒ ÓÒØÖ ×Ø¸Ø Ö Û Ö ÓÒÐÝ » ¼¼ × × Û Ö Ø Ù × ÙÒ Ò ÑÓÙ×ÐÝ Ø ÓÙ Ø Ø Ø Ø ËÅÌ ØÖ Ò×Ð ¹ Ø ÓÒ Û × ÔÖ Ö Ð ¸Û Ø ÙÖØ Ö ½¾ ×ÙÔÔÓÖØ Ý Ñ ÓÖ ØÝ × ÓÒº Ì Ö ×Ø Ó Ø Ø Ð Ú × Ø × × Û Ö Ø Ê ÅÌ Ò ËÅÌ ØÖ Ò×Ð Ø ÓÒ× Û Ö Ù Ø × Ñ ÓÖ × × Ò Û Ø Ù × ×¹ Ö Ø Ö Û Ö ÓÒÐÝ ½» ¼¼ × × Û Ö ÒÓ Ñ ¹ ÓÖ ØÝ × ÓÒ Û × Ö º ÇÙÖ ÓÚ Ö ÐÐ ÓÒÐÙ¹ × ÓÒ × Ø Ø Û Ö Ù×Ø ¢ Ò Ú ÐÙ Ø Ò Ø ËÅÌ Ý Ù× Ò Ø Ä Í ×ÓÖ × Û Ø Ø Ê ÅÌ × Ø Ö Ö Ò º Ç Ø × × Û Ö Ø ØÛÓ ×Ý×Ø Ñ× ¹ Ö¸ÓÒÐÝ Ø ÒÝ Ö Ø ÓÒ¸ Ø ÑÓ×Ø ½ » ¼¼¸ Ò Ø ØØ Ö ØÖ Ò×Ð Ø ÓÒ ÖÓÑ Ø ËÅÌ¸ Ò Û ÐÐ ÓÚ Ö Ð Ö ØÖ Ò×Ð Ø ØØ Ö Ý Ø Ê ÅÌº Ì Ð × ÓÛ× ×ÓÑ Ü ÑÔÐ × Ó ËÅÌ ØÖ Ò×Ð Ø ÓÒ× Ò Ø Ò¹ Ð × Ö Ò Ô Ö¸ÓÒØÖ ×Ø Û Ø Ø ØÖ Ò×Ð Ø ÓÒ× ÔÖÓ Ù Ý Ø Ê ÅÌº Ì ¢Ö×Ø ØÛÓ Ö Ö ÑÑ Ø¹ Ð ÖÖÓÖ× ´ ×ÙÔ Ö£ÙÓÙ× ÜØÖ Ú Ö Ò Ø ¢Ö×Ø¸ Ò Ö Ñ ÒØ ÖÖÓÖ× Ò Ø × ÓÒ µº Ì Ø Ö × Ò Ó Ó Ø Ò× Ò ÔÖ ÔÓ× Ø ÓÒ ÐØ ÓÙ Ö ÑÑ Ø ¹ Ð¸Ø Ø Ö Ø Ð Ò Ù × ÒØ Ò Ð× ØÓ ÔÖ × ÖÚ Ø Ñ Ò Ò ¸ Ò ¸Ö Ø Ö Ø Ò Ö ÖÖ Ò ØÓ ¾¼ Ý Ô ¹ Ö Ó Ò Ò ÒÓÛ¸ Ò×Ø Ö Ö× ØÓ ¾¼ Ý Ô Ö Ó ×ÓÑ Ø Ñ Ò Ø Ô ×Øº Ì Ð ½¼ × ÓÛ× × Ñ Ð Ö Ú ÐÙ Ø ÓÒ ÓÖ Ø Ò¹ Ð × Â Ô Ò × º À Ö ¸Ø Ö Ò ØÛ Ò Ø ËÅÌ Ò Ê ÅÌ Ú Ö× ÓÒ× Û × ×Ó ÔÖÓÒÓÙÒ Ø Ø Û ÐØ Ù×Ø ¢ Ò Ø Ò ×Ñ ÐÐ Ö × ÑÔÐ ¸Ó ÓÒÐÝ ½ ¼ × ÒØ Ò ×º Ì × Ø Ñ ¸ ¾»½ ¼ × × Û Ö ÙÒ Ò ¹ ÑÓÙ×ÐÝ Ù × Ú Ò ØØ Ö Ê ÅÌ ØÖ Ò×Ð Ø ÓÒ¸ Ò Ø Ö Û × ÒÓØ × Ò Ð × Û Ö Ú Ò Ñ ¹ ÓÖ ØÝ ÓÙÒ Ø Ø Ø ËÅÌ Û × ØØ Öº Ö Ñ ÒØ Û × ÓÓ Ö ØÓÓ¸Û Ø ÓÒÐÝ »½ ¼ × × ÒÓØ Ý Ð ¹ Ò Ø Ð ×Ø Ñ ÓÖ ØÝ × ÓÒº ÍÒ×ÙÖÔÖ × Ò ÐÝ¸Ø Ñ Ò ÔÖÓ Ð Ñ Û Ø Ø × Ð Ò Ù ¹Ô Ö Û × Ò Ð ØÝ ØÓ Ò Ð ÓÖÖ ØÐÝ Ø Ö Ò × ØÛ Ò Ò Ð × Ò Â Ô Ò × ÛÓÖ ¹ÓÖ Öº Ì Ð Ò × ÓÛ× ×ÓÑ ØÝÔ Ð Ü ÑÔÐ ×º Ì ÖÖÓÖ× Ö ÑÙ ÑÓÖ × Ö ÓÙ× Ø Ò Ò Ö Ò ¸ Ò Ø ËÅÌ ØÖ Ò×Ð Ø ÓÒ× Ö ÓÒÐÝ Ñ Ö Ò ÐÐÝ ÓÑÔÖ Ò× Ð º Ê ×ÙÐØ Ö Ñ ÒØ ÓÙÒØ Ê ÅÌ ØØ Ö ÐÐ Ù × ¾¼½ Ê ÅÌ ØØ Ö Ñ ÓÖ ØÝ ½¾ ËÅÌ ØØ Ö ÐÐ Ù × ËÅÌ ØØ Ö Ñ ÓÖ ØÝ ½¾ Ë Ñ Ð Ö ÐÐ Ù × ¿ Ë Ñ Ð Ö Ñ ÓÖ ØÝ ½ ÍÒÐ Ö × Ö ½ ÌÓØ Ð ¼¼ Ì Ð ÓÑÔ Ö ×ÓÒ Ó Ê ÅÌ Ò ËÅÌ Ô Ö ÓÖÑ Ò ÓÒ ¼¼ Ö Ò ÓÑÐÝ Ó× Ò Ò Ð × Ö Ò ØÖ Ò×Ð Ø ÓÒ Ü ÑÔÐ ×¸ Ú ÐÙ Ø Ò Ô Ò ÒØÐÝ Ý Ø Ö Ù ×º ÙÖ×ÓÖÝ Ü Ñ Ò Ø ÓÒ Ó Ø Ö Ñ Ò Ò Ð Ò Ù ¹ Ô Ö× ×ØÖÓÒ ÐÝ ×Ù ×Ø Ø Ø Ø × Ñ Ô ØØ ÖÒ× Ó ¹ Ø Ò Ø Ö × Û ÐÐ¸Û Ø Ú ÖÝ Û × × Û Ö ËÅÌ Û × ØØ Ö Ø Ò Ê ÅÌ¸ Ò ÒÙÑ ÖÓÙ× × × Ò Ø ÓÔÔÓ× Ø Ö Ø ÓÒº Ë Ò ÓØ Ö Ú ÐÙ Ø ÓÒ× Ó Ø Å ËÄÌ ×Ý×Ø Ñ ´ º º ´Ê ÝÒ Ö Ø Ðº¸¾¼¼ µµ × ÓÛ Ø Ø ÓÚ Ö ± Ó Ò¹ÓÚ Ö ØÖ Ò×Ð Ø ÓÒ× ÔÖÓ Ù Ý Ø Ê ÅÌ ×Ý×Ø Ñ Ö ÔØ Ð Ò Ø ÖÑ× Ó ÔÖ ¹ × ÖÚ Ò Ñ Ò Ò Ò Ò Ö ÑÑ Ø ÐÐÝ ÓÖÖ Ø¸Ó ÙÖ ÓÚ Ö ÐÐ ÓÒÐÙ× ÓÒ × Ø Ø Ö Ò × ØÛ Ò ËÅÌ Ò Ê ÅÌ Ò ÔÐ Ù× ÐÝ ÒØ ÖÔÖ Ø × Ö ¹ £ Ø Ò ÖÖÓÖ× ÔÖÓ Ù Ý Ø ËÅÌº Ê ×ÙÐØ Ö Ñ ÒØ ÓÙÒØ Ê ÅÌ ØØ Ö ÐÐ Ù × ¾ Ê ÅÌ ØØ Ö Ñ ÓÖ ØÝ ¿¾ ËÅÌ ØØ Ö ÐÐ Ù × ¼ ËÅÌ ØØ Ö Ñ ÓÖ ØÝ ¼ Ë Ñ Ð Ö ÐÐ Ù × ¾ Ë Ñ Ð Ö Ñ ÓÖ ØÝ ½ ÍÒÐ Ö × Ö ÌÓØ Ð ½ ¼ Ì Ð ½¼ ÓÑÔ Ö ×ÓÒ Ó Ê ÅÌ Ò ËÅÌ Ô Ö ÓÖÑ Ò ÓÒ ½ ¼ Ö Ò ÓÑÐÝ Ó× Ò Ò Ð × Â Ô Ò × ØÖ Ò×Ð Ø ÓÒ Ü ÑÔÐ ×¸ Ú ÐÙ Ø Ò Ô Ò ÒØÐÝ Ý Ø Ö Ù ×º ËÙÑÑ ÖÝ Ò ÓÒÐÙ× ÓÒ× Ï Ú ÔÖ × ÒØ Ò ÜÔ Ö Ñ ÒØ Ò Û Û Ò¹ Ö Ø ÙÒ ÓÖÑ ÖØ ¢ Ð Ø ÓÖ ½¾ Ð Ò Ù Ô Ö× Ò ÑÙÐØ Ð Ò Ù Ð ×Ñ ÐÐ¹ÚÓ ÙÐ ÖÝ ÒØ ÖÐ Ò Ù ¹ × ØÖ Ò×Ð Ø ÓÒ ×Ý×Ø Ñº Í× Ó Ø ÒØ ÖÐ Ò Ù Ò ÓÖ ÙÒ ÓÖÑ ØÖ Ò×Ð Ø ÓÒ ×Ø Ò Ö ¸×Ó Û Ð Ù×Ø ¢ Ò Ð Ñ Ò Ø Ø Ø Ö ×ÙÐØ× ÔÖÓÚ Ö Ö Ú Ò Ø Ò Ù×Ù Ð ÓÙØ Ø Ö Ð Ø Ú ×Ù Ø Ð ØÝ Ó Ö ÒØ Ð Ò Ù ¹Ô Ö× ÓÖ ËÅÌº × ÜÔ Ø ¸ØÖ Ò×Ð Ø ÓÒ ØÛ Ò Ò Ð × Ò Ö Ò Ò ÓØ Ö Ø ÓÒ× × ÑÙ ÑÓÖ Ö Ð Ð Ø Ò ØÖ Ò×Ð Ø ÓÒ Ò Ð Ò Ù Ô Ö× ÒÚÓÐÚ Ò Â Ô Ò × º ÌÓ ÓÙÖ ×ÙÖÔÖ × ¸Û Ð×Ó ÓÙÒ Ø Ø ØÖ Ò×Ð Ø ÓÒ ØÛ Ò Ò Ð × ÓÖ Ö Ò Ò Ö ÛÓÖ ÓÙØ × Û ÐÐ × ØÖ Ò×Ð Ø ÓÒ ØÛ Ò Ò Ð × Ò Ö Ò ¸ ×Ô Ø Ø Ø Ø Ø Ö ØÝÔÓÐÓ ÐÐÝ ÐÓÒ × ØÓ ¹ Ö ÒØ Ð Ò Ù Ñ ÐÝº ÁÒ ÓÖÑ Ð ÓÒÚ Ö× Ø ÓÒ× Û Ø ÓÐÐ Ù × Û Ó Ú ÛÓÖ ÓÒ Ö ×Ù ×Ø¸ ÓÛ Ú Ö¸Ø Ø Ø Ö ×ÙÐØ × ÒÓØ × ÙÒ ÜÔ Ø × Û ¢Ö×Ø Ñ Ò º Ì Ð ÔÔ Ö× ØÓ ×Ù ×Ø Ø Ø ØÖ Ò×Ð Ø ÓÒ ÖÓÑ Â Ô Ò × ÛÓÖ × ×Ù ×Ø ÒØ ÐÐÝ Ð ×× Û ÐÐ Ø Ò ØÖ Ò×Ð ¹ Ø ÓÒ ØÓ Â Ô Ò × º Ì ÜÔÐ Ò Ø ÓÒ × ÑÓ×Ø ÔÖÓ ÐÝ Ø Ù×Ù Ð ÔÖÓ Ð Ñ Ó Þ ÖÓ¹ Ò Ô ÓÖ ¸Û × Ú ÖÝ ÓÑÑÓÒ Ò Â Ô Ò × ¸Û Ø ÛÓÖ × Ø Ø Ò Ð ÖÐÝ Ò ÖÖ ÖÓÑ ÓÒØ ÜØ Ò Ö ÐÐÝ Ð Ø º ÁÒ Ø ÖÓÑ¹Â Ô Ò × Ö Ø ÓÒ¸ Ø × Ò ×× ÖÝ ØÓ Ò Ö Ø ØÖ Ò×Ð Ø ÓÒ Ó Þ ÖÓ Ò Ô ÓÖ ´ÑÓ×Ø Ó Ø Ò Ø ÑÔÐ Ø × ÓÒ ¹Ô Ö×ÓÒ ÔÖÓÒÓÙÒµ¸Û Ð Ò Ø ØÓ¹Â Ô Ò × Ò Ð × Ó × Ø ÑÔ Ö ØÙÖ Ò Ù× Ø Ê ÅÌ Ö Ò ÚÓ× Ñ ÙÜ Ø Ø ×ÓÒØ¹ Ð× Ù×ð × Ô Ö × Ò Ñ ÒØ× Ø ÑÔð Ö ØÙÖ ´ÝÓÙÖ × Ö ¹Ø Ý Ù× Ý Ò × Ó Ø ÑÔ Ö ØÙÖ µ ËÅÌ Ö Ò Ú Þ¹ÚÓÙ× ÚÓ× Ñ ÙÜ Ø Ø ×ÓÒØ¹ Ð× Ù×ð × Ô Ö × Ò Ñ ÒØ× Ø ÑÔð Ö ØÙÖ ´ Ú ¹ÝÓÙ ÝÓÙÖ × Ö ¹Ø Ý Ù× Ý Ò × Ó Ø ÑÔ Ö ØÙÖ µ Ò Ð × Ö × Ö Ð Ú Ò Ø Ø ÖÒÓÓÒ Ê ÅÌ Ö Ò ÚÓ× Ñ ÙÜ Ø Ø Ñ ÒÙ ÒØ¹ Ð× Ð³ ÔÖ ×¹Ñ ´ÝÓÙÖ × ´Å Ë ¹ÈÄÍÊµ Ö × ¹Å Ë ¹ÈÄÍÊ Ø Ø ÖÒÓÓÒµ ËÅÌ Ö Ò ÚÓ× Ñ ÙÜ Ø Ø Ñ ÒÙ ¹Ø¹ ÐÐ Ð³ ÔÖ ×¹Ñ ´ÝÓÙÖ × ´Å Ë ¹ÈÄÍÊµ Ö × ¹ Å¹ËÁAE Ø Ø ÖÒÓÓÒµ Ò Ð × Ú ÝÓÙ Ø Ñ ÓÖ ØÛ ÒØÝ Ý× Ê ÅÌ Ö Ò Ú Þ¹ÚÓÙ× ÚÓ× Ñ ÙÜ Ø Ø ÔÙ × Ú Ò Ø ÓÙÖ× ´ Ú ¹ÝÓÙ ÝÓÙÖ × × Ò ØÛ ÒØÝ Ý×µ ËÅÌ Ö Ò Ú Þ¹ÚÓÙ× Ù ÚÓ× Ñ ÙÜ Ø Ø Ô Ò ÒØ Ú Ò Ø ÓÙÖ× ´ Ú ¹ÝÓÙ ÝÓÙÖ × ÙÖ Ò ØÛ ÒØÝ Ý×µ Ì Ð Ü ÑÔÐ × Ó ÒÓÖÖ Ø ËÅÌ ØÖ Ò×Ð Ø ÓÒ× ÖÓÑ Ò Ð × ÒØÓ Ö Ò º ÖÖÓÖ× Ö Ð Ø Ò ÓÐ º Ö Ø ÓÒ Ø × ÓÒÐÝ ÕÙ ×Ø ÓÒ Ó Ð Ø Ò Ñ Ø ¹ Ö Ðº ÐØ ÓÙ ¸ × ÔÓ ÒØ ÓÙØ ÖÐ Ö Ò Ë Ø ÓÒ ¿Û Ò ØÓ Ö ÙÐ Û Ò ÓÑÔ Ö Ò ØÛ Ò Ð Ò Ù ¹Ô Ö× Û Ø Ö ÒØ ×ÓÙÖ ¹Ð Ò Ù ¸Ø Ô Ò Ô Ö ÓÖÑ Ò Ö × Ð Ö ÒÓÙ Ø Ø Û Ò ÜÔ Ø Ø ØÓ Ö £ Ø Ö Ð ØÖ Ò º Ë ÑÔÐ × Ø ×¸Û ÓÔ Ø Ø Ø Ñ Ø Ó ¹ ÓÐÓ Ý ×Ö Ò Ø × Ô Ô Ö Û ÐÐ Ñ Ø ÔÓ×× Ð ØÓ Ú ÐÙ Ø Ø Ö Ð Ø Ú ×Ù Ø Ð ØÝ Ó ËÅÌ ÓÖ Ö¹ ÒØ Ð Ò Ù Ô Ö× Ò ÑÓÖ ÕÙ ÒØ Ø Ø Ú Û Ý Ø Ò × ×Ó Ö Ò ÔÓ×× Ð º ÁÒ Ò Ö Ð¸Ø ÓÒ×ØÖÙ¹ Ø ÓÒ Ù× ÓÙÐ ÕÙ ÐÐÝ Û ÐÐ ÑÔÐ Ñ ÒØ Ò Ø ÓÒØ ÜØ Ó ÒÝ ÓØ Ö ¹Ô Ö ÓÖÑ Ò ÑÙÐØ Ð Ò Ù Ð Ê ÅÌ ×Ý×Ø Ñº Ì Ó ×Ø Ø ×Ø ÐÐÝ ãÖ Ð ÖÒ¹ Ò ä Ò Ê ÅÌ ×Ý×Ø Ñ × Ö ÒØÐÝ ÙÒ ØÓ ÕÙ Ö ×ÓÑ ÔÓÔÙÐ Ö ØÝ ´Ë Ò Ø Ðº¸¾¼¼ Ù ×Ø Ø Ðº3 ¼¼ µ¸ Ò Ø × ÓÙÐ ×Ý ØÓ Û Ø Ö ÓÙÖ Ö ×ÙÐØ× Ö Ò Ö ÐÐÝ Ö ÔÖÓ Ù Ð º Ê Ö Ò × º Ö ¸Åº Ç× ÓÖÒ ¸ Ò Èº ÃÓ Òº ¾¼¼ º ÈÖ Ø¹ Ò ×Ù ×× Ò Ñ Ò ØÖ Ò×Ð Ø ÓÒº ÁÒ ÈÖÓ Ò × Ó ÅAEÄÈ ¾¼¼ ¸Ï ¸À Û º Èº ÓÙ ÐÐÓÒ¸ º ÐÓÖ ×¸Åº ÓÖ ×ÙÐ¸Ëº À Ð Ñ ¸ º º ÀÓ Ý¸Àº Á× Ö ¸Ãº Ã ÒÞ ¸ º AE Ó¸Åº Ê ÝÒ ÖÅ º Ë ÒØ ÓÐÑ ¸Åº ËØ ÖÐ Ò Ö¸ Ò AEº Ì×ÓÙÖ ×º ¾¼¼ º Å ÒÝ¹ØÓ¹Ñ ÒÝ ÑÙÐØ Ð Ò Ù Ð Ñ Ð ×Ô ØÖ Ò×Ð Ø ÓÒ ÓÒ È º ÁÒ ÈÖÓ Ò × Ó Ì Ø ÓÒ Ö Ò Ó Ø ××Ó Ø ÓÒ ÓÖ Å Ò ÌÖ Ò×Ð Ø ÓÒ Ò Ø Ñ Ö ×¸Ï ¸À Û º Èº ÖÓÛÒ¸Âº Ó ¸Ëº ÐÐ È ØÖ ¸Îº ÐÐ È ØÖ ¸ º Â ¹ Ð Ò ¸Âº Ä ÖØÝ¸Êº Å Ö Ö¸ Ò Èº ÊÓÓ×× Òº ½ ¼º ×Ø Ø ×Ø Ð ÔÔÖÓ ØÓ Ñ Ò ØÖ Ò×Ð Ø ÓÒº ÓÑÔÙØ ¹ Ø ÓÒ Ð Ä Ò Ù ×Ø ×¸½ ´¾µ ae º AEº ØÞ Ö × ¢×¸Èº ÓÙ ÐÐÓÒ¸Åº Ê ÝÒ Ö¸Åº Ë ÒØ ¹ ÓÐÑ ¸Åº ËØ ÖÐ Ò Ö¸ Ò º º ÀÓ Ýº ¾¼¼ º Ú ÐÙ¹ Ø Ò Ø × Ô Ö ÓÖÑ Ò ÓÖ ÙÒ Ö Ø ÓÒ Ð ÓÒØÖÓÐÐ Ð Ò Ù Ñ Ð ×Ô ØÖ Ò×Ð Ø ÓÒ ×Ý×Ø Ñº ÁÒ ÈÖÓ¹ Ò × Ó Ø ÀÄÌ¹AE Ä ÁÒØ ÖÒ Ø ÓÒ Ð ÏÓÖ × ÓÔ ÓÒ Å Ð ËÔ ÌÖ Ò×Ð Ø ÓÒ¸Ô × ae½ ¸AE Û ÓÖ º Äº Ù ×Ø¸Âº Ë Ò ÐÐ ÖØ¸ Ò Èº ÃÓ Òº ¾¼¼ º Ò Û Ö ¹ Ð ÖÒ Ò Ê ÅÌ ×Ý×Ø Ñ ÁÒ ÈÖÓ Ò × Ó Ø Ì Ö ÏÓÖ × ÓÔ ÓÒ ËØ Ø ×Ø Ð Å Ò ÌÖ Ò×Ð Ø ÓÒ¸Ô × ½ ae½ ¸ ÓÐÙÑ Ù×¸Ç Óº º º ÀÓ Ý¸Åº Ê ÝÒ Ö¸ Ò º Ö ×Ø Òº ¾¼¼ º ÌÖ Ò Ò ×Ø Ø ×Ø Ð Ð Ò Ù ÑÓ Ð× ÖÓÑ Ö ÑÑ Ö¹ Ò Ö Ø Ø ÓÑÔ Ö Ø Ú × ¹×ØÙ Ýº ÁÒ ÈÖÓ¹ Ò × Ó Ø Ø ÁÒØ ÖÒ Ø ÓÒ Ð ÓÒ Ö Ò ÓÒ AE ØÙ¹ Ö Ð Ä Ò Ù ÈÖÓ ×× Ò ¸ ÓØ Ò ÙÖ ¸ËÛ Òº Êº ÂÓÒ×ÓÒº ¾¼¼ º Ò Ö Ø Ò ×Ø Ø ×Ø Ð Ð Ò Ù ÑÓ Ð× ÖÓÑ ÒØ ÖÔÖ Ø Ø ÓÒ Ö ÑÑ Ö× Ò ÐÓ Ù ×Ý×Ø Ñ×º ÁÒ ÈÖÓ Ò × Ó Ø ½½Ø Ä¸ÌÖ ÒØÓ¸ÁØ ÐÝº Ò Ð × Ú ÝÓÙ Ø ØØ × Ò Ø Ú Ò Ò Ê ÅÌ Â Ô Ò × Ò Ö Ñ × Ø Ú Ò Ò ÈÇÄÁÌ ¹È ËÌ É ËÅÌ Â Ô Ò × Ö Ñ × Ø Ò ÈÇÄÁÌ ¹È ËÌ É Ú Ò Ò Ò Ð × Ó × ÒÓ × ØÝÔ ÐÐÝ Ú ÝÓÙ ÝÓÙÖ × Ê ÅÌ Â Ô Ò × ×ÓÙÓÒ Ò Ò ÖÙ ØÓ Ø Ø Ñ Û Ø Ñ Ñ ×Ù ÒÓ × ËÍ Â ÜÔ Ö Ò ÏÀ AE Ó Ø Ò ÌÇÈ ÙÖØ ÈÇÄÁÌ É ËÅÌ Â Ô Ò × ×ÓÙÓÒ Ò Ò ÖÙ ØÓ Ø Ñ Ñ ×Ù Ø Ø Ñ Û Ø Ñ Ñ ×Ù ÒÓ × ËÍ Â ÜÔ Ö Ò ÏÀ AE ÙÖØ ÈÇÄÁÌ É Ó Ø Ò ÌÇÈ ÙÖØ ÈÇÄÁÌ É Ò Ð × × Ø Ô Ò Ù×Ù ÐÐÝ Ù× Ý ×Ù Ò ÑÓÚ Ñ ÒØ× Ê ÅÌ Â Ô Ò × Ø ØÓØ×ÙÞ Ò Ø Ñ ÛÓ Ù Ó ×Ù ØÓ Ø Ñ Ñ ×Ù Ù×Ù ÐÐÝ ÕÙ ÐÝ Ç Â ÑÓÚ ÏÀ AE ÙÖØ ÈÇÄÁÌ É ËÅÌ Â Ô Ò × Ø Ø Ñ Û ØÓØ×ÙÞ Ò Ø Ñ ÛÓ Ù Ó ×Ù ØÓ Å ×× Ò Ø Ñ Ñ ×Ù ℄ Ù×Ù ÐÐÝ Ô Ò ËÍ Â ÕÙ ÐÝ Ç Â ÑÓÚ ÏÀ AE Å ×× Ò ÙÖØ ÈÇÄÁÌ É℄ Ì Ð Ü ÑÔÐ × Ó ÒÓÖÖ Ø ËÅÌ ØÖ Ò×Ð Ø ÓÒ× ÖÓÑ Ò Ð × ÒØÓ Â Ô Ò × º ÖÖÓÖ× Ö Ð Ø Ò ÓÐ º º ÂÙÖ × Ý¸ º ÏÓÓØ Ö×¸Âº Ë Ð¸ º ËØÓÐ ¸ º Ó×¹ Ð Ö¸ º Ì Ñ Ò¸ Ò AEº ÅÓÖ Òº ½ º Í× Ò ×ØÓ ×Ø ÓÒØ ÜØ¹ Ö Ö ÑÑ Ö × Ð Ò Ù ÑÓ Ð ÓÖ ×Ô Ö Ó Ò Ø ÓÒº ÁÒ ÈÖÓ Ò × Ó Ø Á ÁÒ¹ Ø ÖÒ Ø ÓÒ Ð ÓÒ Ö Ò ÓÒ ÓÙ×Ø ×¸ËÔ Ò Ë ¹ Ò Ð,59436176,b196a29bfe73a14cb5df655e160f3d6f60479682,3,https://aclanthology.org/2009.mtsummit-posters.16,,"Ottawa, Canada",2009,August 26-30,Proceedings of Machine Translation Summit XII: Posters,"Rayner, Manny  and
Estrella, Paula  and
Bouillon, Pierrette  and
Nakao, Yukie",Using Artificial Data to Compare the Difficulty of Using Statistical Machine Translation in Different Language-Pairs,,,,,,,,inproceedings,rayner-etal-2009-using-artificial,,,
44,R13-1056,"We report on our efforts aimed at building an Open Domain Question Answering system for Polish. Our contribution is twofold: we gathered a set of question-answer pairs from various Polish sources and we performed an empirical evaluation of two re-ranking methods. The gathered collection contains factoid, list, non-factoid and yes-no questions, which makes a challenging material for experiments. We show that using two re-ranking methods based on term proximity allows to obtain significant improvement on simple information retrieval baseline. The improvement is observed as finding more answer-bearing documents among the top n search results.","We report on our efforts aimed at building an Open Domain Question Answering system for Polish. Our contribution is twofold: we gathered a set of question-answer pairs from various Polish sources and we performed an empirical evaluation of two re-ranking methods. The gathered collection contains factoid, list, non-factoid and yes-no questions, which makes a challenging material for experiments. We show that using two re-ranking methods based on term proximity allows to obtain significant improvement on simple information retrieval baseline. The improvement is observed as finding more answer-bearing documents among the top n search results. Background Question Answering (QA) is an information retrieval task in which the user information need is expressed in terms of a natural language question. As this way of expressing information needs is very flexible, QA systems are mostly constructed as Open Domain QA systems (ODQA), not limited to any particular text collection or narrow domain (Pas ¸ca, 2003 ). An ODQA system can deliver an answer, as a text of a data record, but mostly it is required that it returns passages extracted from a collection of documents that are supposed to include an answer to the user's question. The goal of ODQA is to answer questions not restricted to any pre-defined domain (Pas ¸ca, 2003) . Most ODQA systems process user questions in four steps, cf (Pas ¸ca, 2003; Monz, 2003; Ferrucci, 2012) : question analysis, document retrieval, document analysis and answer selection. In addition to this general scheme, we can distinguish several typical substeps or tasks: question classification (Lally et al., 2012) , query selection and ex-pansion (Pas ¸ca, 2003) , passage retrieval and ranking (Pas ¸ca, 2003 ), candidate answer identification (Chu-Carroll et al., 2012) , answer extraction and ranking (Gondek et al., 2012) , etc., but the core is shared among systems. There are only a few known works on ODQA (working on text collections) for Polish, e.g. Walas and Jassem (2011) , Walas (2012) , and two systems publicly available: Hipisek.pl and KtoCo. pl. The latter is a commercial system and little is known about its structure. Hipisek implements the ODQA blueprint described above (Walas and Jassem, 2011) , but was focused on processing yes-no questions about time and location. The system depends on a dedicated rulebased parser (Walas and Jassem, 2011) , and was extended with a knowledge base for spatial relations (Walas, 2012) . Our long-term goal is large-scale, broadapplication ODQA with respect to different types of questions and documents indexed. We utilize the following architecture of QA system: • query analysis -query processing by language tools and rules, and generation of the search query, • search engine -fetches the N most relevant documents from a large collection of documents, • module for document ranking -the set of documents returned by the search engine is re-ranked using medium time-consuming techniques. The M top documents are selected, where M N , • module for extracting candidate answers -the most time-consuming operations are performed on the reduced set of documents, • module for answer ranking -the list of candidate answers with their context are ranked and best items are presented to user. In this paper we focus on the first three elements of QA system with the special focus on search engine and document re-ranking. QA dataset for Polish Most works performed for English rely upon the TREC datasets (Voorhees, 2001) . No such dataset was available for Polish, so we started construction of a set of question-answer pairs for Polish. We surveyed several possible ways of collecting questions and answers for Polish from the available resources. Our first idea was to crawl Internet QA communities such as zapytaj.onet.pl, pytki.pl, pytano.pl (Polish counterparts of ask.com) and grab both questions and answers. We anticipated the need for substantial manual work needed to select and curate the data, but the actual scale of the problem was quite overwhelming: while it is already not easy to find suitable questions there, finding a number of suitable answers in reasonable time was practically infeasible. The main problem was that if the answers would serve as a testing material for a system based on document retrieval, the answers should mimic normal documents. The answers posted by users of such sites are usually very short and devoid of the necessary context to understand them -they make sense only when paired with the corresponding questions (those that make sense at all). The same problem turned out to apply for FAQ sites, even official ones. This way we faced the necessity to divide the process into two separate phases: gathering questions and then finding documents that provide answers to them. Gathering questions We developed simple guidelines that help to recognise acceptable questions. A question must have syntactic structure of a question (rather than a string of query terms), be simple (one question per a sentence), not requiring any additional context for its interpretation. We did not accept questions referring to the person asked ('What bands do you like?'). Questions about opinions were discouraged unless could be conceived as addressed to a domain expert (e.g. 'Which wines go well with fish?'). We did not exclude questions which were vulgar or asked just for fun as long as they satisfied all other requirements. We considered four sources of candidate questions which we hope to reflect the actual information need of the Internet users. First, thanks to the courtesy of Marcin Walas we were given access to user query logs of Hipisek.pl. The second source was 'manual crawling' around the QA communities. Similarly, we considered FAQ sites of several Inland Revenue offices. Lastly, we decided to abuse the auto-complete feature of Google and Bing search engines to gain insight into the questions that have actually been posed (it turns out that a number of users indeed ask natural language questions as search engine queries). The task was to enter word/words that typical questions start with and copy the suggestions. This could have led to some bias concerning the selection of the question-initial words. On the other hand, the mechanism seems to work surprisingly well and it is sufficient to give two words to obtain a lot of sensible questions. All of the questions that were decided as appropriate were subjected to orthographical and grammatical correction. We considered manual translation of the TREC questions, as was done, e.g., in (Lombarović et al., 2011) . We decided against this solution, since the TREC questions seem too much oriented on the American culture and geography for our purposes. Also the TREC datasets cotains mainly factoid questions while we wanted to create a balanced dataset containing both factoid and nonfactoid questions. Finding answers We required from the answer documents to have included at least one passage (a couple of consecutive sentences) that contained the answer, i.e. that there was no necessity to construct an answer from information scattered across the document. This is because we assume the final version of the system will present such a passage to the user. We also required the answer-bearing passage to be comprehensive even when not paired with the question, e.g. if the question was about Linux, this name or its equivalent should appear in the passage rather than only general terms such as 'the system'. The set of candidate questions was given to linguists, which were asked to devote a couple of minutes per each question and try to find a satisfactory answer using search engines. They were asked to avoid typing the whole question as a query to prevent from favouring those documents that contain questions. For each candidate at most one answer document was found. Each answer document (a website) was downloaded as HTML files. We used the Web As Corpus Toolkit (Ziai and Ott, 2005) to clean up the files and remove boilerplate elements. The final output contained no mark-up and its structure was limited to plain text divided into paragraphs. Note that only those questions that the linguists were able to find an answer for have made their way to the final dataset. Final collection Ultimately, the set of questions paired with answers contains 598 entries. The statistics regarding source distribution is given as Table 1 . The collection contains the following types of questions (according to the expected answer type): Source 1. Factoid and list questions (Voorhees, 2004; Voorhees and Dang, 2005) (Mizuno et al., 2007; Fukumoto, 2007) : • definition -What is X?, What does X mean?, Who is X?, • description -What powers does the president have?, • mannerhow questions; How to start a business?, How to make a frappe coffe?, • reasonwhy questions; Why do cats purr?, How do I catch a cold?. 3. Yes-no questions (Walas, 2012; Kanayama et al., 2012) ; Is Lisbon in Europe?. To perform a reliable evaluation of the system, we had to index a lot more data than just the answers to our 598 test questions. We acquired also several collections to serve as 'distractors' and a source of possible answers, namely: • Polish Wikipedia (using dump from 22 January 2013) -956 000 documents. • A collection of press articles from Rzeczpospolita (Weiss, 2008) -180 000 documents. • Three smaller corpora: KPWr (Broda et al., 2012) , CSEN and CSER (Marcińczuk and Piasecki, 2011) -3 000 documents. Evaluation metrics The evaluation was based on the following metrics: • answers at n-th cutoff (a@n) (Monz, 2003) relevant documents recall; a fraction of questions for which the relevant document was present in the first n documents returned by the search engine; • mean reciprocal rank (MRR) -an average of the query reciprocal ranks 1 MRR is used to compare re-ranking algorithms. The higher MRR is, the higher in the ranking the relevant documents are. Baseline information retrieval As a basis for search engine we selected an open source search platform called Solr (The Apache Software Foundation, 2013a). Solr indexes large collection of documents and provides: full-text search, rich query syntax, document ranking, custom document fields and terms weighting. It was also shown that Lucene (the retrieval system underlying Solr) performs no worse for QA than other modern Information Retrieval systems (Tellex et al., 2003) . In the baseline approach we used an existing tool called Web as Corpus ToolKit (Adam Kilgarriff and Ramon Ziai and Niels Ott, 2013) to extracted plain text from the collection of HTML documents. Then, the text was tagged using WCRFT tagger (Radziszewski, 2013) and their base forms were indexed in the Solr. To fetch a ranked list of documents for a query we used a default search ranking algorithm implemented in the Lucene that is a combination of Boolean Model (BM) with refined Vector Space Model (VSM). BM is used to fetch all documents matching the boolean query. Then, VSM is applied to rank the answer documents. The detailed formula used to compute the ranking score is presented in (The Apache Software Foundation, 2013b). The formula includes following factors: • fraction of query terms present in the document -documents containing more query terms are scored higher than those with fewer, • query normalizing factor -to make the score comparable between queries, • document term frequency -documents containing more occurrences of query terms receive higher scores, • inverse document frequency -common terms (present in many documents) have lower impact on the score, • term boosting factor -weight specified in the query can be used to increase importance of selected terms (not used in our approach), • field boosting factor -some fields might be more important than others (not used by us), • field length normalization factor -shorter fields obtain higher scores. Figure 1 presents all steps of question analysis. First, a question is tagged with WCRFT tagger. All punctuation marks and words from a stoplist (including 145 prepositions) are discarded. We assumed that in most cases the answer have a form of a statement and does not mimic question structure. The remaining words are used in a query, formed as a boolean disjunction of the base forms. The a@n and MRR values for the baseline configuration are presented in Table 3 . We measured the a@n for several distinct values of n between 1 and 200 (this is an estimated maximum number of documents which can be effectively processed during re-ranking). The a@n ranges from 26% for n = 1 to 87% for n = 200. This means than only for 26% questions the relevant document was on the first position in the ranking. In the reported tests all non-stop words from the question were used to form a query. We tested also several modification of the heuristic for query term selection proposed in (Pas ¸ca, 2003) , but the results were lower. Proximity-based re-ranking Lucene default ranking algorithm does not take into consideration proximity of query terms in the documents. This leads to favouring longer documents as they are more likely to contain more query terms. However such documents can describe several different topics not related to the question. Ranking of longer documents cannot be decreased by default, as they might contain an answer. A possible solution is to analyse query term 1. Input: Co można odliczyć od podatku? (""What can be deducted from tax?"") 2. Tagging: co można odliczyć od podatek ? (base forms) 3. Filtering: można odliczyć od podatek (""can"", ""deduct"", ""tax"") 4. Query: base:mo_ zna OR base:odliczyć OR case:od OR base:podatek a@n n baseline 1 26.09% 5 52.17% 10 62.04% 20 70.57% 50 76.76% 100 82.61% 200 87.29% MRR 0.3860 Table 3 : a@n and MRR for baseline configuration of information retrieval. proximity inside the documents. We have evaluated two approaches to utilising term proximity in re-ranking. Maximum Cosine Similarity Weighting Maximum Cosine Similarity Weighting (MCSW) is based on the idea of using the same ranking scheme as in the retrieval component, but applied to short passages, not whole documents. Every document is divided into continuous blocks of k sentences. For every block we compute the cosine similarity between a vector representing the block and a vector representing a query. Standard tf-idf weighting (Manning et al., 2008) and cosine measure are used. A document is assigned the maximum per-block cosine similarity that was encountered. Several block sizes (k from 1 to 5) were tested producing very similar results, thus we report results only for k = 1. The final document score is computed as follows: where: • D, ordered list od documents returned from search engine for a query, • score(d), score for document d returned by Solr, • mcs(d), maximum cosine similarity for document d. Minimal Span Weighting Monz ( 2003 ) presented a simple method for weighting based on a minimal text span containing all terms from a query that occur in the document. The re-ranking score combines the original score with MSW score and is computed as follows: score (d) = score(d) * λ + (1 − λ) • |q ∩ d| |s| α • |q ∩ d| |q| β (2) where: • q, set of query terms, • s, the shortest text fragment containing all query terms occurring in the document, • λ, α, β, significance weights for the respective factors (we used default values (Monz, 2003) , i.e. λ = 0.4, α = 0.125, β = 1) Evaluation The a@n and MRR values for MCSW and MSW are presented in Table 4 . For both methods we noticed a small improvement. The increase of MRR values for both methods indicates that the average position of the relevant documents in the ranking was improved. The a@n was improved by up to 12 percentage points for MCSW and n = 1. The lower improvement for MSW might be caused by the assumption that the minimal span must contain all query terms occurring in the document. In addition, the proximity-based ranking algorithms can be used to extract the most relevant document fragments as answers instead of presenting the whole document. According to (Lin et al., 2003) , users prefer paragraph-level chunks of text with appropriate answer highlighting. Despite the observed improvements, the results are still below our expectations. If we assume that user reads up to 10 answers for a question (a typical number of results displayed on a single page in many web search engines), the top a@n will be about 70%. This means that we will not provide any relevant answer for 3 out of 10 questions. According to (Monz, 2003) , results for English reported for TREC sets are between 73% and 86% for a@10. Thus, further improvement in reranking is necessary. Conclusion We presented a preliminary results for a baseline information retrieval system and the simple proximity-based re-ranking methods in the context of a Open Domain Question Answering task for Polish. The evaluation was performed on a corpus of 598 questions and answers, collected from a wide range of questions asked by Internet users (i.e. search engines, Hipisek.pl, QA communities and Revenue FAQ). The collection covers major types of questions including: factoid, list, non-factoid and yes-no questions. The a@n of the baseline IR system (Solr) configuration ranges from 26% for n = 1 to 87% for n = 200 top documents considered. Our queries consisted of base forms of all question words except words from a stoplist. Several heuristics for query term selection inspired by the one proposed in (Monz, 2003) produced lower results. This can be explained by the properties of the ranking algorithm used in Solr -the number of terms covered and their total frequency in a document are important factors. For n = 10 (a typical single page in a Web search) we obtained 62% a@n. Two re-ranking methods based on query term proximity were applied. For both methods we obtained a noticeable improvement up to 12 percentage points of a@n for n = 1 and 9 percentage points for n = 10. Nevertheless, the results are still slightly lower than in the case of systems built for English, e.g., (Monz, 2003) . However, results reported by Monz were obtained on the TREC datasets, which contain mostly factoid and list questions. Our datasets includes also nonfactoid and yes-no questions which are more difficult to deal with. The comparison with Hipisek is difficult as no results concerning ranking precision were not reported. Moreover, Hipisek was focused on selected subclasses of questions. We plan to extend the information retrieval model on the level of document fetching and reranking. We want to utilize plWordNet 2.0 (the Polish wordnet) 2 (Maziarz et al., 2012) , tools for proper names (Marcińczuk et al., 2013) and semantic relations recognition (Marcińczuk and Ptak, 2012) , dependency 3 and shallow syntactic parsers. More advanced but also more timeconsuming tools will be used to select relevant passages in the documents fetched by the presented information retrieval module. Acknowledgements The work was funded by the European Union Innovative Economy Programme project NEKST, No POIG.01.01.02-14-013/09. We would like to thank Marcin Walas for sharing the collection of historic questions that have been posed to the Hipisek system.",7009053,b0069132b52e2e07b000a3959671aebd3256296f,3,https://aclanthology.org/R13-1056,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Marci{\'n}czuk, Micha{\l}  and
Radziszewski, Adam  and
Piasecki, Maciej  and
Piasecki, Dominik  and
Ptak, Marcin",Evaluation of baseline information retrieval for {P}olish open-domain Question Answering system,428--435,,,,,,,inproceedings,marcinczuk-etal-2013-evaluation,,,Information Retrieval and Text Mining
45,W05-0829,This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach.,"This article describes the competitive grouping algorithm at the core of our Integrated Segmentation and Alignment (ISA) model. ISA extracts phrase pairs from a bilingual corpus without requiring the precalculated word alignment as many other phrase alignment models do. Experiments conducted within the WPT-05 shared task on statistical machine translation demonstrate the simplicity and effectiveness of this approach. Introduction In recent years, various phrase translation approaches (Marcu and Wong, 2002; Och et al., 1999; Koehn et al., 2003) have been shown to outperform word-to-word translation models (Brown et al., 1993) . Many of these phrase alignment strategies rely on the pre-calculated word alignment and use different heuristics to extract the phrase pairs from the Viterbi word alignment path. The Integrated Segmentation and Alignment (ISA) model (Zhang et al., 2003) does not require such word alignment. ISA segments the sentence into phrases and finds their alignment simultaneously. ISA is simple and fast. Translation experiments have shown comparable performance to other phrase alignment strategies which require complicated statistical model training. In this paper, we describe the key idea behind this model and connect it with the competitive linking algorithm (Melamed, 1997) which was developed for word-to-word alignment. Translation Likelihood as a Statistical Test Given a bilingual corpus of language pair F (Foreign, source language) and E (English, target language), if we know the word alignment for each sentence pair we can calculate the co-occurrence frequency for each source/target word pair type C(f, e) and the marginal frequency C(f ) = e C(f, e) and C(e) = f C(f, e). We can apply various statistical tests (Manning and Schütze, 1999) to measure how likely is the association between f and e, in other words how likely they are mutual translations. In the following sections, we will use χ 2 statistics to measure the the mutual translation likelihood (Church and Hanks, 1990) . The Core of the Integrated Phrase Segmentation and Alignment The competitive linking algorithm (CLA) (Melamed, 1997 ) is a greedy word alignment algorithm. It was designed to overcome the problem of indirect associations using a simple heuristic: whenever several word tokens f i in one half of the bilingual corpus co-occur with a particular word token e in the other half of the corpus, the word that is most likely to be e's translation is the one for which the likelihood L(f, e) of translational equivalence is highest. The simplicity of this algorithm depends on a one-to-one alignment assumption. Each word translates to at most one other word. Thus when one pair {f, e} is ""linked"", neither f nor e can be aligned with any other words. This assumption renders CLA unusable in phrase level alignment. We propose an extension, the competitive grouping, as the core component in the ISA model. Competitive Grouping Algorithm (CGA) The key modification to the competitive linking algorithm is to make it less greedy. When a word pair is found to be the winner of the competition, we allow it to invite its neighbors to join the ""winner's club"" and group them together as an aligned phrase pair. The one-to-one assumption is thus discarded in CGA. In addition, we introduce the locality assumption for phrase alignment. Locality states that a source phrase of adjacent words can only be aligned to a target phrase composed of adjacent words. This is not true of most language pairs in cases such as the relative clause, passive tense, and prepositional clause, etc.; however this assumption renders the problem tractable. Here is a description of CGA: For a sentence pair {f , e}, represent the word pair statistics for each word pair {f, e} in a two dimensional matrix L I×J , where L(i, j) = χ 2 (f i , e j ) in our implementation. 1 1. Find i * and j * such that L(i * , j * ) is the highest. Create a seed phrase pair [i * , i * , j * , j * ] which is simply the word pair {f i * , e j * } itself. 2. Expand the current phrase pair [i start , i end , j start , j end ] to the neighboring territory to include adjacent source and target words in the phrase alignment group. There are 8 ways to group new words into the phrase pair. For example, one can expand to the north by including an additional source word f istart−1 to be aligned with all the target words in the current group; or one can expand to the northeast by including f istart−1 and e j end +1 (Figure 1 ). Two criteria have to be satisfied for each expansion: (a) If a new source word f i is to be grouped, max jstart≤j≤j end L(i , j) should be no smaller than max 1≤j≤J L(i , j). Since CGA is a greedy algorithm as described below, this is to guarantee that f i will not ""regret"" the decision of joining the phrase pair because it does not have other ""better"" target words to be aligned with. Similar constraint is applied if a new target word e j is to be grouped. (b) The highest value in the newly-expanded area needs to be ""similar"" to the seed value L(i * , j * ). Expand the current phrase pair to the largest extend possible as long as both criteria are satisfied. 3. The locality assumption means that the aligned phrase cannot be aligned again. Therefore, all the source and target words in the phrase pair are marked as ""invalid"" and will be skipped in the following steps. 4. If there is another valid pair {f i , e j }, then repeat from Step 1. Figure 2 and Figure 3 show a simple example of applying CGA on the sentence pair {je déclare reprise la session/i declare resumed the session}. Exploring all possible groupings The similarity criterion 2-(b) described previously is used to control the granularity of phrase pairs. In cases where the pairs {f 1 f 2 , e 1 e 2 }, {f 1 , e 1 } and {f 2 , e 2 } are all valid translations pairs, similarity is used to control whether we want to align {f 1 f 2 , e 1 e 2 } as one phrase pair or two shorter ones. The granularity of the phrase pairs is hard to optimize especially when the test data is unknown. On the one hand, we prefer long phrases since interaction among the words in the phrase, for example word sense, morphology and local reordering could be encapsulated. On the other hand, long phrase pairs are less likely to occur in the test data than the shorter ones and may lead to low coverage. To have both long and short phrases in the alignment, we apply a range of similarity thresholds for each of the expansion operations. By applying a low similarity threshold, the expanded phrase pairs tend to be large, while a higher similarity threshold results in shorter phrase pairs. As described above, CGA is a greedy algorithm and the expansion of the seed pair restricts the possible alignments for the rest of the sentence. Figure 4 shows an example as we explore all the possible grouping choices in a depth-first search. In the end, all unique phrase pairs along the path traveled are output as phrase translation candidates for the current sentence pair. Phrase translation probabilities Each aligned phrase pair { f , ẽ} is assigned a likelihood score L( f , ẽ), defined as: i max j log L(f i , e j ) + j max i log L(f i , e j ) | f | + |ẽ| where i ranges over all words in f and similarly j in ẽ. Given the collected phrase pairs and their likelihood, we estimate the phrase translation probability by their weighted frequency: P ( f |ẽ) = count( f , ẽ) • L( f , ẽ) f count( f , ẽ) • L( f , ẽ) No smoothing is applied to the probabilities. Learning co-occurrence information In most cases, word alignment information is not given and is treated as a hidden parameter in the training process. We initialize a word pair cooccurrence frequency by assuming uniform alignment for each sentence pair, i.e. for sentence pair (f , e) where f has I words and e has J words, each word pair {f, e} is considered to be aligned with frequency 1 I×J . These co-occurrence frequencies will be accumulated over the whole corpus to calculate the initial L(f, e). Then we iterate the ISA model: 1. Apply the competitive grouping algorithm to each sentence pair to find all possible phrase pairs. (French-English, Finnish-English, German-English and Spanish-English). Conclusion In this paper, we introduced the competitive grouping algorithm which is at the core of the ISA phrase alignment model. As an extension to the competitive linking algorithm which is used for word-to-word alignment, CGA overcomes the assumption of oneto-one mapping and makes it possible to align phrase 3 http://www.isi.edu/licensed-sw/pharaoh/ pairs. Despite its simplicity, the ISA model has achieved competitive translation results. We plan to release ISA toolkit 4 to the community in the near future.",2455968,633bf26c3315fcf80a6d51e1d7f8fa9ab2e72412,11,https://aclanthology.org/W05-0829,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Zhang, Ying  and
Vogel, Stephan",Competitive Grouping in Integrated Phrase Segmentation and Alignment Model,159--162,,,,,,,inproceedings,zhang-vogel-2005-competitive,,,
46,W10-3201,"In this paper we propose a framework of verb semantic description in order to organize different granularity of similarity between verbs. Since verb meanings highly depend on their arguments we propose a verb thesaurus on the basis of possible shared meanings with predicate-argument structure. Motivations of this work are to (1) construct a practical lexicon for dealing with alternations, paraphrases and entailment relations between predicates, and (2) provide a basic database for statistical learning system as well as a theoretical lexicon study such as Generative Lexicon and Lexical Conceptual Structure. One of the characteristics of our description is that we assume several granularities of semantic classes to characterize verb meanings. The thesaurus form allows us to provide several granularities of shared meanings; thus, this gives us a further revision for applying more detailed analyses of verb meanings.","In this paper we propose a framework of verb semantic description in order to organize different granularity of similarity between verbs. Since verb meanings highly depend on their arguments we propose a verb thesaurus on the basis of possible shared meanings with predicate-argument structure. Motivations of this work are to (1) construct a practical lexicon for dealing with alternations, paraphrases and entailment relations between predicates, and (2) provide a basic database for statistical learning system as well as a theoretical lexicon study such as Generative Lexicon and Lexical Conceptual Structure. One of the characteristics of our description is that we assume several granularities of semantic classes to characterize verb meanings. The thesaurus form allows us to provide several granularities of shared meanings; thus, this gives us a further revision for applying more detailed analyses of verb meanings. Introduction In natural language processing, to deal with similarities/differences between verbs is essential not only for paraphrase but also textual entailment and QA system which are expected to extract more valuable facts from massively large texts such as the Web. For example, in the QA system, assuming that the body text says ""He lent her a bicycle"", the answer of the question ""He gave her a bicycle?"" should be ""No"", however the answer of ""She rented the bicycle?"" should be ""Yes"". Thus constructing database of verb similarities/differences en-ables us to deal with detailed paraphrase/nonparaphrase relations in NLP. From the view of the current language resource, how the shared/different meanings of ""He lent her a bicycle"" and ""He gave her a bicycle"" can be described? The shared meaning of lend and give in the above sentences is that they are categorized to Giving Verbs, as in Levin's English Verb Classes and Alternations (EVCA) (Levin, 1993) , while the different meaning will be that lend does not imply ownership of the theme, i.e., a bicycle. One of the problematic issues with describing shared meaning among verbs is that semantic classes such as Giving Verbs should be dependent on the granularity of meanings we assumed. For example, the meaning of lend and give in the above sentences is not categorized into the same Frame in FrameNet (Baker et al., 1998) . The reason for this different categorization can be considered to be that the granularity of the semantic class of Giving Verbs is larger than that of the Giving Frame in FrameNet 1 . From the view of natural language processing, especially dealing the with propositional meaning of verbs, all of the above classes, i.e., the wider class of Giving Verbs containing lend and give as well as the narrower class of Giving Frame containing give and donate, are needed. Therefore, in this work, in order to describe verb meanings with several granularities of semantic classes, a thesaurus form is adopted for our verb dictionary. Based on the background, this paper presents a thesaurus of predicate-argument structure for verbs on the basis of a lexical decompositional framework such as Lexical Conceptual Structure (Jackendoff, 1990) ; thus our proposed thesaurus can deal with argument structure level alternations such as causative, transitive/intransitive, stative. Besides, taking a thesaurus form enables us to deal with shared/differenciate meaning of verbs with consistency, e.g., a verb class node of ""lend"" and ""rent"" can be described in the detailed layer of the node ""give"". We constructed this thesaurus on Japanese verbs and the current status of the verb thesaurus is this: we have analyzed 7,473 verb meanings (4,425 verbs) and organized the semantic classes in a Àve-layer thesaurus with 71 semantic roles types. Below, we describe background issues, basic design issues, what kind of problems remain, limitations and perspectives of applications. Existing Lexical Resources and Drawbacks Lexical Resources in English From the view of previous lexical databases In English, several well-considered lexical databases are available, e.g., EVCA, Dorr's LCS (Dorr, 1997 ), FrameNet, WordNet (Fellbaum, 1998 ), VerbNet (Kipper-Schuler, 2005) and PropBank (Palmer et al., 2005) . Besides there is the research project (Pustejovsky and Meyers, 2005) to Ànd general descriptional framework of predicate argument structure by merging several lexical databases such as Prop-Bank, NomBank, TimeBank and PennDiscouse Treebank. Our approach corresponds partly to each lexical database, (i.e., FrameNet's Frame and FrameElements correspond to our verb class and semantic role labels, and the way to organize verb similarity classes with thesaurus corresponds with WordNet's synset), but is not exactly the same; namely, there is no lexical database describing several granularities of semantic classes between verbs with arguments. Of course, since the above English lexical databases have links with each other, it is possible to produce a verb dictionary with several granularities of semantic classes with arguments. However, the basic categories of classify-ing verbs would be little different due to the different background theory of each English lexical database; it must be not easy to add another level of semantic granularity with keeping consistency for all the lexical databases; thus, thesaurus form is needed to be a core form for describing verb meanings 2 . Lexical Resources in Japanese In previous studies, several Japanese lexicons were published: IPAL (IPA, 1986) focuses on morpho-syntactic classes but IPAL is small 3 . EDR (Jap, 1995) consists of a large-scale lexicon and corpus (See Section 3.4). EDR is a well-considered and wide coverage dictionary focusing on translation between Japanese and English, but EDR's semantic classes were not designed with linguistically-motivated lexical relations between verbs, e.g., alternations, causative, transitive, and detransitive relations between verbs. We believe these relations must be key for dealing with paraphrase in NLP. Recently Japanese FrameNet (Ohara et al., 2006) and Japanese WordNet (Bond et al., 2008) are proposed. Japanese FrameNet currently published only less than 100 verbs 4 . Besides Japanese WordNet contains 87000 words and 46000 synsets, however, there are three major difÀculty of dealing with paraphrase relations between verbs: (1) there is no argument information; (2) existing many similar synsets force us to solve Àne disambiguation between verbs when we map a verb in a sentence to WordNet; (3) the basic verbs of Japanese (i.e., highly ambiguous verbs) are wrongly assigned to unrelated synsets because they are constructed by translation from English to Japanese. Thesaurus of Predicate-Argument Structure The proposed thesaurus of predicate-argument structure can deal with several levels of verb classes on the basis of granularity of deÀned verb meaning. In the thesaurus we incorporate LCSbased semantic description for each verb class that can provide several argument structure such as construction grammar (Goldberg, 1995) . This must be high advantage to describe the different factors from the view of not only syntactic functions but also internal semantic relations. Thus this characteristics of the proposed thesaurus can be powerful framework for calculating similarity and difference between verb senses. In the following sections we explain the total design of thesaurus and the details. Design of Thesaurus The proposed thesaurus consists of hierarchy of verb classes we assumed. A verb class, which is a conceptual class, has verbs with a shared meaning. A parent verb class includes concepts of subordinate verb class; thus a subordinate verb class is a concretization of the parent verb class. A verb class has a semantic description that is a kind of semantic skeleton inspired from lexical conceptual structure (Jackendoff, 1990; Kageyama, 1996; Dorr, 1997) . Thus a semantic description in a verb class describes core semantic relations between arguments and shadow arguments of a shared meaning of the verb class. Since verb can be polysemous, each verb sense is designated with example sentences. Verb senses with a shared meaning are assigned to a verb class. Every example sentence is analyzed into their arguments and semantic role types; and then their arguments are linked to variables in semantic description of verb class. This indicates that one semantic description in a verb class can provide several argument structure on the basis of syntactic structure. This architecture is related to construction grammar. Here we explain this structure using verbs such as rent, lend, give, hire, borrow, lease. We assume that each verb sense we focus on here is designated by example sentences, e.g., ""Mother gives a book to her child"", ""Kazuko rents a bicycle from her friend"", and ""Taro lend a car to his friend"". As Figure 1 shows that all of the above verb senses are involved in the verb class Moving of One's Possession 5 . The semantic description, which expresses core meaning of the verb class Moving of One's Possession is ([Agent] CAUSE) BECOME [Theme] BE AT [Goal]. Where the brackets [] denote variables that can be Àlled with arguments in example sentences. Likewise parentheses () denote occasional factor. ""Agent"" and ""Theme"" are semantic role labels that can be annotated to all example sentences. Figure 1 describes semantic relations between ""Agent"" and ""Theme"". Since semantic role labels are annotated to all of the example sentences, the variables in the semantic description can be linked to practical arguments in example sentences via semantic role labels (See Figure 2 ). Construction of Verb Class Hierarchy To organize hierarchical semantic verb class, we take a top down and a bottom up approaches. As for a bottom up approach, we use verb senses deÀned by a dictionary as the most Àne-grained meaning; and then we group verbs that can be considered to share some meaning. As for a dictionary, we use the Lexeed database (Fujita et al., 2006) , which consists of more than 20,000 verbs with explanations of word sense and example sentences. As a top down approach, we take three semantic classes: State, Change of State, and Activity as top level semantic classes of the thesaurus according to Vendler's aspectual analysis (Vendler, 1967) (See Figure 4 ). This is because the above three classes can be useful for dealing with the propositional, especially, resultative aspect of verbs. For example ""He threw a ball"" can be an Activity and have no special result; but ""He broke the door"" can be a Change of State and then we can imagine a result, i.e., broken door. When other verb senses can express the same results, e.g., ""He destroyed the door,"" we would like to regard them as having the same meaning. We deÀne verb classes in intermediate hierarchy by grouping verb sense on the basis of aspectual category (i.e., action, state, change of state), argument type (i.e., physical, mental, information), and more detailed aspects depending on aspectual category. For example, walk the country, travel all over Europe and get up the stairs can be considered to be in the Move on Path class. Verb class is essential for dealing with verb meanings as synsets in WordNet. Even if we had given an incorrect class name, the thesaurus will work well if the whole hierarchy keeps is-a relation, namely, the hierarchy does not contain any multiple inheritance. The most Àne-grained verb class before individual verb sense is a little wider than alternations. Currently, for the Àne-grained verb class, we are organizing what kind of differentiated classes can be assumed (e.g., manner, background, presupposition, and etc.). Semantic Role Labels The aim of describing arguments of a target verb sense is (1) to link the same role arguments in a related verb sense and (2) to provide disambiguated information for mapping a surface expression to a verb sense. The Lexeed database provides a representative sentence for each word sense. The sentence is simple, without adjunctive elements such as unessential time, location or method. Thus, a sentence is broken down into subject and object, and semantic role labels are annotated to them (Figure 3 ). ex.: nihon-ga shigen-wo yunyuu-suru trans.: Japan resouces import (NOM) (ACC) AS: Agent Theme Of course, only one representative sentence would miss some essential arguments; also, we do not know how many arguments are enough. This can be solved by adding examples 6 ; however, we consider the semantic role labels of each representative sentence in a verb class as an example of assumed argument structure to a verb class. That is to say, we regard a verb class as a concept of event and suppose it to be a Àxed argument frame for each verb class. The argument frame is described as compositional relations. The principal function of the semantic role label name is to link arguments in a verb class. One exception is the Agent label. This can be a marker discriminating transitive and intransitive verbs. Since the semantic class of the thesaurus focuses on Change of State, transitive alternation cases such as ""The president expands the business"" and "" The business expands"" can be categorized into the same verb class. Then, these two examples are differentiated by the Agent label. Compositional Semantic Description As described in Section 3.1, we incorporate compositional semantic structure to each verb class to describe syntactically motivated lexical semantic relations and entailment meanings that will expand the thesaurus. The beneÀt of compositional style is to link entailed meanings by means of compositional manner. As an example of entailment, Figure 5 shows that a verb class Move to Goal entails Theme to be Goal, and this corresponds to a verb class Exist. 6 We are currently constructing an SRL annotated corpus. In this verb thesaurus, being different from previous LCS studies, we try to ensure the compositional semantic description as much as possible by means of linking each sub-event structure to both a semantic class and example sentences. Therefore, we believe that our verb thesaurus can provide a basic example data base for LCS study. Intrinsic Evaluation on Coverage We did manual evaluation that how the proposed verb thesaurus covers verb meanings in news articles. The results on Japanese new corpus show that the coverage of verbs is 84.32% (1825/2195) in 1000 sentences randomly sampled from Japanese news articles 7 . Besides we take 200 sentences and check whether the verb meanings in the sentences can correspond to verb meaning in our thesaurus. The result shows that our thesaurus meaning covers 99.5% (199 verb meanings/200 verb meanings) of 200 7 Mainichi news article in 2003. verbs 8 . Discussions Comparison with Existing Resources Table 1 and Table 2 show a comparison of statistical characteristics with existing resources. In the tables, WN and Exp denote the number of word meanings and example sentences, respectively. Also, SRL denotes the number corresponding to semantic role label. Looking at number of concepts, our Thesaurus has 709 types of concepts (verb classes) which is similar to FrameNet and more than VerbNet. This seems to be natural because FrameNet is also language resource constructed on argument structure. Thanks to our thesaurus format, if we need more Àne grained concepts, we can expand our thesaurus by adding concepts as new nodes at the lowest layer in the hierarchy. While at the number of SRL, FrameNet has much more types than our thesaurus, and in the other resources VerbNet and EDR the number of SRL is less than our thesaurus. This comes from the different design issue of semantic role labels. In FrameNet they try to differentiate argument types on the basis of the assumed concept, i.e., Frame. In contrast with FrameNet we try to merge the same type of meaning in arguments. VerbNet and EDR also deÀned abstracted SRL; The difference between their resources and our thesaurus is that our SRLs are deÀned taking into account what kind of roles in the core concept i.e., verb class; while SRLs in VerbNet and EDR are not dependent on verb's class. Table 2 shows that our thesaurus does not have large number in registered words and examples comparing to EDR and JWordNet. As we stated in Section 3.5, the coverage of our verb class to newspaper articles are high, but we try to add examples by constructing annotated Japanese corpus of SRL and verb class. Limitations of Developed Thesaurus One of the difÀculties of annotating the semantic class of word sense is that a word sense can be considered as several semantic classes. The proposed verb thesaurus can deal with multiple semantic classes for a verb sense by adding them into several nodes in the thesaurus. However, this does not seem to be the correct approach. For example, what kind of Change of State semantic class can be considered in the following sentence? a. He took on a passenger. Assuming that passenger is Theme, Move to goal could be possible when we regard the vehicle 9 as Goal. In another semantic class, Change State of Container could be possible when we regard the vehicle as a container. Currently, all of the verb senses are linked to only one semantic class that can be considered as the most related semantic class. From the user side, i.e., dealing with the propositional meaning of the sentence (a.), various meanings should be estimated. Consider the following sentence: b. Thus, we were packed. As the semantic class of the sentence (a.) Change State of Container could better explain why they are packed in the sentence (b.) The other related issue is how we describe the scope, e.g., c. He is hospitalized. If we take the meaning as a simple manner, Move to Goal can be a semantic class. This can be correct from the view of annotation, but we can guess he cannot work or he will have a tough time as following events. FrameNet seems to be able to deal with this by means of a special type of linking between Frames. Consequently, we think the above issues of semantic class should depend on the application side's demands. Since we do not know all of the requirements of NLP applications currently, then it must be sufÀcient to provide an expandable descriptional framework of linguistically motivated lexical semantics. Remaining and Conceivable Ways of Extension One of the aims of the proposed dictionary is to identify the sentences that have the same meanings among different expressions. One of the challenging paraphrase relations is that the sentences expressed from the different view points. Given the buying and selling in Figure 6 , a human can understand that both sentences denote almost the same event from different points of view. This indicates that the sentences made by humans usually contain the point of view of the speaker. This is similar to a camera, and we need to normalize the expressions as to their original meaning. We consider that NLP application researchers need to relate these expressions. Logically, if we know ""buy"" and ""sell"" have shared meanings of giving and taking things, we can describe their relations with ""or"" in logical form. Therefore, Ànding and describing these verb relations will be essential for dealing with propositional meanings of a sentence. For further view of application, event matching to Ànd a similar situation in Web documents is supposed to be a practical and useful application. Assuming that a user is confronted with the fact that wireless LAN in the user's PC does not work, and the user wants to search for documents that provide a solution, the problem is that expressions of situations must be different from the views of individual writers, e.g., ""wireless LAN did not work"" or ""wireless LAN was disconnected"". How can we Ànd the same meaning in these expressions, and how can we extract the answers by Ànding the same situation from FAQ documents? To solve this, a lexical database describing verb relations between ""go wrong"" and ""disconnect"" must be the base for estimating how the expressions can be similar. Therefore, constructing a lexicon can be worthwhile for developing NLP applications. Conclusion In this paper, we presented a framework of a verb dictionary in order to describe shared meaning as well as to differentiate meaning between verbs from the viewpoint of relating eventual expressions of NLP. One of the characteristics is that we describe verb relations on the basis of several semantic granularities using a thesaurus form with argument structure. Semantic granularity is the basis for how we categorize (or recognize which semantic class relates to a verb meaning). Also, we ensure functions and limitations of semantic classes and argument structure from the viewpoint of dealing with paraphrases. That is, required semantic classes will be highly dependent on applications; thus, the framework of the verb-sense dictionary should have expandability. The proposed verb thesaurus can take several semantic granularities; therefore, we hope the verb thesaurus will be applicable to NLP's task 10 . In future work, we will continue to organize differentiated semantic classes between verbs and develop a system to identify the same event descriptions. Acknowledgments We would like to thank NTT Communication Research Laboratory for allowing us to use the Lexeed database, and Professor Koyama for many useful comments.",10903030,4fa1c9b0445f47612572f785ee803214b4e7826d,10,https://aclanthology.org/W10-3201,Coling 2010 Organizing Committee,"Beijing, China",2010,August,Proceedings of the Eighth Workshop on {A}sian Language Resouces,"Takeuchi, Koichi  and
Inui, Kentaro  and
Takeuchi, Nao  and
Fujita, Atsushi",A Thesaurus of Predicate-Argument Structure for {J}apanese Verbs to Deal with Granularity of Verb Meanings,1--8,,,,,,,inproceedings,takeuchi-etal-2010-thesaurus,,,
47,R13-1055,"In this paper we undertake a large crossdomain investigation of sentiment domain adaptation, challenging the practical necessity of sentiment domain adaptation algorithms. We first show that across a wide set of domains, a simple ""all-in-one"" classifier that utilizes all available training data from all but the target domain tends to outperform published domain adaptation methods. A very simple ensemble classifier also performs well in these scenarios. Combined with the fact that labeled data nowadays is inexpensive to come by, the ""kitchen sink"" approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that ""flip"" polarity across domains is not borne out empirically.","In this paper we undertake a large crossdomain investigation of sentiment domain adaptation, challenging the practical necessity of sentiment domain adaptation algorithms. We first show that across a wide set of domains, a simple ""all-in-one"" classifier that utilizes all available training data from all but the target domain tends to outperform published domain adaptation methods. A very simple ensemble classifier also performs well in these scenarios. Combined with the fact that labeled data nowadays is inexpensive to come by, the ""kitchen sink"" approach, while technically nonglamorous, might be perfectly adequate in practice. We also show that the common anecdotal evidence for sentiment terms that ""flip"" polarity across domains is not borne out empirically. Introduction Automatic detection and analysis of sentiment around products, brands, political issues etc. has triggered a large amount of research in the past 15 -20 years (for a recent overview see Pang & Lee 2008 and Liu 2012) . Early work focused on algorithms for mining sentiment dictionaries (Hatzivassiloglou and McKeown 1997, Turney 2002) ; this was followed by the exploration of supervised techniques (Pang et al. 2002) and, somewhat more recently, by investigations of domain adaptation techniques. Also more recently, the focus has broadened from the detection of polarity (negative/positive sentiment) to more nuanced approaches that try to identify targets and holders of sentiment, sentiment strength, or finer-grained mood distinctions (e.g. Wilson et al. 2006, Kim and Hovy 2006) . Within the polarity detection paradigm, a number of common assumptions have been shared in the community and are frequently repeated in the literature. Two of these fundamental assumptions are: 1. Obtaining sufficient labeled data for supervised training is expensive 2. Sentiment models trained on one domain tend to perform poorly on new, unseen domains A conclusion that is often drawn from these assumptions is that domain adaptation of sentiment models from a domain with sufficient labeled data to a new domain with little labeled data is an important problem and requires new and sophisticated algorithms. In this paper, we empirically re-examine the assumptions above. Based on a wide range of experiments on 27 different domains, we challenge the conclusion that domain adaptation for polarity detection necessarily requires novel and sophisticated machinery. It is important to keep in mind, however, that our claims are strictly limited to the problem under investigation, namely polarity detection. We do not make any claims whatsoever about domain adaptation for other sentiment-related problems or general problems in machine learning. Based on readily available data from 27 domains, we show that a ""kitchen sink"" approach where all source domain data are combined to train a single classifier sets a surprisingly high baseline for polarity identification accuracy across domains. We also show on a previously released data set of four domains that the result is competitive with a state-of-theart domain adaptation approach using Structural Correspondence Learning. We then show that a straightforward ensemble learner can, for some domains, improve results further, without any need for specialized learning algorithms. Since most work in domain-adaptation only provides published results on pairwise adaptation between domains and not on multi-domain adaptation, we hope to establish a new baseline for future adaptation techniques to compare against. Related Work Of direct importance to the discussion in this paper are results from domain adaptation in polarity detection. One of the earlier successful approaches (Blitzer et al. 2006 (Blitzer et al. , 2007) ) involved Structural Correspondence Learning (SCL). SCL identifies ""pivot"" features that are both highly discriminative in the labeled source domain data and also frequent in the unlabeled target domain data. In a subsequent step, linear predictors for the pivot terms are learned from the unlabeled target data and from the source data. Daumé (2007) approached domain adaptation from a fully labeled source domain to a partially labeled target domain by augmenting the feature space. Instead of using a single, general, feature set for source and target, three distinct feature sets are created: the general set of features, a source-domain specific version of the feature set, and a target-specific version of the feature set. Li and Zong (NLP-KE 2008) explore a classifier combination technique they call ""Multiple-Label Consensus Training"" which results in better accuracy than non-adapted models on the data sets used in Blitzer et al. (2007) . They also addressed the multi-domain sentiment analysis problem using feature -level fusion and classifier-level fusion approaches in Li and Zong (ACL 2008) . Dredze and Crammer (2008) have proposed a multi-domain online learning framework based on parameter combination from multiple Confidence Weighted (CW) classifiers. Their Multi-Domain Regularization (MDR) framework seeks to learn domain specific parameters guided by the shared parameter across domains. Samdani and Yih (2011) propose an ensemble learner that consists of classifiers trained on different feature groups. The feature groups are Chen et al. (2011) use a specific co-training algorithm for domain adaptation on the Blitzer et al. (2007) data set. In averaged pair-wise comparisons they establish gains over a source-plustarget logistic regression baseline. Glorot et al. (2011) investigate a deep learning approach to domain adaptation and report increased accuracy across domains both on the Blitzer et al. (2007) 4-domain data set and the larger Amazon review data set (25 domains) also made available in that release. They also introduce a new metric for transfer learning: Transfer Ratio. Datasets & Experimental Setup This section illustrates the datasets, the methods and the setup of our experiments. Datasets The datasets we used in our experiments have been obtained from three sources: 1. Amazon reviews 1 : this dataset contains more than 5.8 million reviews. It has been used in previous work on sentiment analysis (see Glorot et al. (2011) ). The Amazon reviews include 25 domains as shown in 2010 to Oct. 31, 2011 . The dataset has been originally annotated for affects. We mapped the positive affects ""joviality"" and ""serenity"" to positive sentiment and the negative affects ""fatigue"", ""hostility"", and ""sadness"" to negative sentiment. We selected a balanced dataset of 2,000 tweets from the various months of the collected tweets. The average review length for the Amazon and hotel reviews is 437 characters and 97 words. In total, we used 27 domains namely the identified based on how stable the feature distribution is across domains, which can either be estimated from the data directly or can be hypothesized based on domain knowledge. 25 Amazon domains, the hotel domain and the Twitter domain. We considered Twitter as a domain though the content of tweets spans multiple domains since it has different characteristics from the product reviews. Tweets are constrained log-likelihood ratio (LLR). Further, we used the accuracy metric to indicate the performance of each of the above four domain adaptation techniques. We also employed the Transfer Ratio metric proposed by Glorot et al. (2011) The Amazon reviews and the hotel reviews are rated between 1 and 5 on a 5 point scale where 1 is the most negative and 5 is the most positive. We have extracted only the reviews that are rated 5 and 1 to represent the positive and negative reviews respectively. Further, we ensured that the datasets we extracted and used for training are balanced between positive and negative reviews. Table 1 summarizes the 27 domains and their dataset sizes including the balanced datasets we used for training. Experimental Setup In our experiments, we employed the datasets of the 27 domains mentioned in section 3.1. In each experiment, we have employed one domain for testing while the other 26 domains have been used for training. We compared four domain adaptation techniques: 1. One classifier trained in all source domains. 2. An ensemble of classifiers, each trained on a source domain, combined into an ensemble. 3. The domain adaptation approach proposed in Daumé (2007) . 4. We also compared the results of approaches 1 and 2 to published results on Structural Correspondence Learning (SCL) by using the same datasets as in Blitzer et al. (2007) . In all our experiments, we employed Maximum Entropy-based classification with vanilla parameter settings and feature reduction using ure the performance of the all-in-one and ensemble classifiers. The rest of the subsection illustrates the experimental setup for each of the above four approaches. In-domain Classifiers To establish a ""ceiling"" performance we built an in-domain classifier for each of the 27 domains. The in-domain classifier is trained with a dataset of that one domain and tested on the same domain (using cross-validation). This standard indomain supervised setup establishes an upper bound for classification performance (although in some cases we will see that other techniques can outperform this upper bound). Features consist of binary unigram and bigram features. On average, the total number of features in each domain is 52,039. Feature reduction was performed using LLR, retaining only the top 20,000 most predictive features as established on the training set. We compare the results obtained from testing each domain with the three approaches to its indomain classifier results. All-in-one Classifier The all-in-one classifier is a maximum entropy classifier trained with the source domain datasets merged together. In this setting, the classifier is trained with data from multiple domains, which exposes it to multiple sentiment vocabularies at training time, creating a somewhat domain-independent and general model. The all-in-one classifier is trained with 26 domains datasets while being tested on the held-out 27 th domain. Ensemble Classifiers One approach to address the problem of domain adaptation is to construct an ensemble of classifiers, all of which contribute partially to the final result (see Dietterich (1997) for an overview). We constructed an ensemble of in-domain sentiment classifiers, one for each source domain. http://svmlight.joachims.org/ Daumé (2007) addresses domain adaptation where a large, annotated corpus of data from the source domain is available with only a small, annotated corpus of the target domain. Daumé's work leverages both annotated datasets to obtain a model that performs well on the target domain. For K source domains, the augmented feature space consists of K+1 copies of the original feature space. However, creating three versions of each feature in both the source and the target domains grows the feature space exponentially, which is prohibitive in a many-domain adaptation scenario such as ours which consists of a total of 27 domains. We addressed this challenge by considering the 26 source domains as a single source domain being adapted to the target domain. This setup along with feature reduction enabled us to apply Daumé's approach without too much of an inflation of the feature space. However, we also recognize that this likely compromises the power of the feature augmentation approach. Blitzer et al. (2007) employ the Structural Correspondence Learning (SCL) algorithm for sentiment domain adaptation. Blitzer et al. evaluate the SCL domain adaptation on four publicly released datasets from Amazon product reviews: books, DVDs, electronics and kitchen appliances. In these four datasets, reviews with rating > 3 were labeled positive, those with rating < 3 were labeled negative, and the rest discarded because their polarity was ambiguous. 1000 positive and 1000 negative labeled examples were used for each domain. Some unlabeled data were additionally used including 3685 (DVDs) and 5945 (kitchen). Each labeled dataset was split into 1600 instances for training and 400 instances for testing. The baseline in Blitzer et al. (2007) is a linear classifier trained without adaptation, while their ceiling reference is the same as ours, which is the in-domain classifier trained and tested on the same domain. Blitzer's Structural Correspondence Learning We conducted a set of experiments employing the four datasets used for SCL domain adaptation. In these experiments, we compare the results of our all-in-one classifier and the ensemble classifier trained and tested on the four datasets to the results of SCL and its variation SCL-MI domain adaptation as reported by Blitzer et al. (2007) baseline and ceiling in-domain classifiers for the four domains. Results & Discussion This section summarizes the results of the experiments described in section 3.2 while further scrutinizing the comparison between the four domain adaptation sentiment analysis techniques. We also report the Transfer Ratio results of the all-in-one and ensemble classifiers. Generally, the all-in-one classifier is closely comparable to the in-domain classifier of each domain Results In this section, we summarize the various results obtained from the set of experiments described in section 3.2. In the summary of each experiment results, we also plot the in-domain classifier results of each domain as the ceiling of comparison. All-in-one Classifier Experiments In the all-in-one classifier experiments, the sentiment classifier is trained with 26 domain datasets while testing it with the 27 th domain. Table 3 summarizes the results. The results of the allin-one classifier are very close to the in-domain classifiers in most domains except for the apparel, beauty, magazines, outdoor living, office products and software. Ensemble Classifier Experiments We produced the results of the ensemble of classifiers using the three settings: majority votes, sum of weights, and meta-classification using both logistic regression and SVM. Table 3 summarizes the results of the three settings used in the ensemble. Table 3 shows that the ensembles with sum of weights and meta-training (SVM sigmoid kernel) are the most comparable to the in-domain classifier of each domain. We also experimented with variations of logistic regression and SVM for meta-training. The non-linear (RBF kernel) SVM meta-classifier outperforms the linear logistic regression model. We have employed two variations of SVM, namely, a radial basis function with gamma 0.01 and sigmoid kernel. In most domains, the SVM model trained with 50 positive and 50 negative feedback examples is not far off the one trained with 5 positive and 5 negative feedback examples. This shows that even with little labeled data in the target domain, the en-semble could effectively combine the weights of the classifier votes. We expect the ensemble to achieve steady but slow performance gains over time while collecting more feedback examples. Hal Daumé's Domain-Adaptation Approach We compared the performance of the all-in-one and ensemble classifiers to Daumé's feature augmentation algorithm. Table 3 shows that the all-in-one classifier exceeds Daumé's approach in all 27 domains given our current implementation of Daumé's approach. The ensemble exceeds Daumé's approach on all domains except office, kitchen & housewares, magazines, office products, and tweets. Structural Correspondence Learning (SCL) We employed the four domains datasets used in Blitzer et al. (2007) to train and test the all-in one and the ensemble classifiers. We also replicated the in-domain results of these four datasets using our maximum entropy classifier. We compare the results of the all-in-one and the ensemble classifier to the SCL and its variation SCL-MI adaptation techniques using the four datasets used to evaluate SCL and SCL-MI in Blitzer et al. (2007) . Note that the results published in Blitzer's work represent pairwise domain-adaptation, while our ensemble and all-in-one results are based on training on three of Blitzer's domains and testing on the held-out fourth domain. This makes it impossible to draw a direct comparison, but we can still observe that in general, it is best to simply combine as many domains as possible in an all-in-one or ensemble approach as compared to carefully adapting a single domain. Table 2 Where is the transfer error defined as the test error obtained by a method trained on the source domain S and tested on the target domain T. is the test error obtained by the baseline method. The transfer ratio Q also characterizes the transfer but is defined by replacing the difference by a quotient in t:  Where n is the number of couples (S, T) with S≠T. The all-in-one classifier had a 1.12 transfer ratio across domains, which is very close to the best result of ~1.07 in Glorot et al. The ensemble with Sigmoid kernel of SVM trained on 50 positive and 50 negative feedback examples from the target domain had 1.81 transfer ratio. The ensemble with radial basis function (gamma=0.01) trained on 5 positive and 5 negative feedback examples from the target domain had 1.85 transfer ratio. Note that the transfer ratio of the indomain classifier, which is used a base-line for calculating the transfer ratio is 1. The transfer ratio of the all-in-one classifier is better than the transfer ratio of the ensemble with its two variations. Discussion The results in the previous section indicate that both the all-in-one and the ensemble approaches exceed both Daumé's domain adaptation technique on the 27 datasets (given our current implementation of Daumé's approach) and SCL on the four datasets in Blitzer et al. (2007) and that the all-in-one approach achieves comparable results in terms of transfer ratio to Glorot et al. (2011) . The ensemble approach exceeds the all-in-one in some domains like apparel and automotive. They both are very close in some domains like When comparing the all-in-one and the ensemble approaches on the four datasets in Blitzer et al. (2007) , the all-in-one exceeds the ensemble only in the DVD domain. The ensemble exceeds the all-in-one in electronics and kitchen & housewares. They both perform at the same accuracy level on the books domain. We have also employed NcNemar significance test between pairs of the all-in-one, the ensemble and Daumé's approaches on the 27 domains. Table 4 shows the significance difference between the approaches' combinations. Finally, we would like to do some initial exploration of the role of features across domains. The commonly held belief is that sentiment indicators such as ""hot"" can change their polarity from domain to domain (e.g. it is positive in the food domain while it is negative in the negative domain), contributing to the need for domain adaptation. On the other hand, the success of the all-in-one classifier indicates that a greater number of observed sentiment features and more solid statistics on those features are more important than capturing domain-specific polarity changes. In order to gather evidence for or against these hypotheses, we first calculated the number of overlapping features between each pair of domains within the 27 domains. The average percentage of features that overlap between pairs of domain is only 12.48%. Furthermore, only a very small set of the highly sentiment-correlated features overlap. 16 features overlap among the 27 domains which accounts for only 0.08% of the features. Examples of positive overlapping feature are ""highly"", ""excellent"", and ""great"". Negative overlapping features are ""waste"", ""terrible"", and ""worst"". This low feature overlap of sentiment-bearing features lends some support to the hypothesis that in order to capture a general, large-scale sentiment vocabulary nothing beats diverse and plentiful training data. The low feature overlap also justifies why the all-in-one classifier exceeds the ensemble though the latter has access to some labeled data in the target Second, we examined the question of polaritychanging sentiment features. Among the top 1000 features in each domain ranked by LLR, we counted the common features among multiple domains. The number of common features among 15 domains is 42 features. Only 13 features are common among 20 domains while there are no common features from the highest 1000 likelihood ratio features among the 27 domains. Most features do not flip polarity across domains. For example the word ""waste"" is common among 20 domains and maintains a negative polarity across the domains. Very few features flip polarity across domains. The word ""highly"" is shared across 23 domains. It maintains a positive polarity in all domains while it flips in Tools & Hardware. The word ""refund"" is shared in 20 domains. It maintains a negative polarity in almost all domains except Gourmet Food. Conclusion In this paper, we empirically re-examine the assumption that adapting one or multiple domains with plenty of labeled sentiment polarity data to one domain with little labeled data requires new and sophisticated algorithms. We evaluate four domain adaptation techniques on a wide variety of domains in two major groups of state-of-theart datasets. Our experiments show that overall, simple domain adaptation techniques like the allin-one classifier do comparably well if not better than more sophisticated domain adaptation techniques. Combined with the fact that labeled sentiment data tends to be cheap to come by through either the collection of product reviews from the web or inexpensive crowd-sourced labeling, this indicates that in practice, domain-adaptation for sentiment detection might be of less importance than previously claimed. We also show that the often anecdotally observed ""polarity-flip"" of sentiment terms from one domain to another in practice is a rather rare occurrence and might not be as detrimental to sentiment domain adaptation as assumed in much of the literature.",17625127,d0f6b69813dd7eaef42fd07f3c23948cd1749bdb,12,https://aclanthology.org/R13-1055,"INCOMA Ltd. Shoumen, BULGARIA","Hissar, Bulgaria",2013,September,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,"Mansour, Riham  and
Refaei, Nesma  and
Gamon, Michael  and
Abdul-Hamid, Ahmed  and
Sami, Khaled",Revisiting the Old Kitchen Sink: Do we Need Sentiment Domain Adaptation?,420--427,,,,,,,inproceedings,mansour-etal-2013-revisiting,,,"Sentiment Analysis, Stylistic Analysis, and Argument Mining"
48,W05-0831,"This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality.","This paper presents novel approaches to reordering in phrase-based statistical machine translation. We perform consistent reordering of source sentences in training and estimate a statistical translation model. Using this model, we follow a phrase-based monotonic machine translation approach, for which we develop an efficient and flexible reordering framework that allows to easily introduce different reordering constraints. In translation, we apply source sentence reordering on word level and use a reordering automaton as input. We show how to compute reordering automata on-demand using IBM or ITG constraints, and also introduce two new types of reordering constraints. We further add weights to the reordering automata. We present detailed experimental results and show that reordering significantly improves translation quality. Introduction Reordering is of crucial importance for machine translation. Already (Knight et al., 1998) use full unweighted permutations on the level of source words in their early weighted finite-state transducer approach which implemented single-word based translation using conditional probabilities. In a refinement with additional phrase-based models, (Kumar et al., 2003) define a probability distribution over all possible permutations of source sentence phrases and prune the resulting automaton to reduce complexity. A second category of finite-state translation approaches uses joint instead of conditional probabilities. Many joint probability approaches originate in speech-to-speech translation as they are the natural choice in combination with speech recognition models. The automated transducer inference techniques OMEGA (Vilar, 2000) and GIATI (Casacuberta et al., 2004 ) work on phrase level, but ignore the reordering problem from the view of the model. Without reordering both in training and during search, sentences can only be translated properly into a language with similar word order. In (Bangalore et al., 2000) weighted reordering has been applied to target sentences since defining a permutation model on the source side is impractical in combination with speech recognition. In order to reduce the computational complexity, this approach considers only a set of plausible reorderings seen on training data. Most other phrase-based statistical approaches like the Alignment Template system of Bender et al. (2004) rely on (local) reorderings which are implicitly memorized with each pair of source and target phrases in training. Additional reorderings on phrase level are fully integrated into the decoding process, which increases the complexity of the system and makes it hard to modify. Zens et al. (2003) reviewed two types of reordering constraints for this type of translation systems. In our work we follow a phrase-based translation approach, applying source sentence reordering on word level. We compute a reordering graph ondemand and take it as input for monotonic translation. This approach is modular and allows easy introduction of different reordering constraints and probabilistic dependencies. We will show that it performs at least as well as the best statistical machine translation system at the IWSLT Evaluation. In the next section we briefly review the basic theory of our translation system based on weighted finite-state transducers (WFST). In Sec. 3 we introduce new methods for reordering and alignment monotonization in training. To compare different reordering constraints used in the translation search process we develop an on-demand computable framework for permutation models in Sec. 4. In the same section we also define and analyze unrestricted and restricted permutations with some of them being first published in this paper. We conclude the paper by presenting and discussing a rich set of experimental results. Machine Translation using WFSTs Let f J 1 and e I i be two sentences from a source and target language. Assume that we have word level alignments A of all sentence pairs from a bilingual training corpus. We denote with ẽJ 1 the segmentation of a target sentence e I 1 into J phrases such that f J 1 and ẽJ 1 can be aligned to form bilingual tuples (f j , ẽj ). If alignments are only functions of target words A : {1, . . . , I} → {1, . . . , J}, the bilingual tuples (f j , ẽj ) can be inferred with e. g. the GIATI method of (Casacuberta et al., 2004) , or with our novel monotonization technique (see Sec. 3). Each source word will be mapped to a target phrase of one or more words or an ""empty"" phrase ε. In particular, the source words which will remain non-aligned due to the alignment functionality restriction are paired with the empty phrase. We can then formulate the problem of finding the best translation êI 1 of a source sentence f J 1 : êI 1 = argmax e I 1 P r(f J 1 , e I 1 ) = argmax ẽJ 1 A∈A P r(f J 1 , ẽJ 1 , A) ∼ = argmax ẽJ 1 max A∈A P r(A) • P r(f J 1 , ẽJ 1 |A) ∼ = argmax ẽJ 1 max A∈A f j :j=1...J P r(f j , ẽj |f j−1 1 , ẽj−1 1 , A) = argmax ẽJ 1 max A∈A f j :j=1...J p(f j , ẽj |f j−1 j−m , ẽj−1 j−m , A) In other words: if we assume a uniform distribution for P r(A), the translation problem can be mapped to the problem of estimating an m-gram language model over a learned set of bilingual tuples (f j , ẽj ). Mapping the bilingual language model to a WFST T is canonical and it has been shown in (Kanthak et al., 2004 ) that the search problem can then be rewritten using finite-state terminology: êI 1 = project-output(best(f J 1 • T )) . This implementation of the problem as WFSTs may be used to efficiently solve the search problem in machine translation. Reordering in Training When the alignment function A is not monotonic, target language phrases ẽ can become very long. For example in a completely non-monotonic alignment all target words are paired with the last aligned source word, whereas all other source words form tuples with the empty phrase. Therefore, for language pairs with big differences in word order, probability estimates may be poor. This problem can be solved by reordering either source or target training sentences such that alignments become monotonic for all sentences. We suggest the following consistent source sentence reordering and alignment monotonization approach in which we compute optimal, minimum-cost alignments. First, we estimate a cost matrix C for each sentence pair (f J 1 , e I 1 ). The elements of this matrix c ij are the local costs of aligning a source word f j to a target word e i . Following (Matusov et al., 2004) , we compute these local costs by interpolating state occupation probabilities from the source-to-target and target-to-source training of the HMM and IBM-4 models as trained by the GIZA++ toolkit (Och et al., 2003) . For a given alignment A ⊆ I × J, we define the costs of this alignment c(A) as the sum of the local costs of all aligned word pairs: c(A) = (i,j)∈A c ij (1) The goal is to find an alignment with the minimum costs which fulfills certain constraints. Source Sentence Reordering To reorder a source sentence, we require the alignment to be a function of source words A 1 : {1, . . . , J} → {1, . . . , I}, easily computed from the cost matrix C as: A 1 (j) = argmin i c ij (2) We do not allow for non-aligned source words. A 1 naturally defines a new order of the source words f J 1 which we denote by f J 1 . By computing this permutation for each pair of sentences in training and applying it to each source sentence, we create a corpus of reordered sentences. Alignment Monotonization In order to create a ""sentence"" of bilingual tuples ( f J 1 , ẽJ 1 ) we required alignments between reordered source and target words to be a function of target words A 2 : {1, . . . , I} → {1, . . . , J}. This alignment can be computed in analogy to Eq. 2 as: A 2 (i) = argmin j cij (3) where cij are the elements of the new cost matrix C which corresponds to the reordered source sentence. We can optionally re-estimate this matrix by repeating EM training of state occupation probabilities with GIZA++ using the reordered source corpus and the original target corpus. Alternatively, we can get the cost matrix C by reordering the columns of the cost matrix C according to the permutation given by alignment A 1 . In alignment A 2 some target words that were previously unaligned in A 1 (like ""the"" in Fig. 1 ) may now still violate the alignment monotonicity. The monotonicity of this alignment can not be guaranteed for all words if re-estimation of the cost matrices had been performed using GIZA++. The general GIATI technique (Casacuberta et al., 2004) is applicable and can be used to monotonize the alignment A 2 . However, in our experiments the following method performs better. We make use of the cost matrix representation and compute a monotonic minimum-cost alignment with a dynamic programming algorithm similar to the Levenshtein string edit distance algorithm. As costs of each ""edit"" operation we consider the local alignment costs. The resulting alignment A 3 represents a minimum-cost monotonic ""path"" through the cost matrix. To make A 3 a function of target words we do not consider the source words non-aligned in A 2 and also forbid ""deletions"" (""many-to-one"" source word alignments) in the DP search. An example of such consistent reordering and monotonization is given in Fig. 1 . Here, we reorder the German source sentence based on the initial alignment A 1 , then compute the function of target words A 2 , and monotonize this alignment to A 3 Reordering in Search When searching the best translation ẽJ 1 for a given source sentence f J 1 , we permute the source sentence as described in (Knight et al., 1998) : êI 1 = project-output(best(permute(f J 1 ) • T )) Permuting an input sequence of J symbols results in J! possible permutations and representing the permutations as a finite-state automaton requires at least 2 J states. Therefore, we opt for computing the permutation automaton on-demand while applying beam pruning in the search. Lazy Permutation Automata For on-demand computation of an automaton in the flavor described in (Kanthak et al., 2004) it is sufficient to specify a state description and an algorithm that calculates all outgoing arcs of a state from the state description. In our case, each state represents a permutation of a subset of the source words f J 1 , which are already translated. This can be described by a bit vector b J 1 (Zens et al., 2002) . Each bit of the state bit vector corresponds to an arc of the linear input automaton and is set to one if the arc has been used on any path from the initial to the current state. The bit vectors of two states connected by an arc differ only in a single bit. Note that bit vectors elegantly solve the problem of recombining paths in the automaton as states with the same bit vectors can be merged. As a result, a fully minimized permutation automaton has only a single initial and final state. Even with on-demand computation, complexity using full permutations is unmanagable for long sentences. We further reduce complexity by additionally constraining permutations. Refer to Figure 2 for visualizations of the permutation constraints which we describe in the following. IBM Constraints The IBM reordering constraints are well-known in the field of machine translation and were first described in (Berger et al., 1996) . The idea behind these constraints is to deviate from monotonic translation by postponing translations of a limited number of words. More specifically, at each state we can translate any of the first l yet uncovered word positions. The implementation using a bit vector is straightforward. For consistency, we associate window size with the parameter l for all constraints presented here. Inverse IBM Constraints The original IBM constraints are useful for a large number of language pairs where the ability to skip some words reflects the differences in word order between the two languages. For some other pairs, it is beneficial to translate some words at the end of the sentence first and to translate the rest of the sentence nearly monotonically. Following this idea we can define the inverse IBM constraints. Let j be the first uncovered position. We can choose any position for translation, unless l − 1 words on positions j > j have been translated. If this is the case we must translate the word in position j. The inverse IBM constraints can also be expressed by invIBM(x) = transpose(IBM(transpose(x))) . As the transpose operation can not be computed on-demand, our specialized implementation uses bit vectors b J 1 similar to the IBM constraints. Local Constraints For some language pairs, e.g. Italian -English, words are moved only a few words to the left or right. The IBM constraints provide too many alternative permutations to chose from as each word can be moved to the end of the sentence. A solution that allows only for local permutations and therefore has very low complexity is given by the following permutation rule: the next word for translation comes from the window of l positions 1 counting from the first yet uncovered position. Note, that the local constraints define a true subset of the permutations defined by the IBM constraints. ITG Constraints Another type of reordering can be obtained using Inversion Transduction Grammars (ITG) (Wu, 1997) . These constraints are inspired by bilingual bracketing. They proved to be quite useful for machine translation, e.g. see (Bender et al., 2004) . Here, we interpret the input sentence as a sequence of segments. In the beginning, each word is a segment of its own. Longer segments are constructed by recursively combining two adjacent segments. combination step, we either keep the two segments in monotonic order or invert the order. This process continues until only one segment for the whole sentence remains. The on-demand computation is implemented in spirit of Earley parsing. We can modify the original ITG constraints to further limit the number of reorderings by forbidding segment inversions which violate IBM constraints with a certain window size. Thus, the resulting reordering graph contains the intersection of the reorderings with IBM and the original ITG constraints. Weighted Permutations So far, we have discussed how to generate the permutation graphs under different constraints, but permutations were equally probable. Especially for the case of nearly monotonic translation it is make sense to restrict the degree of non-monotonicity that we allow when translating a sentence. We propose a simple approach which gives a higher probability to the monotone transitions and penalizes the nonmonotonic ones. A state description b J 1 , for which the following condition holds: M on(j) : b j = δ(j ≤ j) ∀ 1 ≤ j ≤ J represents the monotonic path up to the word f j . At each state we assign the probability α to that outgoing arc where the target state description fullfills M on(j +1) and distribute the remaining probability mass 1 − α uniformly among the remaining arcs. In case there is no such arc, all outgoing arcs get the same uniform probability. This weighting scheme clearly depends on the state description and the outgoing arcs only and can be computed on-demand. Experimental Results Corpus Statistics The translation experiments were carried out on the Basic Travel Expression Corpus (BTEC), a multilingual speech corpus which contains tourism-related sentences usually found in travel phrase books. We tested our system on the so called Chinese-to-English (CE) and Japanese-to-English (JE) Supplied Tasks, the corpora which were provided during the International Workshop on Spoken Language Translation (IWSLT 2004) (Akiba et al., 2004) . In addition, we performed experiments on the Italian-to-English (IE) task, for which a larger corpus was kindly provided to us by ITC/IRST. The corpus statistics for the three BTEC corpora are given in Tab. 1. The development corpus for the Italian-to-English translation had only one reference translation of each Italian sentence. A set of 506 source sentences and 16 reference translations is used as a development corpus for Chinese-to-English and Japanese-to-English and as a test corpus for Italianto-English tasks. The 500 sentence Chinese and Japanese test sets of the IWSLT 2004 evaluation campaign were translated and automatically scored against 16 reference translations after the end of the campaign using the IWSLT evaluation server. Evaluation Criteria For the automatic evaluation, we used the criteria from the IWSLT evaluation campaign (Akiba et al., 2004) , namely word error rate (WER), positionindependent word error rate (PER), and the BLEU and NIST scores (Papineni et al., 2002; Doddington, 2002) . The two scores measure accuracy, i. e. larger scores are better. The error rates and scores were computed with respect to multiple reference transla- tions, when they were available. To indicate this, we will label the error rate acronyms with an m. Both training and evaluation were performed using corpora and references in lowercase and without punctuation marks. Experiments We used reordering and alignment monotonization in training as described in Sec. 3. To estimate the matrices of local alignment costs for the sentence pairs in the training corpus we used the state occupation probabilities of GIZA++ IBM-4 model training and interpolated the probabilities of source-to-target and target-to-source training directions. After that we estimated a smoothed 4-gram language model on the level of bilingual tuples f j , ẽj and represented it as a finite-state transducer. When translating, we applied moderate beam pruning to the search automaton only when using reordering constraints with window sizes larger than 3. For very large window sizes we also varied the pruning thresholds depending on the length of the input sentence. Pruning allowed for fast translations and reasonable memory consumption without a significant negative impact on performance. In our first experiments, we tested the four reordering constraints with various window sizes. We aimed at improving the translation results on the development corpora and compared the results with two baselines: reordering only the source training sentences and translation of the unreordered test sentences; and the GIATI technique for creating bilingual tuples (f j , ẽj ) without reordering of the source sentences, neither in training nor during translation. Highly Non-Monotonic Translation (JE) Fig. 3 (left) shows word error rate on the Japanese-to-English task as a function of the window size for different reordering constraints. For each of the constraints, good results are achieved using a window size of 9 and larger. This can be attributed to the Japanese word order which is very different from English and often follows a subjectobject-verb structure. For small window sizes, ITG or IBM constraints are better suited for this task, for larger window sizes, inverse IBM constraints perform best. The local constraints perform worst and require very large window sizes to capture the main word order differences between Japanese and English. However, their computational complexity is low; for instance, a system with local constraints and window size of 9 is as fast (25 words per second) as the same system with IBM constraints and window size of 5. Using window sizes larger than 10 is computationally expensive and does not significantly improve the translation quality under any of the constraints. Tab. 2 presents the overall improvements in translation quality when using the best setting: inverse IBM constraints, window size 9. The baseline without reordering in training and testing failed completely for this task, producing empty translations for 37 % of the sentences 2 . Most of the original alignments in training were non-monotonic which resulted in mapping of almost all Japanese words to ε when using only the GIATI monotonization technique. Thus, the proposed reordering methods are of crucial importance for this task. Further improvements were obtained with a rescoring procedure. For rescoring, we produced a k-best list of translation hypotheses and used the word penalty and deletion model features, the IBM Model 1 lexicon score, and target language n-gram models of the order up to 9. The scaling factors for all features were optimized on the development corpus for the NIST score, as described in (Bender et al., 2004) . Moderately Non-Mon. Translation (CE) Word order in Chinese and English is usually similar. However, a few word reorderings over quite large distances may be necessary. This is especially true in case of questions, in which question words like ""where"" and ""when"" are placed at the end of a sentence in Chinese. The BTEC corpora contain many sentences with questions. The inverse IBM constraints are designed to perform this type of reordering (see Sec. 4.3). As shown in Fig. 3 , the system performs well under these con- straints already with relatively small window sizes. Increasing the window size beyond 4 for these constraints only marginally improves the translation error measures for both short (under 8 words) and long sentences. Thus, a suitable language-pair-specific choice of reordering constraints can avoid the huge computational complexity required for permutations of long sentences. Tab. 2 includes error measures for the best setup with inverse IBM constraints with window size of 7, as well as additional improvements obtained by a kbest list rescoring. The best settings for reordering constraints and model scaling factors on the development corpora were then used to produce translations of the IWSLT Japanese and Chinese test corpora. These translations were evaluated against multiple references which were unknown to the authors. Our system (denoted with WFST, see Tab. 3) produced results competitive with the results of the best system at the evaluation campaign (denoted with AT (Bender et al., 2004) ) and, according to some of the error measures, even outperformed this system. Almost Monotonic Translation (IE) The word order in the Italian language does not differ much from the English. Therefore, the absolute translation error rates are quite low and translating without reordering in training and search already results in a relatively good performance. This is reflected in Tab. 4. However, even for this language pair it is possible to improve translation quality by performing reordering both in training and during translation. The best performance on the development corpus is obtained when we constrain the reodering with relatively small window sizes of 3 to 4 and use either IBM or local reordering constraints. On the test corpus, as shown in Tab. 4, all error measures can be improved with these settings. Especially for languages with similar word order it is important to use weighted reorderings (Sec. 4.6) in order to prefer the original word order. Introduction of reordering weights for this task results in notable improvement of most error measures using either the IBM or local constraints. The optimal probability α for the unreordered path was determined on the development corpus as 0.5 for both of these constraints. The results on the test corpus using this setting are also given in Tab. 4. Conclusion In this paper, we described a reordering framework which performs source sentence reordering on word level. We suggested to use optimal alignment functions for monotonization and improvement of translation model training. This allowed us to translate monotonically taking a reordering graph as input. We then described known and novel reordering constraints and their efficient finite-state implementations in which the reordering graph is computed ondemand. We also utilized weighted permutations. We showed that our monotonic phrase-based translation approach effectively makes use of the reordering framework to produce quality translations even from languages with significantly different word order. On the Japanese-to-English and Chinese-to-English IWSLT tasks, our system performed at least as well as the best machine translation system. Acknowledgement This work was partially funded by the Deutsche Forschungsgemeinschaft (DFG) under the project ""Statistische Textübersetzung"" (Ne572/5) and by the European Union under the integrated project TC-STAR -Technology and Corpora for Speech to Speech Translation (IST-2002-FP6-506738).",1854610,0cfe82145332522ed6665bd039f4fd089ae24141,93,https://aclanthology.org/W05-0831,Association for Computational Linguistics,"Ann Arbor, Michigan",2005,June,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,"Kanthak, Stephan  and
Vilar, David  and
Matusov, Evgeny  and
Zens, Richard  and
Ney, Hermann",Novel Reordering Approaches in Phrase-Based Statistical Machine Translation,167--174,,,,,,,inproceedings,kanthak-etal-2005-novel,,,
49,C10-1025,Text mining for global health surveillance is an emerging technology that is gaining increased attention from public health organisations and governments. The lack of multilingual resources such as Word-Nets specifically targeted at this task have so far been a major bottleneck. This paper reports on a major upgrade to the BioCaster Web monitoring system and its freely available multilingual ontology; improving its original design and extending its coverage of diseases from 70 to 336 in 12 languages.,"Text mining for global health surveillance is an emerging technology that is gaining increased attention from public health organisations and governments. The lack of multilingual resources such as Word-Nets specifically targeted at this task have so far been a major bottleneck. This paper reports on a major upgrade to the BioCaster Web monitoring system and its freely available multilingual ontology; improving its original design and extending its coverage of diseases from 70 to 336 in 12 languages. Introduction The number of countries who can sustain teams of experts for global monitoring of human/animal health is limited by scarce national budgets. Whilst some countries have advanced sensor networks, the world remains at risk from the health impacts of infectious diseases and environmental accidents. As seen by the recent A(H5N1), A(H1N1) and SARS outbreaks, a problem in one part of the world can be rapidly exported, leading to global hardship. The World Health Organization (WHO) estimates that in the future, between 2 to 7.4 million people could be at risk worldwide from a highly contageous avian flu virus that spreads rapidly through the international air travel network (WHO, 2005) . Pandemics of novel pathogens have the capacity to overwhelm healthcare systems, leading to widespread morbidity, mortality and socio-economic disruption (Cox et al., 2003) . Furthermore, outbreaks of livestock diseases, such as foot-and-mouth disease or equine influenza can have a devastating impact on industry, commerce and human health (Blake et al., 2003) . The challenge is to enhance vigilance and control the emergence of outbreaks. Whilst human analysis remains essential to spot complex relationships, automated analysis has a key role to play in filtering the vast volume of data in real time and highlighting unusual trends using reliable predictor indicators. BioCaster (http://born.nii.ac.jp) (Collier et al., 2008) is a Web 2.0 monitoring station for the early detection of infectious disease events. The system exploits a high-throughput semantic processing pipeline, converting unstructured news texts to structured records, alerting events based on time-series analysis and then sharing this information with users via geolocating maps (Fig. 1(a) ), graphs (Fig. 1(b )) and alerts. Underlying the system is a publicly available multilingual application ontology. Launched in 2006 (Collier et al., 2006) the BioCaster Ontology (BCO) has been downloaded by over 70 academic and industrial groups worldwide. This paper reports on a major upgrade to the system and the ontology -expanding the number of languages from 6 to 12, redefining key relations and extending coverage in the number of diseases from 70 to 336, including many veterinary diseases. Background As the world becomes more interconnected and urbanized and animal production becomes increasingly intensive, the speed with which epidemics spread becomes faster, adding to pressure on biomedical experts and governments to make quick decisions. Traditional validation methods such as field investigations or laboratory analysis are the mainstay of public health but can require days or weeks to issue reports. The World Wide Web with its economical and real time delivery of information represents a new modality in health surveillance (Wagner and Johnson, 2006) and has been shown to be an effective source by the World Health Organization (WHO) when Public Health Canada's GPHIN system detected the SARS outbreak in southern China from news reports during November 2002. The recent A(H1N1) 'swine flu' pandemic highlighted the trend towards agencies using unvalidated sources. The technological basis for such systems can be found in statistical classification approaches and light weight ontological reasoning. For example, Google Flu Trends (Ginsberg et al., 2009) is a system that depends almost entirely on automatic statistical classification of user queries; MedISys-PULS (Yangarber et al., 2008) , HealthMap (Freifeld et al., 2008) and BioCaster use a mixture of statistical and ontological classification; and GPHIN (Mawudeku and Blench, 2006) and Argus (Wilson, 2007) rely on a mixture of ontological classification and manual analysis. Compared to other similar systems BioCaster is characterized by its richly featured and publicly downloadable ontology and emphasizes critical evaluation of its text mining modules. Empirical results have included: topic classification, named entity recognition, formal concept analysis and event recognition. In the absence of a community gold standard, task performance was assessed on the best available 'silver' standard -the ProMED-mail network (Madoff and Woodall, 2005) , achieving F-score of 0.63 on 14 disease-country pairs over a 365-day period (Collier, 2010) . Despite initial skepticism within the public health community, health surveillance systems based on NLP-supported human analysis of media reports are becoming firmly established in Europe, North America and Japan as sources of health information available to governments and the public (Hartley et al., 2010) . Whilst there is no substitute for trained human analysts, automated filtering has helped experts save time by allowing them to sift quickly through massive volumes of media data. It has also enabled them to supplement traditional sources with a broader base of information. In comparison with other areas of biomedical NLP such as the clinical and genetics' domains, a relative lack of building block resources may have hindered the wider participation of NLP groups in public health applications. It is hoped that the provision of common resources like the BCO can help encourage further development and benchmarking. Method BioCaster performs analysis of over 9000 news articles per day using the NPACI Rocks cluster middleware (http://www.rockcsclusters.org) on a platform of 48 3.0GHz Xeon cores. Data is ingested 24/7 into a semantic processing pipeline in a short 1 hour cycle from over 1700 public domain RSS feeds such as Google news, the European Media Monitor and ProMED-mail. Since 2009, news has also being gathered under contract from a commercial news aggregation company, providing access to over 80,000 sources across the world's languages. The new 2010 version of BioCaster uses machine translation into English (eleven languages) to source news stories related to currently occurring infectious and environmental disease outbreaks in humans, animals and plants. Access to the site is freely available but login registration applies to some functions such as email alerts. Processing is totally automatic, but we have the potential within the login system to enable human moderated alerts which broadcast to Twitter and RSS. Below we describe in detail two key aspects of the system that have been significantly upgraded: the BCO and the event detection system. Ontology Aim The BioCaster Ontology aims: • To describe the terms and relations necessary to detect and risk assess public health events in the grey literature; • To bridge the gap between (multilingual) grey literature and existing standards in biomedicine; • To mediate integration of content across languages; • To be freely available. The central knowledge source for BioCaster is the multilingual ontology containing domain terms such as diseases, agents, symptoms, syndromes and species as well as domain sensitive relations such as a disease causing symptoms or an agent affecting particular host species. This allows the text mining system to have a basic understanding of the key concepts and relationships within the domain to fill in gaps not mentioned explicitly in the news reports. To the best of our knowledge the BCO is unique as an application ontology, providing freely available multilingual support to system developers interested in outbreak surveillance in the language of the open media. The BCO however has little to say outside of its application domain, e.g. in disease-gene interaction or for supporting automatic diagnosis. As discussed in Grey Cowell and Smith (2010), there are many other resources available that have the potential to support applications for infectious disease analysis including controlled vocabularies and ontologies such as the the Unified Medical Language System (UMLS) (Lindberg et al., 1993) , International Classification of Diseases (ICD-10) (WHO, 2004) , SNOMED CT (Stearns et al., 2001) , Medical Subject Headings (MeSH) (Lipscomb, 2000) and the Infectious Disease Ontology (IDO) (Grey Cowell and Smith, 2010) . In (Collier et al., 2006) we discussed how BCO compared to such ontologies so we will focus from now on the implication of the extensions. Scope The new version of the BCO now covers 12 languages including all the United Nation's official languages: Arabic (968 terms), English (4113), French (1281), Indonesian (1081), Japanese (2077), Korean (1176), Malaysian (1001), Russian (1187), Spanish (1171), Thai (1485), Vietnamese (1297) and Chinese (1142). The multilingual ontology can be used as a direct knowledge source in language-specific text mining modules, as an indexing resource for searching across concepts in various languages and as a dictionary for future translation modules. Currently news in all 12 languages is available via the Web portal but news in additional languages such as German, Italian and Dutch are being added using machine translation. Design Like EuroWordNet (Vossen, 1998) , on which it is loosely based, the BCO adopts a thesauruslike structure with synonym sets linking together terms across languages with similar meaning. Synonym sets are referred to using root terms. Root terms themselves are fully defined instances that provide bridges to external classification schemes and nomenclatures such as ICD10, MeSH, SNOMED CT and Wikipedia. The central backbone taxonomy is deliberately shallow and taken from the ISO's Suggested Upper Merged Ontology (Niles and Pease, 2001) . To maintain consistency and computability we kept a single inheritance structure throughout. 18 core domain concepts corresponding to named entities in the text mining system such as DISEASE and SYMP-TOM were the results of analysis using a formal theory (Guarino and Welty, 2000) . We have endeavoured to construct definitions for root terms along Aristotelean principles by specifying the difference to the parent. For example in the case of Eastern encephalitis virus: Eastern equine encephalitis virus is a species of virus that belongs to the genus Alphavirus of the family Togaviridae (order unassigned) of the group IV ((+)ssRNA) that possesses a positive single stranded RNA genome. It is the etiological agent of the eastern equine encephalitis. We are conscious though that terms used in the definitions still require more rigorous control to be considered useful for machine reasoning. To aid both human and machine analysis root terms are linked by a rich relational structure reflecting domain sensitive relations such as causes(virus,disease), has symptom(disease, symptom), has associated syndrome(disease, syndrome), has reservoir(virus, organism). In such a large undertaking, the order of work was critical. We proceeded by collecting a list of notifiable diseases from national health agencies and then grouped the diseases according to perceived relevance to the International Health Regulations 2005 (Lawrence and Gostin, 2004) . In this way we covered approximately 200 diseases, and then explored freely available resources and the biomedical literature to find academic and layman's terminology to describe their agents, affected hosts, vector species, symptoms, etc. We then expanded the coverage to less well known human diseases, zoonotic diseases, animal diseases and diseases caused by toxic substances such as sarin, hydrogen sulfide, sulfur dioxide and ethylene. At regular stages we checked and validated terms against those appearing in the news media. As we expanded the number of conditions to include veterinary diseases we found a major structural reorganization was needed to support animal symptoms. For example, a high temperature in humans would not be the same as one in bovids. This prompted us in the new version to group diseases and symptoms around major animal familes and related groups, e.g. high temperature (human) and high temperature (bovine). A second issue that we encountered was the need to restructure the hierarchy under Organi-cObject which was divided between MicroOrganism and Animal. The structure of the previous version meant that the former were doing double duty as infecting agents and the later were affected hosts. The MicroOrganism class contained bacterium, helminth, protozoan, fungus and virus, which then became the domain in a relation 'x causes y'. Expansion forced us to accomodate the fact that some animals such as worms and mites (e.g. scabies) also infect humans as well as animals. The result was a restructuring of the organic classes using the Linnean taxonomy as a guideline, although this is probably not free from errors (e.g. virus is typically not considered to be an organism). Event alerting system Figure 1 (c) shows a schematic of the modular design used by the BioCaster text mining system. Following on from machine translation and topic classification is named entity recognition and template recognition which we describe in more detail below. The final structured event frames include slot values normalized to ontology root terms for disease, pathogen (virus or bacterium), country and province. Additionally we also identify 15 aspects of public health events critical to risk assessment such as: spread across international borders, hospital worker infection, accidental or deliberate release, food contamination and vaccine contamination. Latitude and longitude of events down to the province level are found in two ways: using the Google API up to a limit of 15000 lookups per day, and then using lookup on the BCO taxonomy of 5000 country and province names derived from open sources such as Wikipedia. Each hour events are automatically alerted to a Web portal page by comparing daily aggregated event counts against historical norms (Collier, 2010) . Login users can also sign up to receive emails on specific topics. A topic would normally specify a disease or syndrome, a country or region and a specific risk condition. In order to extract knowledge from documents, BioCaster maintains a collection of rule patterns in a regular expression language that converts surface expressions into structured information. For example the surface phrase ""man exposes airline passengers to measles"" would be converted into the three templates ""species(human); disease(measles); international travel(true)"". Writing patterns to produce such templates can be very time consuming and so the BioCaster project has developed its own D3: :-name(disease){ list(@undiagnosed) words(,1) list(@disease) } S2: :-name(symptom) { list(@severity) list(@symptom)} CF1: contaminated food(""true"") :-""caused"" ""by"" list(@contaminate verbs past) list(@injested material) SP4: species(""animal"") :-name(animal,A) words(,3) list(@cull verbs past) Table 1 : Examples of SRL rules for named entity and template recognition. Template rules contain a label, a head and a body, where the head specifies the template pattern to be output if the body expression matches. The body can contain word lists, literals, and wild cards. Various conditions can be placed on each of these such as orthographic matching. light weight rule language -called the Simple Rule Language (SRL) and a pattern building interface for maintaining the rule base (McCrae et al., 2009) . Both are freely available to the research community under an open source license. Currently BioCaster uses approximately 130 rules for entity recognition, 1000 word lists and 3200 template rules (of which half are for location recognition) to identify events of interest in English. Using SRL allows us to quickly adapt the system to newly emerging terminology such as the 11+ designations given to A(H1N1) during the first stages of the 2009 pandemic. The SRL rulebook for BioCaster can recognize a range of entities related to the task of disease surveillance such as bacteria, chemicals, diseases, countries, provinces, cities and major airports. Many of these classes are recognized using terms imported from the BCO. The rule book also contains specialised thesauri to recognize subclasses of entities such as locations of habitation, eateries and medical service centres. Verb lists are maintained for lexical classes such as detection, mutation, investigation, causation, contamination, culling, blaming, and spreading. Some examples of SRL rules for named entity recognition are shown in Table 1 and described below: Rule D3 in the rulebook tags phrases like 'mystery illness' or 'unknown killer bug' by matching on strings contained within two wordlists, @undiagnosed and @disease, separated by up to one word. Rule S2 allows severity indicators such as 'severe' or 'acute' to modify a list of known symptoms in order to identify symptom entities. Rule CF1 is an example of a template rule. If the body of the rule matches by picking out expressions such as 'was caused by tainted juice', this triggers the head to output an alert for contaminated food. Rule SP4 identifies the victim species as 'animal' in contexts like '250 geese were destroyed'. The rulebook also supports more complex inferences such as the home country of national public health organizations. Since BioCaster does not employ systematic manual checking of its reports, it uses a number of heuristic filters to increase specificity (the proportion of correctly identified negatives) for reports that appear on the public Web portal pages. For example, reports with no identified disease and country are rejected. Since these heuristics may reduce sensitivity they are not applied to news that appears on the user login portal pages. Results and Discussion Version 3 of the ontology represents a significant expansion in the coverage of diseases, symptoms and pathogens on version 2. Table 2 summarizes the number of root terms for diseases classified by animal familes. The thesaurus like structure of the BCO is compatible in many respects to the Simple Knowledge Organization System (SKOS) (Miles et al., 2005) . In order to extend exchange and re-use we have produced a SKOS version of the BCO which is available from the BCO site. We have also converted the BCO terms into 12 SRL rule books (1 for each language) for entity tagging. These too are freely available from the BCO site. As the ontology expands we will consider adopting a more detailed typing of diseases such as hasInfectingPart to indicate the organ affected Conclusion Multilingual resources specifically targeted at the task of global health surveillance have so far been very rare. We hope that the release of version 3 can be used to support a range of applications such as text classification, cross language search, machine translation, query expansion and so on. The BCO has been constructed to provide core vocabulary and knowledge support to the Bio-Caster project but it has also been influential in the construction of other public health ori-ented application ontologies such as the Syndromic Surveillance Ontology (Okhamatovskaia et al., 2009) . The BCO is freely available from http://code.google.com/p/biocaster-ontology/ under a Creative Commons license. Acknowledgements The authors greatly acknowledge the many coworkers who have provided comments and feedback on BioCaster. Funding support was provided in part by the Japan Science and Technology Agency under the PRESTO programme.",14382934,112c9ec3ec20b29e1faee91867fe4c0bebf3a770,27,https://aclanthology.org/C10-1025,Coling 2010 Organizing Committee,"Beijing, China",2010,August,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),"Collier, Nigel  and
Matsuda Goodwin, Reiko  and
McCrae, John  and
Doan, Son  and
Kawazoe, Ai  and
Conway, Mike  and
Kawtrakul, Asanee  and
Takeuchi, Koichi  and
Dien, Dinh",An ontology-driven system for detecting global health events,215--222,,,,,,,inproceedings,collier-etal-2010-ontology,,,Resources and Evaluation
